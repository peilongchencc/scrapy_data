# mdniceç½‘å€å†…å®¹çˆ¬å–:

æœ¬é¡¹ç›®ä»…ç”¨äºçˆ¬è™«ç ”ç©¶ï¼Œç›®çš„ä»¥ `mdnice.com` ä¸ºä¾‹ï¼Œå­¦ä¹ åˆ©ç”¨ Scrapy è¿›è¡Œ HTML æ ‡ç­¾æŠ½å–ï¼Œæ ‡é¢˜æå–ã€è·³è½¬åçš„ç½‘é¡µæå–ã€è·³è½¬åç½‘é¡µæ˜¾ç¤ºçš„é˜…è¯»æ•°æå–ã€‚<br>

å†…å®¹ç¤ºæ„å›¾å¦‚ä¸‹:<br>

[](./mdnice_intro.jpg)


## å¤åˆ¶å®Œæ•´çš„HTML(Macç‰ˆã€ä»¥Chromeä¸¾ä¾‹):

1. æ‰“å¼€ä½ æƒ³è¦å¤åˆ¶HTMLçš„ç½‘é¡µã€‚

2. ä½¿ç”¨å¿«æ·é”® `Option + Command + I` æ‰“å¼€ **å¼€å‘è€…å·¥å…·**ã€‚

3. åœ¨å¼€å‘è€…å·¥å…·ä¸­ï¼Œä½ ä¼šçœ‹åˆ°HTMLæºä»£ç çš„æ ‘å½¢ç»“æ„ã€‚ä¸€èˆ¬æƒ…å†µä¸‹ï¼Œ`<html>` æ ‡ç­¾ä¼šæ˜¯è¿™æ£µæ ‘çš„æ ¹èŠ‚ç‚¹ã€‚

4. å³é”®ç‚¹å‡»<html>æ ‡ç­¾ï¼Œé€‰æ‹©â€œCopyâ€>Copy outerHTMLã€‚

è¿™æ ·å°±ä¼šå¤åˆ¶æ•´ä¸ªç½‘é¡µçš„HTMLä»£ç åˆ°å‰ªè´´æ¿ä¸­ã€‚<br>


## ç»ˆç«¯æµ‹è¯•æ•ˆæœ(å¯é€‰):

å¯ä»¥ç»ˆç«¯è¾“å…¥ä»¥ä¸‹æŒ‡ä»¤ï¼Œç®€å•æŸ¥çœ‹ Scrapy ä»ç½‘é¡µæŠ“å–åˆ°çš„å†…å®¹ï¼Œç„¶åé€‰æ‹©è¦æå–çš„å†…å®¹ï¼Œå¸®åŠ©è‡ªå·±æ›´å¥½å®Œæˆè‡ªå·±çš„çˆ¬è™«æ–‡ä»¶:<br>

```bash
scrapy shell 'https://www.mdnice.com/'
```

ä¾‹å¦‚åœ¨ scrapy shell ä¸­è¾“å…¥ä»¥ä¸‹æŒ‡ä»¤æŸ¥çœ‹æ•ˆæœ:<br>

```bash
response.xpath('//script[@id="__NEXT_DATA__"]/text()').get()
```


## çˆ¬å–éœ€è¦çš„ç»“æœ:

```python
import scrapy
import json
from datetime import datetime, timedelta, timezone

class MdniceSpider(scrapy.Spider):
    name = 'mdnice'
    start_urls = ['https://www.mdnice.com/']

    def parse(self, response):
        json_data = response.xpath('//script[@id="__NEXT_DATA__"]/text()').get()
        data = json.loads(json_data)

        articles = data['props']['pageProps']['list']
        for article in articles:
            out_id = article['outId']
            title = article['title']
            url = f"https://www.mdnice.com/writing/{out_id}"
            create_time_str = article['createTime']

            # è§£æISO 8601æ ¼å¼æ—¶é—´å­—ç¬¦ä¸²ï¼ŒåŒ…æ‹¬æ—¶åŒºä¿¡æ¯
            create_time = datetime.strptime(create_time_str, "%Y-%m-%dT%H:%M:%S.%f%z")

            # è·å–å½“å‰æ—¶é—´ï¼ˆè€ƒè™‘åˆ°æ—¶åŒºï¼‰
            current_time = datetime.now(timezone.utc)

            # è®¡ç®—æ—¶é—´å·®
            time_difference = current_time - create_time

            # åˆ¤æ–­æ–‡ç« æ˜¯å¦åœ¨è¿‡å»2å°æ—¶å†…åˆ›å»º
            if time_difference <= timedelta(hours=20):
                # ä½¿ç”¨metaä¼ é€’é¢å¤–æ•°æ®
                yield scrapy.Request(url, callback=self.parse_article, meta={'title': title, 'create_time': create_time_str})

    def parse_article(self, response):
        # ä»metaä¸­æå–ä¼ é€’çš„æ•°æ®
        title = response.meta['title']
        create_time = response.meta['create_time']

        # æå–é˜…è¯»æ¬¡æ•°
        script = response.xpath("//script[@id='__NEXT_DATA__']/text()").get()
        # è§£æ JSON æ•°æ®
        data = json.loads(script)
        # æå– readingNum å­—æ®µ
        reading_num = data["props"]["pageProps"]["writingDetail"]["readingNum"]
        
        yield {
            'title': title,
            'create_time': create_time,
            'url': response.url,
            'read_count': reading_num
        }
```

## ç¤¼è²Œçˆ¬å–:

é€šå¸¸ï¼Œä½ éœ€è¦æ›´ç¤¼è²Œçš„çˆ¬å–æ•°æ®ï¼Œæ‰èƒ½ä¸è¢«ç¦æ‰IPã€é™åˆ¶è®¿é—®ã€‚å¯ä»¥å‚è€ƒä»¥ä¸‹å†™æ³•ã€‚<br>

### è®¾ç½®å›ºå®šçš„ä¸‹è½½å»¶è¿Ÿï¼ˆDOWNLOAD_DELAYï¼‰ï¼š

åœ¨ `settings.py` æ–‡ä»¶ä¸­æ·»åŠ ä»¥ä¸‹å†…å®¹ï¼Œè¿™ç§æ–¹æ³•é€‚ç”¨äºä½ æƒ³è¦åœ¨æ¯æ¬¡è¯·æ±‚ä¹‹é—´å¼ºåˆ¶ç­‰å¾…å›ºå®šæ—¶é—´çš„æƒ…å†µï¼Œä¾‹å¦‚æ¯2ç§’å‘é€ä¸€ä¸ªè¯·æ±‚ã€‚è¿™æ ·åšå¯ä»¥ç®€å•ç²—æš´åœ°é™åˆ¶è¯·æ±‚é€Ÿåº¦ï¼Œå‡å°‘å¯¹ç›®æ ‡ç½‘ç«™çš„å‹åŠ›ã€‚<br>

```conf
# settings.py

# è®¾ç½®ä¸‹è½½å»¶è¿Ÿä¸º2ç§’
DOWNLOAD_DELAY = 2
```

### å¯ç”¨è‡ªåŠ¨é™é€Ÿï¼ˆAUTOTHROTTLEï¼‰ï¼š

åœ¨ `settings.py` æ–‡ä»¶ä¸­æ·»åŠ ä»¥ä¸‹å†…å®¹ï¼Œè¿™ç§æ–¹æ³•æ›´ä¸ºæ™ºèƒ½ï¼Œå®ƒä¼šæ ¹æ®æœåŠ¡å™¨çš„å“åº”æ—¶é—´åŠ¨æ€è°ƒæ•´è¯·æ±‚é—´çš„å»¶è¿Ÿï¼Œä»¥æ‰¾åˆ°æœ€ä¼˜çš„çˆ¬å–é€Ÿåº¦ã€‚è¿™ä¸ä»…å¯ä»¥ä¿æŠ¤ç›®æ ‡ç½‘ç«™ä¸è¢«è¿‡åº¦è¯·æ±‚å½±å“ï¼Œè¿˜å¯ä»¥åœ¨å…è®¸çš„æƒ…å†µä¸‹æé«˜çˆ¬è™«çš„æ•ˆç‡ã€‚<br>

```conf
# settings.py

# å¯ç”¨è‡ªåŠ¨é™é€Ÿ
AUTOTHROTTLE_ENABLED = True
# åˆå§‹ä¸‹è½½å»¶è¿Ÿ
AUTOTHROTTLE_START_DELAY = 2
# æœ€å¤§ä¸‹è½½å»¶è¿Ÿï¼Œé˜²æ­¢å»¶è¿Ÿè¿‡é•¿
AUTOTHROTTLE_MAX_DELAY = 60
# å¯ç”¨æ˜¾ç¤ºè‡ªåŠ¨é™é€Ÿçš„è°ƒè¯•ä¿¡æ¯
AUTOTHROTTLE_DEBUG = True
```

çˆ¬å–æ•°æ®æ—¶ï¼Œå¯ä»¥å°†è¿™ä¸¤ä¸ªæ–¹æ¡ˆç»“åˆä½¿ç”¨ï¼Œé€šå¸¸ä¼šå¾—åˆ°æ›´å¥½çš„æ•ˆæœã€‚<br>

ç»“åˆä½¿ç”¨è¿™ä¸¤ç§æ–¹æ³•å¯ä»¥è®©ä½ çš„çˆ¬è™«æ—¢æœ‰ä¸€ä¸ªåŸºæœ¬çš„è¯·æ±‚é—´éš”ä¿éšœï¼ˆé€šè¿‡DOWNLOAD_DELAYï¼‰ï¼Œåˆèƒ½å¤Ÿæ™ºèƒ½åœ°è°ƒæ•´é€Ÿåº¦ä»¥åº”å¯¹ä¸åŒçš„æœåŠ¡å™¨å“åº”æƒ…å†µï¼ˆé€šè¿‡AUTOTHROTTLEï¼‰ã€‚<br>

ä¾‹å¦‚ï¼Œä½ å¯ä»¥è®¾ç½®ä¸€ä¸ªç›¸å¯¹è¾ƒå°çš„DOWNLOAD_DELAYä½œä¸ºåŸºçº¿ï¼Œç„¶åå¯ç”¨AUTOTHROTTLEæ¥å…è®¸Scrapyåœ¨è¿™ä¸ªåŸºçº¿çš„åŸºç¡€ä¸Šæ ¹æ®å®é™…çš„å“åº”æƒ…å†µè¿›è¡Œè°ƒæ•´ã€‚<br>

è¿™æ ·åšçš„å¥½å¤„æ˜¯ï¼Œå³ä½¿åœ¨æœåŠ¡å™¨å“åº”è¾ƒæ…¢æˆ–è€…ç½‘ç»œæ¡ä»¶å˜åŒ–çš„æƒ…å†µä¸‹ï¼Œä½ çš„çˆ¬è™«ä¹Ÿèƒ½å¤Ÿè‡ªé€‚åº”åœ°è°ƒæ•´è¯·æ±‚é€Ÿåº¦ï¼Œé¿å…å¯¹æœåŠ¡å™¨é€ æˆä¸å¿…è¦çš„è´Ÿæ‹…ï¼ŒåŒæ—¶å°½å¯èƒ½åœ°æé«˜çˆ¬å–æ•ˆç‡ã€‚<br>


## æ‰§è¡Œè‡ªå·±çš„pipelines:

æˆ‘ä»¬é€šå¸¸éœ€è¦å¯¹è‡ªå·±çˆ¬å–çš„æ•°æ®è¿›è¡Œä¸€äº›é¢å¤–çš„å¤„ç†ï¼Œä¾‹å¦‚å°†çˆ¬å–çš„ç»“æœå­˜å…¥MongoDBã€MySQLã€‚å­˜å…¥MongoDBçš„ç¤ºä¾‹å®˜æ–¹å·²ç»ç»™å‡ºï¼Œè¿™é‡Œä»¿ç…§å®˜æ–¹ç¤ºä¾‹å®ç°å°†çˆ¬å–çš„æ•°æ®å­˜å…¥MySQL:<br>

### ä¿®æ”¹ pipelines.py:

æ‰¾åˆ°å½“å‰Scrapyé¡¹ç›®æ–‡ä»¶å¤¹ä¸‹çš„ `pipelines.py` æ–‡ä»¶ï¼Œç„¶åå†™å…¥ä»¥ä¸‹å†…å®¹:<br>

```python
import pymysql
from itemadapter import ItemAdapter

import time

def current_timestamp():
    """è¿”å›å½“å‰æ—¥æœŸæ—¶é—´çš„å­—ç¬¦ä¸²è¡¨ç¤ºå½¢å¼,æ ¼å¼ä¸º: 2023-08-15 11:29:22 """
    return time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())

class MdnicePipeline:
    def process_item(self, item, spider):
        return item

class MySQLPipeline:
    """
    A pipeline to store the item in a MySQL database.
    """

    def __init__(self, mysql_host, mysql_db, mysql_user, mysql_password):
        self.mysql_host = mysql_host
        self.mysql_db = mysql_db
        self.mysql_user = mysql_user
        self.mysql_password = mysql_password

    @classmethod
    def from_crawler(cls, crawler):
        return cls(
            mysql_host=crawler.settings.get('MYSQL_HOST'),
            mysql_db=crawler.settings.get('MYSQL_DBNAME'),
            mysql_user=crawler.settings.get('MYSQL_USER'),
            mysql_password=crawler.settings.get('MYSQL_PASSWORD'),
        )

    def open_spider(self, spider):
        self.connection = pymysql.connect(host=self.mysql_host,
                                        user=self.mysql_user,
                                        password=self.mysql_password,
                                        database=self.mysql_db,
                                        charset='utf8mb4',
                                        cursorclass=pymysql.cursors.DictCursor)
        self.cursor = self.connection.cursor()

        # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º
        self.cursor.execute("""
            CREATE TABLE IF NOT EXISTS article_info (
                article_id INT AUTO_INCREMENT PRIMARY KEY COMMENT 'æ–‡ç« çš„å”¯ä¸€ID',
                article_title VARCHAR(255) NOT NULL COMMENT 'æ–‡ç« æ ‡é¢˜',
                article_read_count INT NOT NULL COMMENT 'æ–‡ç« é˜…è¯»æ•°',
                article_time VARCHAR(255) NOT NULL COMMENT 'æ–‡ç« å‘è¡¨æ—¶é—´',
                article_url VARCHAR(255) NOT NULL COMMENT 'æ–‡ç« å…·ä½“é“¾æ¥',
                create_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP COMMENT 'åˆ›å»ºæ—¶é—´',
                modify_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT 'ä¿®æ”¹æ—¶é—´'
            ) CHARSET=utf8mb4;
        """)
        self.connection.commit()

    def close_spider(self, spider):
        self.connection.close()

    def process_item(self, item, spider):
        # æ„å»ºæ’å…¥æ•°æ®çš„SQLè¯­å¥ï¼Œç°åœ¨åŒ…æ‹¬create_timeå’Œmodify_timeå­—æ®µ
        sql = """
        INSERT INTO article_info (article_title, article_read_count, article_time, article_url, create_time, modify_time) 
        VALUES (%s, %s, %s, %s, %s, %s)
        """
        # å‡†å¤‡æ’å…¥æ•°æ®çš„å€¼ï¼Œç¡®ä¿é¡ºåºä¸ä¸Šé¢çš„SQLè¯­å¥ä¸­çš„åˆ—å¯¹åº”
        values = (
            item.get('title'), 
            item.get('read_count'), 
            item.get('create_time'), 
            item.get('url'),
            current_timestamp(),  # ä½¿ç”¨å‡½æ•°ç”Ÿæˆæ’å…¥è®°å½•çš„å½“å‰æ—¶é—´
            current_timestamp(),  # åŒä¸Š
        )
        
        try:
            # æ‰§è¡ŒSQLè¯­å¥
            self.cursor.execute(sql, values)
            # æäº¤åˆ°æ•°æ®åº“æ‰§è¡Œ
            self.connection.commit()
        except pymysql.MySQLError as e:
            # å¦‚æœå‘ç”Ÿé”™è¯¯åˆ™å›æ»š
            print(e)
            self.connection.rollback()
        
        return item
```

å¯èƒ½æœ‰äººè¿·æƒ‘ï¼Œä»£ç ä¸­ä¸éœ€è¦ç”¨åˆ° `ItemAdapter` å—ï¼ŸScrapyçš„å®˜æ–¹ç¤ºä¾‹æ˜æ˜å°±ä½¿ç”¨äº† `ItemAdapter` ã€‚<br>

è¿™æ˜¯å› ä¸ºï¼Œåœ¨Scrapyé¡¹ç›®ä¸­ï¼Œ`ItemAdapter` ç”¨äºæä¾›ä¸€ä¸ªç»Ÿä¸€çš„APIæ¥å¤„ç†ä¸åŒç±»å‹çš„æ•°æ®é¡¹ï¼ˆå¦‚dictã€Itemã€Dataclassç­‰ï¼‰ï¼Œè¿™ä½¿å¾—åœ¨å¤„ç†æ•°æ®æ—¶æ›´åŠ çµæ´»ã€‚<br>

åœ¨ç¬”è€…çš„æƒ…å†µä¸‹ï¼Œå› ä¸ºç¬”è€…ç›´æ¥ä»itemå­—å…¸ä¸­è·å–å€¼ï¼Œä½¿ç”¨ItemAdapterå¹¶ä¸æ˜¯å¿…é¡»çš„ã€‚å½“ç„¶ï¼Œå¼•å…¥ `ItemAdapter` å¯ä»¥è®©ä»£ç æ›´åŠ å¥å£®å’Œçµæ´»ï¼Œå°¤å…¶æ˜¯å½“ä½ éœ€è¦å¤„ç†å¤šç§ç±»å‹çš„æ•°æ®é¡¹æˆ–è®¡åˆ’åœ¨ä¸åŒçš„ç¯å¢ƒä¸‹é‡ç”¨è¿™äº›ä»£ç æ—¶ã€‚<br>

å¦‚æœä½ æƒ³ä½¿ç”¨ `ItemAdapter` ï¼Œå¯ä»¥è¿™æ ·ä¿®æ”¹ `process_item` æ–¹æ³•ï¼š<br>

```python
from itemadapter import ItemAdapter

class MySQLPipeline:

    # çœç•¥å‰é¢çš„ä»£ç ï¼Œå’Œä¸Šæ–‡ä½¿ç”¨å¸¸è§„æ–¹æ³•çš„ä»£ç ä¸€è‡´ã€‚

    def process_item(self, item, spider):
        adapter = ItemAdapter(item)
        # æ„å»ºæ’å…¥æ•°æ®çš„SQLè¯­å¥
        sql = """
        INSERT INTO article_info (article_title, article_read_count, article_time, article_url) 
        VALUES (%s, %s, %s, %s)
        """
        # ä½¿ç”¨ItemAdapterè·å–itemå€¼ï¼Œæé«˜ä»£ç å…¼å®¹æ€§
        values = (
            adapter.get('title'), 
            adapter.get('read_count'), 
            adapter.get('create_time'), 
            adapter.get('url')
        )
        
        try:
            # æ‰§è¡ŒSQLè¯­å¥
            self.cursor.execute(sql, values)
            # æäº¤åˆ°æ•°æ®åº“æ‰§è¡Œ
            self.connection.commit()
        except pymysql.MySQLError as e:
            # å¦‚æœå‘ç”Ÿé”™è¯¯åˆ™å›æ»š
            print(e)
            self.connection.rollback()
        
        return item

```

### æ‹“å±•--pipelinesä¸­çš„å‡½æ•°å:

å†è¯´å¦ä¸€ä¸ªç–‘é—®ç‚¹ï¼Œå¯èƒ½æœ‰äººä¼šé—® "pipelinesä¸­çš„å‡½æ•°åæœ‰è®²ç©¶å—ï¼Ÿ"ï¼Œæ˜¯æœ‰ä»€ä¹ˆå›ºå®šçš„å†™æ³•å—ï¼Ÿ<br>

ç­”æ¡ˆ: æ˜¯çš„ï¼Œæ˜¯æœ‰å›ºå®šå†™æ³•ã€‚<br>

è§£é‡Š:<br>

åœ¨Scrapyæ¡†æ¶ä¸­ï¼Œ`pipelines.py` æ–‡ä»¶ç”¨äºå¤„ç†æ•°æ®ç®¡é“ï¼ˆPipelinesï¼‰ï¼Œè¿™æ˜¯Scrapyçš„ä¸€ä¸ªé‡è¦ç»„æˆéƒ¨åˆ†ï¼Œç”¨äºå¤„ç†ä»çˆ¬è™«ï¼ˆSpidersï¼‰é‚£é‡Œè·å–çš„æ•°æ®é¡¹ï¼ˆItemsï¼‰ã€‚åœ¨`pipelines.py`ä¸­å®šä¹‰çš„ç±»å¯ä»¥å¯¹æ•°æ®è¿›è¡Œå¤„ç†ï¼Œå¦‚æ¸…æ´—ã€éªŒè¯ã€å­˜å‚¨ï¼ˆæ¯”å¦‚å­˜å…¥æ•°æ®åº“ï¼‰ç­‰ã€‚Scrapyçš„æ•°æ®ç®¡é“æ”¯æŒå®šåˆ¶å¤šä¸ªå¤„ç†æ­¥éª¤ï¼Œæ¯ä¸ªæ­¥éª¤ç”±ä¸€ä¸ªPipelineç±»å®ç°ã€‚ä¸‹é¢æ˜¯ä¸€äº›Scrapyç®¡é“ä¸­å¸¸è§çš„å‡½æ•°åŠå…¶ç”¨é€”ï¼š<br>

1. `process_item(self, item, spider)`:
   - è¿™æ˜¯Pipelineä¸­æœ€é‡è¦çš„æ–¹æ³•ï¼Œæ¯ä¸ªItemçˆ¬å–å®Œæˆåéƒ½ä¼šè‡ªåŠ¨è°ƒç”¨æ­¤æ–¹æ³•ã€‚
   - å‚æ•°`item`æ˜¯è¢«çˆ¬å–çš„itemã€‚
   - å‚æ•°`spider`æ˜¯çˆ¬å–æ­¤itemçš„spiderã€‚
   - æ­¤æ–¹æ³•å¿…é¡»è¿”å›ä¸€ä¸ªå…·æœ‰æ•°æ®çš„itemå¯¹è±¡â€¼ï¸ï¼Œæˆ–è€…æ˜¯`DropItem`å¼‚å¸¸ï¼Œåè€…å°†ä¸­æ­¢å½“å‰itemçš„å¤„ç†ã€‚

2. `open_spider(self, spider)`:
   - å½“spiderè¢«å¼€å¯æ—¶ï¼Œè¿™ä¸ªæ–¹æ³•è¢«è°ƒç”¨ã€‚
   - ç”¨äºè¿›è¡Œä¸€äº›åˆå§‹åŒ–å·¥ä½œï¼Œæ¯”å¦‚åœ¨æ•°æ®åº“ç®¡é“ä¸­è¿æ¥æ•°æ®åº“ã€‚

3. `close_spider(self, spider)`:
   - å½“spiderè¢«å…³é—­æ—¶ï¼Œè¿™ä¸ªæ–¹æ³•è¢«è°ƒç”¨ã€‚
   - ç”¨äºè¿›è¡Œä¸€äº›æ¸…ç†å·¥ä½œï¼Œæ¯”å¦‚å…³é—­æ•°æ®åº“è¿æ¥ã€‚

4. `from_crawler(cls, crawler)`:
   - è¿™æ˜¯ä¸€ä¸ªç±»æ–¹æ³•ï¼Œç”¨äºåˆ›å»ºPipelineå®ä¾‹ã€‚
   - å‚æ•°`crawler`æ˜¯Crawlerå¯¹è±¡ï¼Œå¯ä»¥ä»ä¸­è®¿é—®Scrapyçš„é…ç½®ä»¥åŠå…¶ä»–æ ¸å¿ƒç»„ä»¶ï¼Œå¦‚ä¿¡å·ï¼ˆsignalsï¼‰ã€‚
   - é€šå¸¸ç”¨äºé€šè¿‡`crawler.settings`è·å–Scrapyè®¾ç½®æ¥é…ç½®Pipelineã€‚

ç®¡é“ä¸­çš„å‡½æ•°åï¼ˆæ¯”å¦‚`process_item`, `open_spider`, `close_spider`ï¼‰éµå¾ªScrapyçš„é¢„å®šä¹‰çº¦å®šã€‚Scrapyæ¡†æ¶åœ¨è¿è¡Œæ—¶ä¼šè‡ªåŠ¨è°ƒç”¨è¿™äº›æ–¹æ³•ï¼Œå› æ­¤å¿…é¡»æŒ‰ç…§çº¦å®šå®šä¹‰å®ƒä»¬ï¼Œä»¥ç¡®ä¿Scrapyæ¡†æ¶å¯ä»¥æ­£ç¡®åœ°ä¸ä½ çš„Pipelineäº¤äº’ã€‚<br>

åœ¨ç¬”è€…çš„`MySQLPipeline`ç±»ä¸­ï¼Œ`open_spider`æ–¹æ³•ç”¨äºåœ¨çˆ¬è™«å¼€å§‹æ—¶è¿æ¥MySQLæ•°æ®åº“ï¼Œå¹¶åˆ›å»ºè¡¨ï¼ˆå¦‚æœè¡¨ä¸å­˜åœ¨ï¼‰ã€‚`process_item`æ–¹æ³•åˆ™ç”¨äºå¤„ç†æ¯ä¸ªitemï¼Œå¹¶å°†å…¶å­˜å‚¨åˆ°MySQLæ•°æ®åº“ä¸­ã€‚æœ€åï¼Œ`close_spider`æ–¹æ³•åœ¨çˆ¬è™«å…³é—­æ—¶æ–­å¼€æ•°æ®åº“è¿æ¥ã€‚è¿™æ ·çš„ç»“æ„ä½¿å¾—æ•°æ®å¤„ç†é€»è¾‘æ¸…æ™°ä¸”æ˜“äºç»´æŠ¤ã€‚<br>

### settings.py æ·»åŠ æ•°æ®åº“é…ç½®:

æ‰¾åˆ°å½“å‰Scrapyé¡¹ç›®æ–‡ä»¶å¤¹ä¸‹çš„ `settings.py` æ–‡ä»¶ï¼Œç„¶åå†™å…¥ä»¥ä¸‹å†…å®¹:<br>

> å¯ä»¥ä»»æ„æ‰¾ä¸€ä¸ªä½ç½®å†™å…¥ä¸‹åˆ—å†…å®¹ï¼Œä¹Ÿå¯ä»¥æ‰§è¡Œå‰å…ˆæœç´¢å®˜æ–¹æ˜¯å¦æœ‰æ³¨é‡Šéƒ¨åˆ†ï¼Œå†™å…¥ç›¸å…³éƒ¨åˆ†ã€‚

```conf
# MySQLæ•°æ®åº“è¿æ¥é…ç½®
MYSQL_HOST = '8.140.203.xxx'
MYSQL_DBNAME = 'irmdata'
MYSQL_USER = 'root'
MYSQL_PASSWORD = 'Flameaway3.'
MYSQL_PORT = 3306  # é»˜è®¤MySQLç«¯å£æ˜¯3306
```

### settings.py æ·»åŠ pipelinesæ‰§è¡Œä¼˜å…ˆçº§:

ğŸš¨å¿…é¡»åœ¨ `settings.py` ä¸­æ·»åŠ  `pipelines.py` å®šçš„ä¼˜å…ˆçº§æ‰èƒ½åœ¨æ‰§è¡Œæ•°æ®çˆ¬å–åï¼Œæ‰§è¡Œè‡ªå·±çš„ pipelinesã€‚<br>

æ ¹æ®è‡ªå·±å®šä¹‰çš„pipelinesçš„æ–‡ä»¶åï¼Œåœ¨ `settings.py` ä¸­æ·»åŠ ç±»ä¼¼å¦‚ä¸‹å†…å®¹:<br>

```conf
# å¼€å¯è‡ªå·±çš„pipeline
ITEM_PIPELINES = {
   "mdnice.pipelines.MySQLPipeline": 300,
}
```

## è¿è¡Œæ–¹å¼:

ç°åœ¨ä¸€åˆ‡å‡†å¤‡å°±ç»ªï¼Œå¯ä»¥ç»ˆç«¯æ‰§è¡Œä»¥ä¸‹æŒ‡ä»¤è¿›è¡Œæ•°æ®çˆ¬å–æ“ä½œ:<br>

```bash
scrapy crawl mdnice
```

æ•°æ®çˆ¬å–åè‡ªåŠ¨å°†æ•°æ®å†™å…¥MySQLï¼Œæ•ˆæœå¦‚ä¸‹:<br>

![](./mysql_result.jpg)