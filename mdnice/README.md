# mdniceç½‘å€å†…å®¹çˆ¬å–:



## å¤åˆ¶å®Œæ•´çš„HTML(Macç‰ˆã€ä»¥Chromeä¸¾ä¾‹):

1. æ‰“å¼€ä½ æƒ³è¦å¤åˆ¶HTMLçš„ç½‘é¡µã€‚

2. ä½¿ç”¨å¿«æ·é”® `Option + Command + I` æ‰“å¼€ **å¼€å‘è€…å·¥å…·**ã€‚

3. åœ¨å¼€å‘è€…å·¥å…·ä¸­ï¼Œä½ ä¼šçœ‹åˆ°HTMLæºä»£ç çš„æ ‘å½¢ç»“æ„ã€‚ä¸€èˆ¬æƒ…å†µä¸‹ï¼Œ<html>æ ‡ç­¾ä¼šæ˜¯è¿™æ£µæ ‘çš„æ ¹èŠ‚ç‚¹ã€‚

4. å³é”®ç‚¹å‡»<html>æ ‡ç­¾ï¼Œé€‰æ‹©â€œCopyâ€>Copy outerHTMLã€‚

è¿™æ ·å°±ä¼šå¤åˆ¶æ•´ä¸ªç½‘é¡µçš„HTMLä»£ç åˆ°å‰ªè´´æ¿ä¸­ã€‚<br>


## çˆ¬å–éœ€è¦çš„ç»“æœ:

```python
import scrapy
import json
from datetime import datetime, timedelta, timezone

class MdniceSpider(scrapy.Spider):
    name = 'mdnice'
    start_urls = ['https://www.mdnice.com/']

    def parse(self, response):
        json_data = response.xpath('//script[@id="__NEXT_DATA__"]/text()').get()
        data = json.loads(json_data)

        articles = data['props']['pageProps']['list']
        for article in articles:
            out_id = article['outId']
            title = article['title']
            url = f"https://www.mdnice.com/writing/{out_id}"
            create_time_str = article['createTime']

            # è§£æISO 8601æ ¼å¼æ—¶é—´å­—ç¬¦ä¸²ï¼ŒåŒ…æ‹¬æ—¶åŒºä¿¡æ¯
            create_time = datetime.strptime(create_time_str, "%Y-%m-%dT%H:%M:%S.%f%z")

            # è·å–å½“å‰æ—¶é—´ï¼ˆè€ƒè™‘åˆ°æ—¶åŒºï¼‰
            current_time = datetime.now(timezone.utc)

            # è®¡ç®—æ—¶é—´å·®
            time_difference = current_time - create_time

            # åˆ¤æ–­æ–‡ç« æ˜¯å¦åœ¨è¿‡å»2å°æ—¶å†…åˆ›å»º
            if time_difference <= timedelta(hours=20):
                # ä½¿ç”¨metaä¼ é€’é¢å¤–æ•°æ®
                yield scrapy.Request(url, callback=self.parse_article, meta={'title': title, 'create_time': create_time_str})

    def parse_article(self, response):
        # ä»metaä¸­æå–ä¼ é€’çš„æ•°æ®
        title = response.meta['title']
        create_time = response.meta['create_time']

        # æå–é˜…è¯»æ¬¡æ•°
        script = response.xpath("//script[@id='__NEXT_DATA__']/text()").get()
        # è§£æ JSON æ•°æ®
        data = json.loads(script)
        # æå– readingNum å­—æ®µ
        reading_num = data["props"]["pageProps"]["writingDetail"]["readingNum"]
        
        yield {
            'title': title,
            'create_time': create_time,
            'url': response.url,
            'read_count': reading_num
        }
```

## ç¤¼è²Œçˆ¬å–:

é€šå¸¸ï¼Œä½ éœ€è¦æ›´ç¤¼è²Œçš„çˆ¬å–æ•°æ®ï¼Œæ‰èƒ½ä¸è¢«ç¦æ‰IPã€é™åˆ¶è®¿é—®ã€‚å¯ä»¥å‚è€ƒä»¥ä¸‹å†™æ³•ã€‚<br>

### è®¾ç½®å›ºå®šçš„ä¸‹è½½å»¶è¿Ÿï¼ˆDOWNLOAD_DELAYï¼‰ï¼š

åœ¨ `settings.py` æ–‡ä»¶ä¸­æ·»åŠ ä»¥ä¸‹å†…å®¹ï¼Œè¿™ç§æ–¹æ³•é€‚ç”¨äºä½ æƒ³è¦åœ¨æ¯æ¬¡è¯·æ±‚ä¹‹é—´å¼ºåˆ¶ç­‰å¾…å›ºå®šæ—¶é—´çš„æƒ…å†µï¼Œä¾‹å¦‚æ¯2ç§’å‘é€ä¸€ä¸ªè¯·æ±‚ã€‚è¿™æ ·åšå¯ä»¥ç®€å•ç²—æš´åœ°é™åˆ¶è¯·æ±‚é€Ÿåº¦ï¼Œå‡å°‘å¯¹ç›®æ ‡ç½‘ç«™çš„å‹åŠ›ã€‚<br>

```conf
# settings.py

# è®¾ç½®ä¸‹è½½å»¶è¿Ÿä¸º2ç§’
DOWNLOAD_DELAY = 2
```

### å¯ç”¨è‡ªåŠ¨é™é€Ÿï¼ˆAUTOTHROTTLEï¼‰ï¼š

åœ¨ `settings.py` æ–‡ä»¶ä¸­æ·»åŠ ä»¥ä¸‹å†…å®¹ï¼Œè¿™ç§æ–¹æ³•æ›´ä¸ºæ™ºèƒ½ï¼Œå®ƒä¼šæ ¹æ®æœåŠ¡å™¨çš„å“åº”æ—¶é—´åŠ¨æ€è°ƒæ•´è¯·æ±‚é—´çš„å»¶è¿Ÿï¼Œä»¥æ‰¾åˆ°æœ€ä¼˜çš„çˆ¬å–é€Ÿåº¦ã€‚è¿™ä¸ä»…å¯ä»¥ä¿æŠ¤ç›®æ ‡ç½‘ç«™ä¸è¢«è¿‡åº¦è¯·æ±‚å½±å“ï¼Œè¿˜å¯ä»¥åœ¨å…è®¸çš„æƒ…å†µä¸‹æé«˜çˆ¬è™«çš„æ•ˆç‡ã€‚<br>

```conf
# settings.py

# å¯ç”¨è‡ªåŠ¨é™é€Ÿ
AUTOTHROTTLE_ENABLED = True
# åˆå§‹ä¸‹è½½å»¶è¿Ÿ
AUTOTHROTTLE_START_DELAY = 2
# æœ€å¤§ä¸‹è½½å»¶è¿Ÿï¼Œé˜²æ­¢å»¶è¿Ÿè¿‡é•¿
AUTOTHROTTLE_MAX_DELAY = 60
# å¯ç”¨æ˜¾ç¤ºè‡ªåŠ¨é™é€Ÿçš„è°ƒè¯•ä¿¡æ¯
AUTOTHROTTLE_DEBUG = True
```

çˆ¬å–æ•°æ®æ—¶ï¼Œå¯ä»¥å°†è¿™ä¸¤ä¸ªæ–¹æ¡ˆç»“åˆä½¿ç”¨ï¼Œé€šå¸¸ä¼šå¾—åˆ°æ›´å¥½çš„æ•ˆæœã€‚<br>

ç»“åˆä½¿ç”¨è¿™ä¸¤ç§æ–¹æ³•å¯ä»¥è®©ä½ çš„çˆ¬è™«æ—¢æœ‰ä¸€ä¸ªåŸºæœ¬çš„è¯·æ±‚é—´éš”ä¿éšœï¼ˆé€šè¿‡DOWNLOAD_DELAYï¼‰ï¼Œåˆèƒ½å¤Ÿæ™ºèƒ½åœ°è°ƒæ•´é€Ÿåº¦ä»¥åº”å¯¹ä¸åŒçš„æœåŠ¡å™¨å“åº”æƒ…å†µï¼ˆé€šè¿‡AUTOTHROTTLEï¼‰ã€‚<br>

ä¾‹å¦‚ï¼Œä½ å¯ä»¥è®¾ç½®ä¸€ä¸ªç›¸å¯¹è¾ƒå°çš„DOWNLOAD_DELAYä½œä¸ºåŸºçº¿ï¼Œç„¶åå¯ç”¨AUTOTHROTTLEæ¥å…è®¸Scrapyåœ¨è¿™ä¸ªåŸºçº¿çš„åŸºç¡€ä¸Šæ ¹æ®å®é™…çš„å“åº”æƒ…å†µè¿›è¡Œè°ƒæ•´ã€‚<br>

è¿™æ ·åšçš„å¥½å¤„æ˜¯ï¼Œå³ä½¿åœ¨æœåŠ¡å™¨å“åº”è¾ƒæ…¢æˆ–è€…ç½‘ç»œæ¡ä»¶å˜åŒ–çš„æƒ…å†µä¸‹ï¼Œä½ çš„çˆ¬è™«ä¹Ÿèƒ½å¤Ÿè‡ªé€‚åº”åœ°è°ƒæ•´è¯·æ±‚é€Ÿåº¦ï¼Œé¿å…å¯¹æœåŠ¡å™¨é€ æˆä¸å¿…è¦çš„è´Ÿæ‹…ï¼ŒåŒæ—¶å°½å¯èƒ½åœ°æé«˜çˆ¬å–æ•ˆç‡ã€‚<br>


## æ‰§è¡Œè‡ªå·±çš„pipelines:

æˆ‘ä»¬é€šå¸¸éœ€è¦å¯¹è‡ªå·±çˆ¬å–çš„æ•°æ®è¿›è¡Œä¸€äº›é¢å¤–çš„å¤„ç†ï¼Œä¾‹å¦‚å°†çˆ¬å–çš„ç»“æœå­˜å…¥MongoDBã€MySQLã€‚å­˜å…¥MongoDBçš„ç¤ºä¾‹å®˜æ–¹å·²ç»ç»™å‡ºï¼Œè¿™é‡Œä»¿ç…§å®˜æ–¹ç¤ºä¾‹å®ç°å°†çˆ¬å–çš„æ•°æ®å­˜å…¥MySQL:<br>

### ä¿®æ”¹ pipelines.py:

æ‰¾åˆ°å½“å‰Scrapyé¡¹ç›®æ–‡ä»¶å¤¹ä¸‹çš„ `pipelines.py` æ–‡ä»¶ï¼Œç„¶åå†™å…¥ä»¥ä¸‹å†…å®¹:<br>

```python
import pymysql
from itemadapter import ItemAdapter

import time

def current_timestamp():
    """è¿”å›å½“å‰æ—¥æœŸæ—¶é—´çš„å­—ç¬¦ä¸²è¡¨ç¤ºå½¢å¼,æ ¼å¼ä¸º: 2023-08-15 11:29:22 """
    return time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())

class MdnicePipeline:
    def process_item(self, item, spider):
        return item

class MySQLPipeline:
    """
    A pipeline to store the item in a MySQL database.
    """

    def __init__(self, mysql_host, mysql_db, mysql_user, mysql_password):
        self.mysql_host = mysql_host
        self.mysql_db = mysql_db
        self.mysql_user = mysql_user
        self.mysql_password = mysql_password

    @classmethod
    def from_crawler(cls, crawler):
        return cls(
            mysql_host=crawler.settings.get('MYSQL_HOST'),
            mysql_db=crawler.settings.get('MYSQL_DBNAME'),
            mysql_user=crawler.settings.get('MYSQL_USER'),
            mysql_password=crawler.settings.get('MYSQL_PASSWORD'),
        )

    def open_spider(self, spider):
        self.connection = pymysql.connect(host=self.mysql_host,
                                        user=self.mysql_user,
                                        password=self.mysql_password,
                                        database=self.mysql_db,
                                        charset='utf8mb4',
                                        cursorclass=pymysql.cursors.DictCursor)
        self.cursor = self.connection.cursor()

        # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º
        self.cursor.execute("""
            CREATE TABLE IF NOT EXISTS article_info (
                article_id INT AUTO_INCREMENT PRIMARY KEY COMMENT 'æ–‡ç« çš„å”¯ä¸€ID',
                article_title VARCHAR(255) NOT NULL COMMENT 'æ–‡ç« æ ‡é¢˜',
                article_read_count INT NOT NULL COMMENT 'æ–‡ç« é˜…è¯»æ•°',
                article_time VARCHAR(255) NOT NULL COMMENT 'æ–‡ç« å‘è¡¨æ—¶é—´',
                article_url VARCHAR(255) NOT NULL COMMENT 'æ–‡ç« å…·ä½“é“¾æ¥',
                create_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP COMMENT 'åˆ›å»ºæ—¶é—´',
                modify_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT 'ä¿®æ”¹æ—¶é—´'
            ) CHARSET=utf8mb4;
        """)
        self.connection.commit()

    def close_spider(self, spider):
        self.connection.close()

    def process_item(self, item, spider):
        # æ„å»ºæ’å…¥æ•°æ®çš„SQLè¯­å¥ï¼Œç°åœ¨åŒ…æ‹¬create_timeå’Œmodify_timeå­—æ®µ
        sql = """
        INSERT INTO article_info (article_title, article_read_count, article_time, article_url, create_time, modify_time) 
        VALUES (%s, %s, %s, %s, %s, %s)
        """
        # å‡†å¤‡æ’å…¥æ•°æ®çš„å€¼ï¼Œç¡®ä¿é¡ºåºä¸ä¸Šé¢çš„SQLè¯­å¥ä¸­çš„åˆ—å¯¹åº”
        values = (
            item.get('title'), 
            item.get('read_count'), 
            item.get('create_time'), 
            item.get('url'),
            current_timestamp(),  # ä½¿ç”¨å‡½æ•°ç”Ÿæˆæ’å…¥è®°å½•çš„å½“å‰æ—¶é—´
            current_timestamp(),  # åŒä¸Š
        )
        
        try:
            # æ‰§è¡ŒSQLè¯­å¥
            self.cursor.execute(sql, values)
            # æäº¤åˆ°æ•°æ®åº“æ‰§è¡Œ
            self.connection.commit()
        except pymysql.MySQLError as e:
            # å¦‚æœå‘ç”Ÿé”™è¯¯åˆ™å›æ»š
            print(e)
            self.connection.rollback()
        
        return item
```

### settings.py æ·»åŠ æ•°æ®åº“é…ç½®:

æ‰¾åˆ°å½“å‰Scrapyé¡¹ç›®æ–‡ä»¶å¤¹ä¸‹çš„ `settings.py` æ–‡ä»¶ï¼Œç„¶åå†™å…¥ä»¥ä¸‹å†…å®¹:<br>

> å¯ä»¥ä»»æ„æ‰¾ä¸€ä¸ªä½ç½®å†™å…¥ä¸‹åˆ—å†…å®¹ï¼Œä¹Ÿå¯ä»¥æ‰§è¡Œå‰å…ˆæœç´¢å®˜æ–¹æ˜¯å¦æœ‰æ³¨é‡Šéƒ¨åˆ†ï¼Œå†™å…¥ç›¸å…³éƒ¨åˆ†ã€‚

```conf
# MySQLæ•°æ®åº“è¿æ¥é…ç½®
MYSQL_HOST = '8.140.203.xxx'
MYSQL_DBNAME = 'irmdata'
MYSQL_USER = 'root'
MYSQL_PASSWORD = 'Flameaway3.'
MYSQL_PORT = 3306  # é»˜è®¤MySQLç«¯å£æ˜¯3306
```

### settings.py æ·»åŠ pipelinesæ‰§è¡Œä¼˜å…ˆçº§:

ğŸš¨å¿…é¡»åœ¨ `settings.py` ä¸­æ·»åŠ  `pipelines.py` å®šçš„ä¼˜å…ˆçº§æ‰èƒ½åœ¨æ‰§è¡Œæ•°æ®çˆ¬å–åï¼Œæ‰§è¡Œè‡ªå·±çš„ pipelinesã€‚<br>

æ ¹æ®è‡ªå·±å®šä¹‰çš„pipelinesçš„æ–‡ä»¶åï¼Œåœ¨ `settings.py` ä¸­æ·»åŠ ç±»ä¼¼å¦‚ä¸‹å†…å®¹:<br>

```conf
# å¼€å¯è‡ªå·±çš„pipeline
ITEM_PIPELINES = {
   "mdnice.pipelines.MySQLPipeline": 300,
}
```

## è¿è¡Œæ–¹å¼:

ç°åœ¨ä¸€åˆ‡å‡†å¤‡å°±ç»ªï¼Œå¯ä»¥ç»ˆç«¯æ‰§è¡Œä»¥ä¸‹æŒ‡ä»¤è¿›è¡Œæ•°æ®çˆ¬å–æ“ä½œ:<br>

```bash
scrapy crawl mdnice
```

æ•°æ®çˆ¬å–åè‡ªåŠ¨å°†æ•°æ®å†™å…¥MySQLï¼Œæ•ˆæœå¦‚ä¸‹:<br>

![](./mysql_xiaoguo.jpg)