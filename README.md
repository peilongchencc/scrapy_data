# Scrapy
- [Scrapy](#scrapy)
  - [Scrapyæ¦‚è§ˆ:](#scrapyæ¦‚è§ˆ)
  - [å®‰è£…æ–¹å¼:](#å®‰è£…æ–¹å¼)
    - [é€æ­¥è§£æä¸€ä¸ªçˆ¬å–å™¨:](#é€æ­¥è§£æä¸€ä¸ªçˆ¬å–å™¨)
    - [å…·ä½“å‘ç”Ÿäº†ä»€ä¹ˆå‘¢ï¼Ÿ](#å…·ä½“å‘ç”Ÿäº†ä»€ä¹ˆå‘¢)
    - [è¿˜æœ‰å‘¢ï¼Ÿ](#è¿˜æœ‰å‘¢)
    - [æ¥ä¸‹æ¥å‘¢ï¼Ÿ](#æ¥ä¸‹æ¥å‘¢)
  - [Scrapy æ•™ç¨‹:](#scrapy-æ•™ç¨‹)
    - [åˆ›å»ºä¸€ä¸ªæ–°çš„Scrapyé¡¹ç›®:](#åˆ›å»ºä¸€ä¸ªæ–°çš„scrapyé¡¹ç›®)
    - [æˆ‘ä»¬çš„ç¬¬ä¸€ä¸ªçˆ¬è™«:](#æˆ‘ä»¬çš„ç¬¬ä¸€ä¸ªçˆ¬è™«)
    - [æ€æ ·è¿è¡Œæˆ‘ä»¬çš„çˆ¬è™«:](#æ€æ ·è¿è¡Œæˆ‘ä»¬çš„çˆ¬è™«)
    - [åˆšæ‰åº•å±‚å‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿ(What just happened under the hood?)](#åˆšæ‰åº•å±‚å‘ç”Ÿäº†ä»€ä¹ˆwhat-just-happened-under-the-hood)
    - [start\_requestsæ–¹æ³•çš„å¿«æ·æ–¹å¼:](#start_requestsæ–¹æ³•çš„å¿«æ·æ–¹å¼)
    - [æå–æ•°æ®:](#æå–æ•°æ®)
    - [XPath: ç®€ä»‹(a brief intro):](#xpath-ç®€ä»‹a-brief-intro)
    - [æå–å¼•ç”¨å’Œä½œè€…(Extracting quotes and authors):](#æå–å¼•ç”¨å’Œä½œè€…extracting-quotes-and-authors)
    - [åœ¨æˆ‘ä»¬çš„çˆ¬è™«ç¨‹åºä¸­æå–æ•°æ®(Extracting data in our spider):](#åœ¨æˆ‘ä»¬çš„çˆ¬è™«ç¨‹åºä¸­æå–æ•°æ®extracting-data-in-our-spider)
    - [å°†æŠ“å–çš„æ•°æ®å­˜å‚¨èµ·æ¥(Storing the scraped data):](#å°†æŠ“å–çš„æ•°æ®å­˜å‚¨èµ·æ¥storing-the-scraped-data)
    - [è·Ÿéšé“¾æ¥(ç¿»é¡µ)(Following links"):](#è·Ÿéšé“¾æ¥ç¿»é¡µfollowing-links)
    - [åˆ›å»ºè¯·æ±‚çš„å¿«æ·æ–¹å¼(A shortcut for creating Requests):](#åˆ›å»ºè¯·æ±‚çš„å¿«æ·æ–¹å¼a-shortcut-for-creating-requests)
    - [æ›´å¤šçš„ä¾‹å­å’Œæ¨¡å¼(More examples and patterns):](#æ›´å¤šçš„ä¾‹å­å’Œæ¨¡å¼more-examples-and-patterns)
    - [ä½¿ç”¨spiderå‚æ•°(Using spider arguments):](#ä½¿ç”¨spiderå‚æ•°using-spider-arguments)
    - [ä¸‹ä¸€æ­¥(Next steps):](#ä¸‹ä¸€æ­¥next-steps)
  - [ç¤ºä¾‹(Examples):](#ç¤ºä¾‹examples)
  - [é€‰æ‹©å™¨(Selectors):](#é€‰æ‹©å™¨selectors)
    - [ä½¿ç”¨é€‰æ‹©å™¨(Using selectors):](#ä½¿ç”¨é€‰æ‹©å™¨using-selectors)
      - [æ„é€ é€‰æ‹©å™¨(Constructing selectors):](#æ„é€ é€‰æ‹©å™¨constructing-selectors)
      - [ä½¿ç”¨é€‰æ‹©å™¨(Using selectors):](#ä½¿ç”¨é€‰æ‹©å™¨using-selectors-1)
      - [æ‹“å±•-XPathè¯­æ³•è§£é‡Š:](#æ‹“å±•-xpathè¯­æ³•è§£é‡Š)
      - [æ‹“å±•--"::" é€‰æ‹© "ä¼ªå…ƒç´ " :](#æ‹“å±•---é€‰æ‹©-ä¼ªå…ƒç´ -)
    - [CSS é€‰æ‹©å™¨çš„æ‰©å±•(Extensions to CSS Selectors):](#css-é€‰æ‹©å™¨çš„æ‰©å±•extensions-to-css-selectors)
      - [æ‹“å±•-CSSä¸­ `#` ç”¨æ³•è§£æ:](#æ‹“å±•-cssä¸­--ç”¨æ³•è§£æ)
    - [åµŒå¥—é€‰æ‹©å™¨(Nesting selectors):](#åµŒå¥—é€‰æ‹©å™¨nesting-selectors)
  - [Item Pipeline(é¡¹ç›®ç®¡é“):](#item-pipelineé¡¹ç›®ç®¡é“)
    - [Writing your own item pipeline(ç¼–å†™ä½ è‡ªå·±çš„é¡¹ç›®ç®¡é“):](#writing-your-own-item-pipelineç¼–å†™ä½ è‡ªå·±çš„é¡¹ç›®ç®¡é“)
  - [htmlä¸­çš„ `href` æ˜¯ä»€ä¹ˆï¼Ÿ](#htmlä¸­çš„-href-æ˜¯ä»€ä¹ˆ)
  - [scrapyè¿›è¡Œçˆ¬è™«æ—¶ï¼Œä¸ºä»€ä¹ˆä½¿ç”¨yieldå…³é”®å­—ï¼Ÿ](#scrapyè¿›è¡Œçˆ¬è™«æ—¶ä¸ºä»€ä¹ˆä½¿ç”¨yieldå…³é”®å­—)
  - [åœ¨çˆ¬è™«ä¸­ï¼ŒCSSå’ŒXPathæ˜¯ä»€ä¹ˆå…³ç³»ï¼Ÿ](#åœ¨çˆ¬è™«ä¸­csså’Œxpathæ˜¯ä»€ä¹ˆå…³ç³»)
  - [åŠ¨æ€ç½‘é¡µçˆ¬å–:](#åŠ¨æ€ç½‘é¡µçˆ¬å–)
    - [é—®é¢˜æè¿°:](#é—®é¢˜æè¿°)
    - [è§£å†³æ–¹æ¡ˆ:](#è§£å†³æ–¹æ¡ˆ)
  - [çˆ¬å–é¼ æ ‡æ‚¬åœæ‰èƒ½æ˜¾ç¤ºçš„å†…å®¹:](#çˆ¬å–é¼ æ ‡æ‚¬åœæ‰èƒ½æ˜¾ç¤ºçš„å†…å®¹)
    - [é—®é¢˜æè¿°:](#é—®é¢˜æè¿°-1)
    - [è§£å†³æ–¹æ¡ˆ:](#è§£å†³æ–¹æ¡ˆ-1)
  - [scrapyå®æ“:](#scrapyå®æ“)
  - [HTMLç®€ä»‹:](#htmlç®€ä»‹)
    - [HTMLæ ‡ç­¾](#htmlæ ‡ç­¾)
    - [HTMLå…ƒç´ ](#htmlå…ƒç´ )
    - [HTMLå±æ€§](#htmlå±æ€§)
    - [å…³ç³»å’ŒåŒºåˆ«](#å…³ç³»å’ŒåŒºåˆ«)
  - [CSSå’ŒXPathè¯­æ³•:](#csså’Œxpathè¯­æ³•)
    - [CSSé€‰æ‹©å™¨:](#cssé€‰æ‹©å™¨)
    - [XPathé€‰æ‹©å™¨:](#xpathé€‰æ‹©å™¨)
    - [Scrapyä¸­çš„ä½¿ç”¨:](#scrapyä¸­çš„ä½¿ç”¨)
  - [CSS å’Œ XPath é€‰æ‹©å™¨çš„è·¯å¾„é—®é¢˜:](#css-å’Œ-xpath-é€‰æ‹©å™¨çš„è·¯å¾„é—®é¢˜)
    - [é—®é¢˜æè¿°:](#é—®é¢˜æè¿°-2)
    - [é—®é¢˜è§£ç­”:](#é—®é¢˜è§£ç­”)
    - [ä¸ºä»€ä¹ˆä¸éœ€è¦å®Œæ•´è·¯å¾„ï¼Ÿ](#ä¸ºä»€ä¹ˆä¸éœ€è¦å®Œæ•´è·¯å¾„)
    - [ä»€ä¹ˆæ—¶å€™è€ƒè™‘ä½¿ç”¨æ›´å…·ä½“çš„è·¯å¾„ï¼Ÿ](#ä»€ä¹ˆæ—¶å€™è€ƒè™‘ä½¿ç”¨æ›´å…·ä½“çš„è·¯å¾„)
    - [ç¤ºä¾‹](#ç¤ºä¾‹)
    - [ç»“è®º](#ç»“è®º)
  - [`allowed_domains` å’Œ `response.follow` ç”¨æ³•è§£æ:](#allowed_domains-å’Œ-responsefollow-ç”¨æ³•è§£æ)
    - [`allowed_domains`](#allowed_domains)
      - [ç¤ºä¾‹ä»£ç ](#ç¤ºä¾‹ä»£ç )
      - [ç–‘æƒ‘ç‚¹è§£ç­”:](#ç–‘æƒ‘ç‚¹è§£ç­”)
    - [`response.follow`](#responsefollow)
      - [ç¤ºä¾‹ä»£ç ](#ç¤ºä¾‹ä»£ç -1)


## Scrapyæ¦‚è§ˆ:

Scrapyæ˜¯ä¸€ä¸ªç”¨äºçˆ¬è¡Œç½‘ç«™å’Œæå–ç»“æ„åŒ–æ•°æ®çš„åº”ç”¨ç¨‹åºæ¡†æ¶ï¼Œå¯ç”¨äºå¹¿æ³›çš„æœ‰ç”¨åº”ç”¨ç¨‹åºï¼Œå¦‚æ•°æ®æŒ–æ˜ã€ä¿¡æ¯å¤„ç†æˆ–å†å²æ¡£æ¡ˆã€‚<br>

å°½ç®¡ Scrapy æœ€åˆæ˜¯ä¸º Web æŠ“å–è€Œè®¾è®¡çš„ï¼Œä½†æ˜¯å®ƒä¹Ÿå¯ä»¥ç”¨äºä½¿ç”¨ API (æ¯”å¦‚ Amazon Associates Web Services)æå–æ•°æ®ï¼Œæˆ–è€…ä½œä¸ºä¸€ä¸ªé€šç”¨çš„ Web çˆ¬è™«ã€‚<br>

## å®‰è£…æ–¹å¼:

condaå®‰è£…æ–¹å¼:<br>

```bash
conda install -c conda-forge scrapy
```

pipå®‰è£…æ–¹å¼:<br>

```bash
pip install Scrapy
```

ç¬”è€…å®‰è£…çš„Scrapyç‰ˆæœ¬`scrapy==2.11.0 `ï¼Œå‘å¸ƒäº2023å¹´9æœˆ18æ—¥ã€‚<br>


### é€æ­¥è§£æä¸€ä¸ªçˆ¬å–å™¨:

ä¸ºäº†å‘ä½ å±•ç¤º Scrapy å¸¦æ¥äº†ä»€ä¹ˆï¼Œæˆ‘ä»¬å°†é€šè¿‡ä¸€ä¸ª Scrapy Spider çš„ä¾‹å­ï¼Œä½¿ç”¨æœ€ç®€å•çš„æ–¹æ³•æ¥è¿è¡Œä¸€ä¸ª Spiderã€‚<br>

ä¸‹é¢æ˜¯ä¸€ä¸ªçˆ¬è™«çš„ä»£ç ï¼Œå®ƒå¯ä»¥ä»ç½‘ç«™ https://quotes.toscrape.com ä¸‹é¢çš„åˆ†é¡µä¸­æ‰¾åˆ°ä¸€äº›è‘—åçš„å¼•è¨€:<br>

```python
import scrapy


class QuotesSpider(scrapy.Spider):
    name = "quotes"
    start_urls = [
        "https://quotes.toscrape.com/tag/humor/",
    ]

    def parse(self, response):
        for quote in response.css("div.quote"):
            yield {
                "author": quote.xpath("span/small/text()").get(),
                "text": quote.css("span.text::text").get(),
            }

        next_page = response.css('li.next a::attr("href")').get()
        """
        å¦‚æœå­˜åœ¨ä¸‹ä¸€é¡µï¼Œåˆ™è‡ªåŠ¨åœ°ç»§ç»­è¯·æ±‚ä¸‹ä¸€é¡µçš„å†…å®¹ã€‚
        å½“ä¸‹ä¸€é¡µçš„å†…å®¹è¢«ä¸‹è½½åï¼Œç”¨åŒæ ·çš„`parse`æ–¹æ³•æ¥è§£æå’Œæå–ä¿¡æ¯ã€‚
        """
        if next_page is not None:
            yield response.follow(next_page, self.parse)
```

æŠŠå®ƒæ”¾åœ¨ä¸€ä¸ªæ–‡æœ¬æ–‡ä»¶ä¸­ï¼Œå°†å®ƒå‘½åä¸º`quotes_spider.py`ï¼Œç„¶åä½¿ç”¨ `runspider` å‘½ä»¤è¿è¡Œè¿™ä¸ªçˆ¬è™«:<br>

è¯·è¯¦ç»†è§£é‡Šä»¥ä¸‹ç»ˆç«¯æŒ‡ä»¤:

```bash
scrapy runspider quotes_spider.py -o quotes.jsonl
```

è¿™ä¸ªç»ˆç«¯å‘½ä»¤ä½¿ç”¨äº†Scrapyï¼Œä¸€ä¸ªå¿«é€Ÿçš„ã€é«˜å±‚æ¬¡çš„WebæŠ“å–å’Œç½‘ç»œçˆ¬è™«æ¡†æ¶ï¼Œç”¨äºPythonã€‚å‘½ä»¤çš„å„ä¸ªéƒ¨åˆ†è§£é‡Šå¦‚ä¸‹ï¼š<br>

1. **scrapy**: è¿™æ˜¯ä¸»å‘½ä»¤ï¼Œç”¨æ¥è°ƒç”¨Scrapyå·¥å…·ã€‚

2. **runspider**: æ˜¯Scrapyçš„ä¸€ä¸ªå‘½ä»¤ï¼Œç”¨äºç›´æ¥è¿è¡Œä¸€ä¸ªçˆ¬è™«ï¼Œè€Œä¸éœ€è¦é€šè¿‡åˆ›å»ºScrapyé¡¹ç›®æ¥è¿è¡Œã€‚è¿™å¯¹äºå¿«é€Ÿæµ‹è¯•æˆ–è€…è¿è¡Œå•ä¸ªçˆ¬è™«è„šæœ¬å¾ˆæœ‰ç”¨ã€‚

3. **quotes_spider.py**: è¿™æ˜¯ä¸€ä¸ªPythonè„šæœ¬æ–‡ä»¶ï¼Œå…¶ä¸­åŒ…å«äº†çˆ¬è™«çš„å®šä¹‰ã€‚åœ¨è¿™ä¸ªæ–‡ä»¶ä¸­ï¼Œä½ å°†å®šä¹‰çˆ¬è™«çš„åç§°ã€å¼€å§‹çš„URLã€å¦‚ä½•è·Ÿéšé“¾æ¥ä»¥åŠå¦‚ä½•è§£æé¡µé¢å†…å®¹ç­‰ã€‚è¿™ä¸ªæ–‡ä»¶åè¡¨æ˜è¿™ä¸ªçˆ¬è™«å¯èƒ½æ˜¯ç”¨æ¥ä»æŸäº›ç½‘ç«™æŠ“å–å¼•ç”¨æˆ–è€…åè¨€çš„ã€‚

4. **-o quotes.jsonl**: è¿™æ˜¯Scrapyå‘½ä»¤çš„ä¸€ä¸ªå‚æ•°ï¼Œç”¨æ¥æŒ‡å®šè¾“å‡ºæ–‡ä»¶ã€‚

- **-o**: è¿™ä¸ªé€‰é¡¹ç”¨äºæŒ‡å®šè¾“å‡ºæ–‡ä»¶çš„è·¯å¾„ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œçˆ¬å–çš„æ•°æ®å°†ä¼šè¢«è¾“å‡ºã€‚

- **quotes.jsonl**: è¿™æ˜¯è¾“å‡ºæ–‡ä»¶çš„åç§°å’Œæ ¼å¼ã€‚`.jsonl`æ˜¯JSON Linesçš„ç¼©å†™ï¼Œè¿™æ˜¯ä¸€ç§å­˜å‚¨ç»“æ„åŒ–æ•°æ®çš„æ–¹å¼ï¼Œå…¶ä¸­æ¯ä¸€è¡Œæ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„JSONå¯¹è±¡ã€‚ä¸æ ‡å‡†çš„JSONæ–‡ä»¶ç›¸æ¯”ï¼ŒJSON Linesæ–‡ä»¶æ›´å®¹æ˜“è¢«æµå¼å¤„ç†ï¼Œå› ä¸ºæ¯è¡Œéƒ½æ˜¯ç‹¬ç«‹çš„ã€‚

å½“ä½ è¿è¡Œè¿™ä¸ªå‘½ä»¤æ—¶ï¼ŒScrapyå°†ä¼šå¯åŠ¨å¹¶è¿è¡Œ`quotes_spider.py`è„šæœ¬ä¸­å®šä¹‰çš„çˆ¬è™«ï¼ŒæŠ“å–æ•°æ®ï¼Œå¹¶å°†ç»“æœä»¥JSON Linesæ ¼å¼ä¿å­˜åœ¨`quotes.jsonl`æ–‡ä»¶ä¸­ã€‚è¿™ä¸ªå‘½ä»¤é€‚åˆäºå¿«é€ŸæŠ“å–å’Œå­˜å‚¨æ•°æ®ï¼Œç‰¹åˆ«æ˜¯å½“ä½ ä¸éœ€è¦å®Œæ•´çš„Scrapyé¡¹ç›®ç»“æ„æ—¶ã€‚<br>


æœ‰æ—¶ï¼Œä½ è¿è¡Œå½“å‰scrapyæ–‡ä»¶æ—¶ï¼Œå¯èƒ½æç¤ºä»¥ä¸‹å†…å®¹ï¼Œæ­¤æ—¶è¿è¡Œ `pip install chardet` å³å¯:<br>

```txt
ImportError: cannot import name 'is_ascii' from 'charset_normalizer.utils' (/opt/anaconda3/envs/nazhi/lib/python3.10/site-packages/charset_normalizer/utils.py)
```


å®Œæˆåï¼Œåœ¨ `quotes.jsonl` æ–‡ä»¶ä¸­å°†æœ‰ä¸€ä¸ª JSON Lines æ ¼å¼çš„å¼•è¨€åˆ—è¡¨ï¼Œå…¶ä¸­åŒ…å«æ–‡æœ¬å’Œä½œè€…ï¼Œå¦‚ä¸‹æ‰€ç¤º:<br>

```json
{"author": "Jane Austen", "text": "\u201cThe person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.\u201d"}
{"author": "Steve Martin", "text": "\u201cA day without sunshine is like, you know, night.\u201d"}
{"author": "Garrison Keillor", "text": "\u201cAnyone who thinks sitting in church can make you a Christian must also think that sitting in a garage can make you a car.\u201d"}
...
```

### å…·ä½“å‘ç”Ÿäº†ä»€ä¹ˆå‘¢ï¼Ÿ

å½“ä½ è¿è¡Œå‘½ä»¤ `scrapy runspider quotes_spider.py` æ—¶ï¼ŒScrapy ä¼šå¯»æ‰¾å…¶ä¸­çš„ Spider å®šä¹‰å¹¶é€šè¿‡å…¶çˆ¬è™«å¼•æ“è¿è¡Œå®ƒã€‚<br>

çˆ¬å–è¿‡ç¨‹é¦–å…ˆå‘ `start_urls` å±æ€§ä¸­å®šä¹‰çš„ URLï¼ˆåœ¨æœ¬ä¾‹ä¸­ï¼Œåªæœ‰ **humor** (å¹½é»˜)ç±»åˆ«çš„å¼•è¨€çš„ URLï¼‰å‘å‡ºè¯·æ±‚ï¼Œ**å¹¶è°ƒç”¨é»˜è®¤çš„å›è°ƒæ–¹æ³• parse** ğŸ¤¨ğŸ¤¨ğŸ¤¨ï¼Œå°†å“åº”å¯¹è±¡ä½œä¸ºå‚æ•°ä¼ é€’ã€‚åœ¨ parse å›è°ƒä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ CSS é€‰æ‹©å™¨å¾ªç¯éå†å¼•è¯­å…ƒç´ ï¼Œç”Ÿæˆä¸€ä¸ªåŒ…å«æå–çš„å¼•è¯­æ–‡æœ¬å’Œä½œè€…çš„ Python å­—å…¸ï¼Œ**å¯»æ‰¾åˆ°ä¸‹ä¸€é¡µçš„é“¾æ¥**ğŸ³ğŸ³ğŸ³å¹¶ä½¿ç”¨ç›¸åŒçš„ parse æ–¹æ³•ä½œä¸ºå›è°ƒæ¥å®‰æ’å¦ä¸€ä¸ªè¯·æ±‚ã€‚<br>

åœ¨æ­¤ï¼Œä½ ä¼šæ³¨æ„åˆ° Scrapy çš„ä¸€ä¸ªä¸»è¦ä¼˜åŠ¿ï¼š**è¯·æ±‚è¢«å®‰æ’å’Œå¼‚æ­¥å¤„ç†**ã€‚è¿™æ„å‘³ç€ Scrapy ä¸éœ€è¦ç­‰å¾…ä¸€ä¸ªè¯·æ±‚å®Œæˆå¹¶å¤„ç†ï¼Œå®ƒå¯ä»¥åœ¨æ­¤æœŸé—´å‘é€å¦ä¸€ä¸ªè¯·æ±‚æˆ–åšå…¶ä»–äº‹æƒ…ã€‚è¿™ä¹Ÿæ„å‘³ç€ï¼Œå³ä½¿æŸäº›è¯·æ±‚å¤±è´¥æˆ–åœ¨å¤„ç†æ—¶å‡ºç°é”™è¯¯ï¼Œå…¶ä»–è¯·æ±‚ä¹Ÿå¯ä»¥ç»§ç»­è¿›è¡Œã€‚<br>

è™½ç„¶è¿™ä½¿ä½ èƒ½å¤Ÿè¿›è¡Œéå¸¸å¿«é€Ÿçš„çˆ¬å–ï¼ˆåŒæ—¶å‘é€å¤šä¸ªå¹¶å‘è¯·æ±‚ï¼Œä»¥å®¹é”™æ–¹å¼ï¼‰ï¼ŒScrapy ä¹Ÿé€šè¿‡ä¸€äº›è®¾ç½®ï¼ˆhttps://docs.scrapy.org/en/latest/topics/settings.html#topics-settings-refï¼‰è®©ä½ æ§åˆ¶çˆ¬å–çš„ç¤¼è²Œæ€§ã€‚ä½ å¯ä»¥åšè¯¸å¦‚è®¾ç½®æ¯ä¸ªè¯·æ±‚ä¹‹é—´çš„ä¸‹è½½å»¶è¿Ÿã€é™åˆ¶æ¯ä¸ªåŸŸæˆ–æ¯ä¸ª IP çš„å¹¶å‘è¯·æ±‚æ•°é‡ï¼Œç”šè‡³ä½¿ç”¨ä¸€ä¸ªè‡ªåŠ¨è°ƒèŠ‚æ‰©å±•æ¥å°è¯•è‡ªåŠ¨åœ°è§£å†³è¿™äº›é—®é¢˜ã€‚<br>

### è¿˜æœ‰å‘¢ï¼Ÿ

æ‚¨å·²äº†è§£å¦‚ä½•ä½¿ç”¨Scrapyä»ç½‘ç«™æå–å’Œå­˜å‚¨é¡¹ç›®ï¼Œä½†è¿™åªæ˜¯å†°å±±ä¸€è§’ã€‚Scrapyæä¾›äº†è®¸å¤šå¼ºå¤§åŠŸèƒ½ï¼Œä½¿å¾—æŠ“å–å˜å¾—ç®€å•é«˜æ•ˆï¼Œä¾‹å¦‚ï¼š

- å†…ç½®æ”¯æŒä½¿ç”¨æ‰©å±•çš„CSSé€‰æ‹©å™¨å’ŒXPathè¡¨è¾¾å¼ä»HTML/XMLæºé€‰æ‹©å’Œæå–æ•°æ®ï¼Œé…æœ‰è¾…åŠ©æ–¹æ³•é€šè¿‡æ­£åˆ™è¡¨è¾¾å¼æå–æ•°æ®ã€‚

- ä¸€ä¸ªäº¤äº’å¼shellæ§åˆ¶å°ï¼ˆæ”¯æŒIPythonï¼‰ï¼Œç”¨äºå°è¯•CSSå’ŒXPathè¡¨è¾¾å¼æ¥æŠ“å–æ•°æ®ï¼Œç¼–å†™æˆ–è°ƒè¯•æ‚¨çš„çˆ¬è™«æ—¶éå¸¸æœ‰ç”¨ã€‚

- å†…ç½®æ”¯æŒç”Ÿæˆå¤šç§æ ¼å¼ï¼ˆJSON, CSV, XMLï¼‰çš„feedå¯¼å‡ºï¼Œå¹¶èƒ½å°†å®ƒä»¬å­˜å‚¨åœ¨å¤šç§åç«¯ï¼ˆFTP, S3, æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿï¼‰ã€‚

- åšå®çš„ç¼–ç æ”¯æŒå’Œè‡ªåŠ¨æ£€æµ‹åŠŸèƒ½ï¼Œç”¨äºå¤„ç†å¤–æ¥çš„ã€éæ ‡å‡†çš„åŠæŸåçš„ç¼–ç å£°æ˜ã€‚

- å¼ºå¤§çš„å¯æ‰©å±•æ€§æ”¯æŒï¼Œå…è®¸æ‚¨æ’å…¥è‡ªå·±çš„åŠŸèƒ½ï¼Œä½¿ç”¨ä¿¡å·å’Œå®šä¹‰æ˜ç¡®çš„APIï¼ˆä¸­é—´ä»¶ã€æ‰©å±•å’Œç®¡é“ï¼‰ã€‚

- å¹¿æ³›çš„å†…ç½®æ‰©å±•å’Œä¸­é—´ä»¶ï¼Œç”¨äºå¤„ç†ï¼š
  - cookieå’Œä¼šè¯å¤„ç†
  - HTTPåŠŸèƒ½ï¼Œå¦‚å‹ç¼©ã€è®¤è¯ã€ç¼“å­˜
  - ç”¨æˆ·ä»£ç†ä¼ªè£…
  - robots.txt
  - çˆ¬è¡Œæ·±åº¦é™åˆ¶
  - ä»¥åŠæ›´å¤š

- ä¸€ä¸ªTelnetæ§åˆ¶å°ï¼Œç”¨äºè¿æ¥åˆ°è¿è¡Œåœ¨æ‚¨çš„Scrapyè¿›ç¨‹ä¸­çš„Pythonæ§åˆ¶å°ï¼Œä»¥ä¾¿å†…çœå’Œè°ƒè¯•æ‚¨çš„çˆ¬è™«ã€‚

- è¿˜æœ‰å…¶ä»–å¥½ä¸œè¥¿ï¼Œå¦‚å¯é‡ç”¨çš„çˆ¬è™«æ¥ä»ç«™ç‚¹åœ°å›¾å’ŒXML/CSVæºæŠ“å–ç½‘ç«™ï¼Œä¸€ä¸ªè‡ªåŠ¨ä¸‹è½½ä¸æŠ“å–é¡¹ç›®ç›¸å…³çš„å›¾åƒï¼ˆæˆ–ä»»ä½•å…¶ä»–åª’ä½“ï¼‰çš„åª’ä½“ç®¡é“ï¼Œä¸€ä¸ªç¼“å­˜DNSè§£æå™¨ï¼Œä»¥åŠæ›´å¤šï¼

### æ¥ä¸‹æ¥å‘¢ï¼Ÿ

ä½ æ¥ä¸‹æ¥çš„æ­¥éª¤æ˜¯å®‰è£…Scrapyï¼Œè·Ÿéšæ•™ç¨‹å­¦ä¹ å¦‚ä½•åˆ›å»ºä¸€ä¸ªå®Œæ•´çš„Scrapyé¡¹ç›®ï¼Œå¹¶åŠ å…¥ç¤¾åŒºã€‚æ„Ÿè°¢ä½ çš„å…³æ³¨ï¼


## Scrapy æ•™ç¨‹:

åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å‡è®¾ä½ çš„ç³»ç»Ÿä¸Šå·²ç»å®‰è£…äº†Scrapyã€‚å¦‚æœä¸æ˜¯è¿™æ ·ï¼Œè¯·å‚é˜…å®‰è£…æŒ‡å—ã€‚<br>

æˆ‘ä»¬å°†çˆ¬å– `quotes.toscrape.com` è¿™ä¸ªç½‘ç«™ï¼Œå®ƒåˆ—å‡ºäº†è‘—åä½œè€…çš„å¼•è¨€ã€‚<br>

æœ¬æ•™ç¨‹å°†æŒ‡å¯¼ä½ å®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼š<br>

1. åˆ›å»ºä¸€ä¸ªæ–°çš„Scrapyé¡¹ç›®ï¼›

2. ç¼–å†™ä¸€ä¸ªçˆ¬è™«æ¥æŠ“å–ç½‘ç«™å¹¶æå–æ•°æ®ï¼›

3. ä½¿ç”¨å‘½ä»¤è¡Œå¯¼å‡ºæŠ“å–çš„æ•°æ®ï¼›

4. ä¿®æ”¹çˆ¬è™«ä»¥é€’å½’è·Ÿéšé“¾æ¥ï¼›

5. ä½¿ç”¨çˆ¬è™«å‚æ•°ï¼›

Scrapyæ˜¯ç”¨Pythonç¼–å†™çš„ã€‚å¦‚æœä½ å¯¹è¿™é—¨è¯­è¨€ä¸ç†Ÿæ‚‰ï¼Œå¯èƒ½æƒ³å…ˆäº†è§£ä¸€ä¸‹è¯­è¨€çš„æ¦‚è²Œï¼Œä»¥ä¾¿æ›´å¥½åœ°åˆ©ç”¨Scrapyã€‚<br>

### åˆ›å»ºä¸€ä¸ªæ–°çš„Scrapyé¡¹ç›®:

åœ¨ä½ å¼€å§‹æŠ“å–æ•°æ®ä¹‹å‰ï¼Œä½ éœ€è¦è®¾ç½®ä¸€ä¸ªæ–°çš„Scrapyé¡¹ç›®ã€‚è¿›å…¥ä¸€ä¸ªä½ æƒ³å­˜æ”¾ä»£ç çš„ç›®å½•ï¼Œå¹¶è¿è¡Œï¼š<br>

```bash
scrapy startproject tutorial
```

è¿™å°†åˆ›å»ºä¸€ä¸ªåŒ…å«ä»¥ä¸‹å†…å®¹çš„ `tutorial` ç›®å½•:<br>

```txt
tutorial/
    scrapy.cfg            # deploy configuration file

    tutorial/             # project's Python module, you'll import your code from here
        __init__.py

        items.py          # project items definition file

        middlewares.py    # project middlewares file

        pipelines.py      # project pipelines file

        settings.py       # project settings file

        spiders/          # a directory where you'll later put your spiders
            __init__.py
```

å¹¶ä¸”ç»ˆç«¯æç¤ºä»¥ä¸‹å†…å®¹:<br>

```txt
New Scrapy project 'tutorial', using template directory '/opt/anaconda3/envs/nazhi/lib/python3.10/site-packages/scrapy/templates/project', created in:
    /Users/peilongchencc/Desktop/code_draft/tutorial

You can start your first spider with:
    cd tutorial
    scrapy genspider example example.com
```


### æˆ‘ä»¬çš„ç¬¬ä¸€ä¸ªçˆ¬è™«:

Spidersæ˜¯ä½ å®šä¹‰çš„ç±»ï¼ŒScrapyåˆ©ç”¨å®ƒä»¬ä»ä¸€ä¸ªç½‘ç«™ï¼ˆæˆ–ä¸€ç»„ç½‘ç«™ï¼‰ä¸ŠæŠ“å–ä¿¡æ¯ã€‚**å®ƒä»¬å¿…é¡»ç»§æ‰¿Spiderç±»ï¼Œå¹¶å®šä¹‰åˆå§‹è¯·æ±‚â€¼ï¸**ã€å¯é€‰åœ°å¦‚ä½•è·Ÿè¸ªé¡µé¢ä¸­çš„é“¾æ¥ï¼Œä»¥åŠå¦‚ä½•è§£æä¸‹è½½çš„é¡µé¢å†…å®¹ä»¥æå–æ•°æ®ã€‚<br>

è¿™æ˜¯æˆ‘ä»¬çš„ç¬¬ä¸€ä¸ªçˆ¬è™«ä»£ç ã€‚è¯·å°†å…¶ä¿å­˜åœ¨é¡¹ç›®çš„ `tutorial/spiders` ç›®å½•ä¸‹ï¼Œæ–‡ä»¶åä¸º `quotes_spider.py`ï¼š<br>

```python
from pathlib import Path

import scrapy


class QuotesSpider(scrapy.Spider):
    name = "quotes"

    def start_requests(self):
        urls = [
            "https://quotes.toscrape.com/page/1/",
            "https://quotes.toscrape.com/page/2/",
        ]
        for url in urls:
            yield scrapy.Request(url=url, callback=self.parse)

    def parse(self, response):
        page = response.url.split("/")[-2]
        filename = f"quotes-{page}.html"
        Path(filename).write_bytes(response.body)
        self.log(f"Saved file {filename}")
```

æ­£å¦‚æ‚¨æ‰€è§ï¼Œæˆ‘ä»¬çš„Spiderå­ç±»ç»§æ‰¿äº† `scrapy.Spider` å¹¶å®šä¹‰äº†ä¸€äº›å±æ€§å’Œæ–¹æ³•ï¼š<br>

- nameï¼šæ ‡è¯†Spiderã€‚å®ƒåœ¨ä¸€ä¸ªé¡¹ç›®ä¸­å¿…é¡»æ˜¯å”¯ä¸€çš„ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œæ‚¨ä¸èƒ½ä¸ºä¸åŒçš„Spiderè®¾ç½®ç›¸åŒçš„åç§°ã€‚ğŸš¨

- start_requests()ï¼šå¿…é¡»è¿”å›ä¸€ä¸ªè¯·æ±‚çš„å¯è¿­ä»£å¯¹è±¡ï¼ˆæ‚¨å¯ä»¥è¿”å›ä¸€ä¸ªè¯·æ±‚åˆ—è¡¨æˆ–ç¼–å†™ä¸€ä¸ªç”Ÿæˆå™¨å‡½æ•°ï¼‰ï¼ŒSpiderå°†ä»ä¸­å¼€å§‹çˆ¬å–ã€‚åç»­çš„è¯·æ±‚å°†ä¼šä¾æ¬¡ä»è¿™äº›åˆå§‹è¯·æ±‚ç”Ÿæˆã€‚

- parse()ï¼šè¿™ä¸ªæ–¹æ³•å°†è¢«è°ƒç”¨æ¥å¤„ç†ä¸ºæ¯ä¸ªè¯·æ±‚ä¸‹è½½çš„å“åº”ã€‚responseå‚æ•°æ˜¯ä¸€ä¸ª `TextResponse` å®ä¾‹ï¼Œå®ƒä¿å­˜äº†é¡µé¢å†…å®¹ï¼Œå¹¶æä¾›äº†è¿›ä¸€æ­¥å¤„ç†å®ƒçš„æœ‰ç”¨æ–¹æ³•ã€‚

é€šå¸¸ï¼Œparse()æ–¹æ³•ä¼šè§£æå“åº”ï¼Œæå–ä½œä¸ºå­—å…¸çš„çˆ¬å–æ•°æ®ï¼Œå¹¶ä¸”è¿˜ä¼šå¯»æ‰¾æ–°çš„URLæ¥è·Ÿè¿›ï¼Œå¹¶ä»ä¸­åˆ›å»ºæ–°çš„è¯·æ±‚ï¼ˆRequestï¼‰ã€‚<br>

### æ€æ ·è¿è¡Œæˆ‘ä»¬çš„çˆ¬è™«:

è¦ä½¿æˆ‘ä»¬çš„spideræ­£å¸¸å·¥ä½œï¼Œè¯·è½¬åˆ°é¡¹ç›®çš„é¡¶çº§ç›®å½•(æœ€ä¸Šå±‚çš„é‚£ä¸ª`tutorial`æ‰€åœ¨å±‚çº§)å¹¶è¿è¡Œï¼š<br>

```bash
scrapy crawl quotes
```

> å¦‚æœä¸Šé¢çš„ä»£ç ä¸­ `name = "ex"`ï¼Œåˆ™è¿è¡Œ `scrapy crawl ex`ã€‚

è¿™ä¸ªå‘½ä»¤ä¼šè¿è¡Œæˆ‘ä»¬åˆšæ·»åŠ çš„åä¸º `quotes` çš„çˆ¬è™«ï¼Œå®ƒä¼šå‘ `quotes.toscrape.com` åŸŸåå‘é€ä¸€äº›è¯·æ±‚ã€‚ä½ å°†å¾—åˆ°ç±»ä¼¼ä»¥ä¸‹çš„è¾“å‡ºï¼š<br>

```txt
... (omitted for brevity)
2016-12-16 21:24:05 [scrapy.core.engine] INFO: Spider opened
2016-12-16 21:24:05 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2016-12-16 21:24:05 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2016-12-16 21:24:05 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://quotes.toscrape.com/robots.txt> (referer: None)
2016-12-16 21:24:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://quotes.toscrape.com/page/1/> (referer: None)
2016-12-16 21:24:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://quotes.toscrape.com/page/2/> (referer: None)
2016-12-16 21:24:05 [quotes] DEBUG: Saved file quotes-1.html
2016-12-16 21:24:05 [quotes] DEBUG: Saved file quotes-2.html
2016-12-16 21:24:05 [scrapy.core.engine] INFO: Closing spider (finished)
...
```

ç°åœ¨ï¼Œè¯·æ£€æŸ¥å½“å‰ç›®å½•ä¸­çš„æ–‡ä»¶ã€‚æ‚¨åº”è¯¥ä¼šæ³¨æ„åˆ°æœ‰ä¸¤ä¸ªæ–°æ–‡ä»¶è¢«åˆ›å»ºäº†ï¼š`quotes-1.html` å’Œ `quotes-2.html`ï¼Œå®ƒä»¬åŒ…å«äº†å„è‡ªç½‘å€çš„å†…å®¹ï¼Œæ­£å¦‚æˆ‘ä»¬çš„ `parse` æ–¹æ³•æ‰€æŒ‡ç¤ºçš„ã€‚<br>

> å¦‚æœä½ æƒ³çŸ¥é“ä¸ºä»€ä¹ˆæˆ‘ä»¬è¿˜æ²¡æœ‰è§£æHTMLï¼Œåˆ«æ€¥ï¼Œæˆ‘ä»¬å¾ˆå¿«å°±ä¼šè®²åˆ°è¿™ä¸ªã€‚


### åˆšæ‰åº•å±‚å‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿ(What just happened under the hood?)

Scrapyå®‰æ’ç”±Spiderçš„ `start_requests` æ–¹æ³•è¿”å›çš„ `scrapy.Request` å¯¹è±¡ã€‚æ¯æ”¶åˆ°ä¸€ä¸ªçš„å“åº”åï¼Œå®ƒå°±ä¼šå®ä¾‹åŒ– `Response` å¯¹è±¡å¹¶è°ƒç”¨ä¸è¯·æ±‚ç›¸å…³è”çš„å›è°ƒæ–¹æ³•ï¼ˆåœ¨è¿™ç§æƒ…å†µä¸‹æ˜¯ `parse` æ–¹æ³•ï¼‰ï¼ŒåŒæ—¶ä¼ é€’å“åº”ä½œä¸ºå‚æ•°ã€‚<br>

### start_requestsæ–¹æ³•çš„å¿«æ·æ–¹å¼:

ä½ å¯ä»¥ä¸ç”¨å®ç°ä¸€ä¸ªç”Ÿæˆ `scrapy.Request` å¯¹è±¡çš„ `start_requests()` æ–¹æ³•ï¼Œè€Œæ˜¯ç›´æ¥å®šä¹‰ä¸€ä¸ªåŒ…å«URLåˆ—è¡¨çš„ `start_urls` ç±»å±æ€§ã€‚è¿™ä¸ªåˆ—è¡¨éšåå°†è¢« `start_requests()` çš„é»˜è®¤å®ç°æ‰€ä½¿ç”¨ï¼Œä»¥åˆ›å»ºä½ çš„çˆ¬è™«çš„åˆå§‹è¯·æ±‚ã€‚<br>

```python
from pathlib import Path

import scrapy


class QuotesSpider(scrapy.Spider):
    name = "quotes"
    start_urls = [
        "https://quotes.toscrape.com/page/1/",
        "https://quotes.toscrape.com/page/2/",
    ]

    def parse(self, response):
        page = response.url.split("/")[-2]
        filename = f"quotes-{page}.html"
        Path(filename).write_bytes(response.body)
```

`parse()` æ–¹æ³•å°†è¢«è°ƒç”¨æ¥å¤„ç†å¯¹è¿™äº›URLçš„æ¯ä¸ªè¯·æ±‚ï¼Œå³ä½¿æˆ‘ä»¬æ²¡æœ‰æ˜ç¡®å‘Šè¯‰Scrapyè¿™æ ·åšã€‚è¿™æ˜¯å› ä¸º `parse()` æ˜¯Scrapyçš„é»˜è®¤å›è°ƒæ–¹æ³•ï¼Œå¯¹äºæ²¡æœ‰æ˜ç¡®åˆ†é…å›è°ƒçš„è¯·æ±‚ï¼Œå°†è‡ªåŠ¨è°ƒç”¨æ­¤æ–¹æ³•ã€‚<br>

### æå–æ•°æ®:

è¦å­¦ä¹ å¦‚ä½•ä½¿ç”¨Scrapyæå–æ•°æ®çš„æœ€ä½³æ–¹å¼æ˜¯å°è¯•ä½¿ç”¨Scrapy shellæ¥è¿è¡Œé€‰æ‹©å™¨ã€‚è¿è¡Œï¼š<br>

```bash
scrapy shell 'https://quotes.toscrape.com/page/1/'
```

å½“ä»å‘½ä»¤è¡Œè¿è¡ŒScrapy shellæ—¶ï¼Œ**è¯·è®°å¾—å§‹ç»ˆå°†ç½‘å€ç”¨å¼•å·æ‹¬èµ·æ¥**ï¼Œå¦åˆ™å«æœ‰å‚æ•°ï¼ˆå³åŒ…å«&å­—ç¬¦ï¼‰çš„ç½‘å€å°†æ— æ³•å·¥ä½œã€‚åœ¨Windowsä¸Šï¼Œè¯·ä½¿ç”¨åŒå¼•å·ï¼Œä¾‹å¦‚ï¼š<br>

```bash
scrapy shell "https://quotes.toscrape.com/page/1/"
```

ä½ å°†çœ‹åˆ°ä»¥ä¸‹å†…å®¹ï¼š<br>

```txt
[ ... Scrapy log here ... ]
2016-09-19 12:09:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://quotes.toscrape.com/page/1/> (referer: None)
[s] Available Scrapy objects:
[s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)
[s]   crawler    <scrapy.crawler.Crawler object at 0x7fa91d888c90>
[s]   item       {}
[s]   request    <GET https://quotes.toscrape.com/page/1/>
[s]   response   <200 https://quotes.toscrape.com/page/1/>
[s]   settings   <scrapy.settings.Settings object at 0x7fa91d888c10>
[s]   spider     <DefaultSpider 'default' at 0x7fa91c8af990>
[s] Useful shortcuts:
[s]   shelp()           Shell help (print this help)
[s]   fetch(req_or_url) Fetch request (or URL) and update local objects
[s]   view(response)    View response in a browser
```

åœ¨Shellä¸­ï¼Œæ‚¨å¯ä»¥å°è¯•ä½¿ç”¨CSSåŠå“åº”å¯¹è±¡æ¥é€‰æ‹©å…ƒç´ (selecting elements)ï¼š<br>

```bash
>>> response.css("title")
[<Selector query='descendant-or-self::title' data='<title>Quotes to Scrape</title>'>]
```

è¿è¡Œ `response.css('title')` çš„ç»“æœæ˜¯ä¸€ä¸ªç±»ä¼¼åˆ—è¡¨çš„å¯¹è±¡ï¼Œåä¸º `SelectorList` ã€‚å®ƒä»£è¡¨äº†ä¸€ç³»åˆ—çš„ `Selector` å¯¹è±¡ï¼Œè¿™äº›å¯¹è±¡å›´ç»•ç€ `XML/HTML` å…ƒç´ ï¼Œå¹¶**å…è®¸ä½ è¿›ä¸€æ­¥æŸ¥è¯¢ä»¥ç»†åŒ–é€‰æ‹©æˆ–æå–æ•°æ®**ã€‚è¦ä»ä¸Šè¿°æ ‡é¢˜ä¸­æå–æ–‡æœ¬ï¼Œä½ å¯ä»¥è¿™æ ·åšï¼šğŸš€<br>

```bash
>>> response.css("title::text").getall()
['Quotes to Scrape']
```

There are two things to note here: one is that weâ€™ve added ::text to the CSS query, to mean we want to select only the text elements directly inside <title> element. If we donâ€™t specify ::text, weâ€™d get the full title element, including its tags:

è¿™é‡Œæœ‰ä¸¤ç‚¹éœ€è¦æ³¨æ„ï¼šä¸€æ˜¯æˆ‘ä»¬åœ¨**CSSæŸ¥è¯¢**ä¸­æ·»åŠ äº† `::text` ï¼Œæ„å‘³ç€æˆ‘ä»¬åªæƒ³é€‰æ‹© `<title>` å…ƒç´ å†…éƒ¨çš„**æ–‡æœ¬å…ƒç´ **ğŸ¤¨ã€‚å¦‚æœæˆ‘ä»¬ä¸æŒ‡å®š `::text`ï¼Œæˆ‘ä»¬å°†å¾—åˆ°å®Œæ•´çš„æ ‡é¢˜å…ƒç´ ï¼ŒåŒ…æ‹¬å®ƒçš„æ ‡ç­¾ï¼š<br>

```bash
>>> response.css("title").getall()
['<title>Quotes to Scrape</title>']
```

å¦å¤–ä¸€ç‚¹æ˜¯è°ƒç”¨ `.getall()` çš„ç»“æœæ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼šé€‰æ‹©å™¨å¯èƒ½è¿”å›å¤šäºä¸€ä¸ªç»“æœï¼Œæ‰€ä»¥æˆ‘ä»¬æå–å®ƒä»¬æ‰€æœ‰ã€‚**å½“ä½ çŸ¥é“ä½ åªæƒ³è¦ç¬¬ä¸€ä¸ªç»“æœæ—¶**ï¼Œå°±åƒåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä½ å¯ä»¥è¿™ä¹ˆåšï¼š<br>

```bash
>>> response.css("title::text").get()
'Quotes to Scrape'
```

ä½œä¸ºæ›¿ä»£ï¼Œä½ ä¹Ÿå¯ä»¥å†™æˆ:<br>

```bash
>>> response.css("title::text")[0].get()
'Quotes to Scrape'
```

è®¿é—® `SelectorList` å®ä¾‹ä¸Šçš„ç´¢å¼•ï¼Œ**å¦‚æœæ²¡æœ‰ç»“æœ**å°†ä¼šå¼•å‘ `IndexError` å¼‚å¸¸ã€‚<br>

```bash
>>> response.css("noelement")[0].get()
Traceback (most recent call last):
...
IndexError: list index out of range
```

You might want to use .get() directly on the SelectorList instance instead, which returns None if there are no results:

ä½ å¯èƒ½æƒ³è¦ç›´æ¥åœ¨SelectorListå®ä¾‹ä¸Šä½¿ç”¨ `.get()`ï¼Œå¦‚æœæ²¡æœ‰ç»“æœçš„è¯ï¼Œå®ƒä¼šè¿”å› `None`ï¼š<br>

> ç›´æ¥è·³åˆ°ä¸‹ä¸€è¡Œï¼Œä»€ä¹ˆä¹Ÿä¸æ˜¾ç¤ºã€‚

```bash
>>> response.css("noelement").get()
>>>
```

è¿™é‡Œæœ‰ä¸€ä¸ªå€¼å¾—å­¦ä¹ çš„åœ°æ–¹ï¼šå¯¹äºå¤§å¤šæ•°çˆ¬è™«ä»£ç ï¼Œä½ å¸Œæœ›å®ƒèƒ½å¤Ÿå¯¹äºé¡µé¢ä¸Šæ‰¾ä¸åˆ°å†…å®¹è€Œå¯¼è‡´çš„é”™è¯¯å…·æœ‰éŸ§æ€§ï¼Œè¿™æ ·å³ä½¿æŸäº›éƒ¨åˆ†æ— æ³•çˆ¬å–ï¼Œä½ è‡³å°‘èƒ½è·å–ä¸€äº›æ•°æ®ã€‚<br>

é™¤äº† `getall()` å’Œ `get()` æ–¹æ³•ä¹‹å¤–ï¼Œä½ è¿˜å¯ä»¥ä½¿ç”¨ `re()` æ–¹æ³•é€šè¿‡**æ­£åˆ™è¡¨è¾¾å¼**æ¥æå–æ•°æ®âœ…âœ…âœ…ï¼š<br>

```bash
>>> response.css("title::text").re(r"Quotes.*")
['Quotes to Scrape']
>>> response.css("title::text").re(r"Q\w+")
['Quotes']
>>> response.css("title::text").re(r"(\w+) to (\w+)")
['Quotes', 'Scrape']
```

ä¸ºäº†æ‰¾åˆ°åˆé€‚çš„CSSé€‰æ‹©å™¨ï¼Œä½ å¯èƒ½ä¼šå‘ç°åœ¨ä½ çš„ç½‘ç»œæµè§ˆå™¨ä¸­ä½¿ç”¨ `view(response)` å‘½ä»¤æ‰“å¼€å“åº”é¡µé¢å¾ˆæœ‰å¸®åŠ©ã€‚ä½ å¯ä»¥ä½¿ç”¨æµè§ˆå™¨çš„**å¼€å‘è€…å·¥å…·**æ£€æŸ¥HTMLï¼Œå¹¶åˆ¶å®šä¸€ä¸ªé€‰æ‹©å™¨ï¼ˆå‚è§ä½¿ç”¨ä½ çš„æµè§ˆå™¨çš„å¼€å‘è€…å·¥å…·è¿›è¡ŒæŠ“å–ï¼Œhttps://docs.scrapy.org/en/latest/topics/developer-tools.html#topics-developer-toolsï¼‰ã€‚<br>

**"Selector Gadget"** ä¹Ÿæ˜¯ä¸€ä¸ªå¾ˆå¥½çš„å·¥å…·ï¼Œå¯ä»¥å¿«é€Ÿä¸ºè§†è§‰é€‰å®šçš„å…ƒç´ æ‰¾åˆ°CSSé€‰æ‹©å™¨ï¼Œå®ƒåœ¨è®¸å¤šæµè§ˆå™¨ä¸­éƒ½èƒ½å·¥ä½œã€‚<br>


### XPath: ç®€ä»‹(a brief intro):

é™¤äº†CSSï¼ŒScrapyé€‰æ‹©å™¨è¿˜æ”¯æŒä½¿ç”¨XPathè¡¨è¾¾å¼ï¼š<br>

```bash
>>> response.xpath("//title")
[<Selector query='//title' data='<title>Quotes to Scrape</title>'>]
>>> response.xpath("//title/text()").get()
'Quotes to Scrape'
```

XPathè¡¨è¾¾å¼éå¸¸å¼ºå¤§ï¼Œå®ƒä»¬æ˜¯Scrapyé€‰æ‹©å™¨çš„åŸºç¡€ã€‚**å®é™…ä¸Šï¼ŒCSSé€‰æ‹©å™¨åœ¨åº•å±‚è¢«è½¬æ¢ä¸ºXPath**ã€‚å¦‚æœä½ ä»”ç»†é˜…è¯»shellä¸­é€‰æ‹©å™¨å¯¹è±¡çš„æ–‡æœ¬è¡¨ç¤ºï¼Œä½ å°±å¯ä»¥çœ‹åˆ°è¿™ä¸€ç‚¹ã€‚<br>

è™½ç„¶XPathè¡¨è¾¾å¼å¯èƒ½æ²¡æœ‰CSSé€‰æ‹©å™¨é‚£ä¹ˆæµè¡Œï¼Œä½†å®ƒä»¬æä¾›äº†æ›´å¤šçš„èƒ½åŠ›ï¼Œå› ä¸ºé™¤äº†å¯¼èˆªç»“æ„å¤–ï¼Œå®ƒè¿˜å¯ä»¥æŸ¥çœ‹å†…å®¹ã€‚ä½¿ç”¨XPathï¼Œä½ å¯ä»¥é€‰æ‹©è¯¸å¦‚ **â€œé€‰æ‹©åŒ…å«æ–‡æœ¬â€˜ä¸‹ä¸€é¡µâ€™çš„é“¾æ¥â€** ä¹‹ç±»çš„å…ƒç´ ã€‚è¿™ä½¿å¾—XPathéå¸¸é€‚åˆäºæŠ“å–ä»»åŠ¡ï¼Œå³ä½¿ä½ å·²ç»çŸ¥é“å¦‚ä½•æ„å»ºCSSé€‰æ‹©å™¨ï¼Œæˆ‘ä»¬ä¹Ÿé¼“åŠ±ä½ å­¦ä¹ XPathï¼Œå®ƒä¼šä½¿æŠ“å–å·¥ä½œå˜å¾—æ›´åŠ å®¹æ˜“ã€‚<br>

æˆ‘ä»¬è¿™é‡Œä¸ä¼šè¯¦ç»†ä»‹ç»XPathï¼Œä½†ä½ å¯ä»¥åœ¨è¿™é‡Œé˜…è¯»æ›´å¤šå…³äº[å¦‚ä½•ä½¿ç”¨Scrapyé€‰æ‹©å™¨çš„XPath]([é“¾æ¥çš„åœ°å€](https://docs.scrapy.org/en/latest/topics/selectors.html#topics-selectors))ã€‚ä¸ºäº†æ›´æ·±å…¥åœ°äº†è§£XPathï¼Œæˆ‘ä»¬æ¨èè¿™ä¸ªé€šè¿‡[ç¤ºä¾‹](https://zvon.org/comp/r/tut-XPath_1.html)å­¦ä¹ XPathçš„æ•™ç¨‹ï¼Œä»¥åŠè¿™ä¸ªå­¦ä¹ [â€œå¦‚ä½•ç”¨XPathæ€è€ƒâ€](http://plasmasturm.org/log/xpath101/)çš„æ•™ç¨‹ã€‚<br>




### æå–å¼•ç”¨å’Œä½œè€…(Extracting quotes and authors):

ç°åœ¨ä½ å·²ç»äº†è§£äº†é€‰æ‹©å’Œæå–çš„ä¸€äº›çŸ¥è¯†ï¼Œè®©æˆ‘ä»¬é€šè¿‡ç¼–å†™ä»£ç æ¥æå–ç½‘é¡µä¸Šçš„å¼•ç”¨æ¥å®Œæˆæˆ‘ä»¬çš„çˆ¬è™«ã€‚<br>

åœ¨ `https://quotes.toscrape.com` ä¸Šçš„æ¯ä¸ªå¼•ç”¨éƒ½æ˜¯ç”±è¿™æ ·çš„HTMLå…ƒç´ è¡¨ç¤ºçš„ï¼š<br>

```html
<div class="quote">
    <span class="text">â€œThe world as we have created it is a process of our
    thinking. It cannot be changed without changing our thinking.â€</span>
    <span>
        by <small class="author">Albert Einstein</small>
        <a href="/author/Albert-Einstein">(about)</a>
    </span>
    <div class="tags">
        Tags:
        <a class="tag" href="/tag/change/page/1/">change</a>
        <a class="tag" href="/tag/deep-thoughts/page/1/">deep-thoughts</a>
        <a class="tag" href="/tag/thinking/page/1/">thinking</a>
        <a class="tag" href="/tag/world/page/1/">world</a>
    </div>
</div>
```

è®©æˆ‘ä»¬æ‰“å¼€ `scrapy shell`ï¼Œç¨å¾®å°è¯•ä¸€ä¸‹ï¼Œæ‰¾å‡ºå¦‚ä½•æå–æˆ‘ä»¬æƒ³è¦çš„æ•°æ®ï¼š<br>

```bash
scrapy shell 'https://quotes.toscrape.com'
```

æˆ‘ä»¬é€šè¿‡ä»¥ä¸‹æ–¹å¼è·å–å¼•ç”¨HTMLå…ƒç´ çš„é€‰æ‹©å™¨åˆ—è¡¨ï¼š<br>

```bash
>>> response.css("div.quote")
[<Selector query="descendant-or-self::div[@class and contains(concat(' ', normalize-space(@class), ' '), ' quote ')]" data='<div class="quote" itemscope itemtype...'>,
<Selector query="descendant-or-self::div[@class and contains(concat(' ', normalize-space(@class), ' '), ' quote ')]" data='<div class="quote" itemscope itemtype...'>,
...]
```

ä½ å¯ä»¥ä½¿ç”¨ä¸‹åˆ—pythonä»£ç æ•´ç†ä¸€ä¸‹è·å–åˆ°çš„å†…å®¹çš„æ ¼å¼:<br>

```python
import re
import json

text_data = """
[<Selector query="descendant-or-self::div[@class and contains(concat(' ', normalize-space(@class), ' '), ' quote ')]" data='<div class="quote" itemscope itemtype...'>, <Selector query="descendant-or-self::div[@class and contains(concat(' ', normalize-space(@class), ' '), ' quote ')]" data='<div class="quote" itemscope itemtype...'>, <Selector query="descendant-or-self::div[@class and contains(concat(' ', normalize-space(@class), ' '), ' quote ')]" data='<div class="quote" itemscope itemtype...'>, <Select...
"""

# å®šä¹‰æ¨¡å¼ä»¥ä»æ¯ä¸ªé€‰æ‹©å™¨ä¸­æ•è·æŸ¥è¯¢å’Œæ•°æ®å­—æ®µ
pattern = r'<Selector query="([^"]+)" data=\'([^>]+)>'
matches = re.findall(pattern, text_data)

# å°†åŒ¹é…è½¬æ¢ä¸ºæ›´æ˜“è¯»çš„æ ¼å¼ï¼Œä¾‹å¦‚å­—å…¸åˆ—è¡¨
organized_data = [{'query': match[0], 'data': match[1]} for match in matches]

# å°†ç»„ç»‡å¥½çš„æ•°æ®è½¬æ¢ä¸ºæ ¼å¼åŒ–çš„å­—ç¬¦ä¸²
pretty_data = json.dumps(organized_data, indent=2)
print(pretty_data)
```

ç»ˆç«¯è¾“å‡ºå¦‚ä¸‹:<br>

```txt
[
  {
    "query": "descendant-or-self::div[@class and contains(concat(' ', normalize-space(@class), ' '), ' quote ')]",
    "data": "<div class=\"quote\" itemscope itemtype...'"
  },
  {
    "query": "descendant-or-self::div[@class and contains(concat(' ', normalize-space(@class), ' '), ' quote ')]",
    "data": "<div class=\"quote\" itemscope itemtype...'"
  },
  {
    "query": "descendant-or-self::div[@class and contains(concat(' ', normalize-space(@class), ' '), ' quote ')]",
    "data": "<div class=\"quote\" itemscope itemtype...'"
  }
]
```

ä¸Šè¿°æŸ¥è¯¢è¿”å›çš„æ¯ä¸ªé€‰æ‹©å™¨(Selector)éƒ½å…è®¸æˆ‘ä»¬å¯¹å…¶å­å…ƒç´ è¿›è¡Œè¿›ä¸€æ­¥çš„æŸ¥è¯¢ã€‚è®©æˆ‘ä»¬å°†ç¬¬ä¸€ä¸ªé€‰æ‹©å™¨èµ‹å€¼ç»™ä¸€ä¸ªå˜é‡ï¼Œè¿™æ ·æˆ‘ä»¬å°±å¯ä»¥ç›´æ¥åœ¨ç‰¹å®šå¼•è¯­ä¸Šè¿è¡Œæˆ‘ä»¬çš„CSSé€‰æ‹©å™¨ï¼š<br>

```bash
>>> quote = response.css("div.quote")[0]
```

ç°åœ¨ï¼Œè®©æˆ‘ä»¬ä½¿ç”¨åˆšåˆšåˆ›å»ºçš„å¼•ç”¨å¯¹è±¡æ¥æå–å¼•æ–‡çš„æ–‡æœ¬(`text`)ã€ä½œè€…(`author`)å’Œæ ‡ç­¾(`tags`)ï¼š<br>

```bash
>>> text = quote.css("span.text::text").get()
>>> text
'â€œThe world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.â€'
>>> author = quote.css("small.author::text").get()
>>> author
'Albert Einstein'
```


ç»™å®šæ ‡ç­¾(`tags`)æ˜¯**å­—ç¬¦ä¸²åˆ—è¡¨**ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `.getall()` æ–¹æ³•æ¥è·å–å®ƒä»¬æ‰€æœ‰çš„å†…å®¹ï¼š<br>

```bash
>>> tags = quote.css("div.tags a.tag::text").getall()
>>> tags
['change', 'deep-thoughts', 'thinking', 'world']
```

æˆ‘ä»¬å·²ç»å¼„æ¸…æ¥šäº†å¦‚ä½•æå–æ¯ä¸€ä¸ªéƒ¨åˆ†ï¼Œç°åœ¨æˆ‘ä»¬å¯ä»¥éå†æ‰€æœ‰çš„**å¼•è¨€å…ƒç´ (quotes elements)**ï¼Œå¹¶å°†å®ƒä»¬æ•´åˆæˆä¸€ä¸ªPythonå­—å…¸ï¼š<br>

```bash
>>> for quote in response.css("div.quote"):
...     text = quote.css("span.text::text").get()
...     author = quote.css("small.author::text").get()
...     tags = quote.css("div.tags a.tag::text").getall()
...     print(dict(text=text, author=author, tags=tags))
...
{'text': 'â€œThe world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.â€', 'author': 'Albert Einstein', 'tags': ['change', 'deep-thoughts', 'thinking', 'world']}
{'text': 'â€œIt is our choices, Harry, that show what we truly are, far more than our abilities.â€', 'author': 'J.K. Rowling', 'tags': ['abilities', 'choices']}
...
```

### åœ¨æˆ‘ä»¬çš„çˆ¬è™«ç¨‹åºä¸­æå–æ•°æ®(Extracting data in our spider):

è®©æˆ‘ä»¬å›åˆ°æˆ‘ä»¬çš„çˆ¬è™«ç¨‹åºã€‚åˆ°ç›®å‰ä¸ºæ­¢ï¼Œå®ƒè¿˜æ²¡æœ‰æå–ä»»ä½•ç‰¹å®šçš„æ•°æ®ï¼Œåªæ˜¯å°†æ•´ä¸ªHTMLé¡µé¢ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶ä¸­ã€‚è®©æˆ‘ä»¬å°†ä¸Šé¢çš„æå–é€»è¾‘æ•´åˆåˆ°æˆ‘ä»¬çš„çˆ¬è™«ç¨‹åºä¸­ã€‚<br>

ä¸€ä¸ªå…¸å‹çš„Scrapyçˆ¬è™«é€šå¸¸ä¼šç”Ÿæˆè®¸å¤šåŒ…å«ä»é¡µé¢æå–çš„æ•°æ®çš„å­—å…¸ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬åœ¨å›è°ƒä¸­ä½¿ç”¨Pythonå…³é”®å­— `yield` ï¼Œæ­£å¦‚ä½ ä¸‹é¢æ‰€è§ï¼š<br>

> åŸºäºå¼‚æ­¥å’Œå†…å­˜è€ƒè™‘ï¼Œæ‰€ä»¥ç”¨ `yield` å…³é”®å­—ã€‚

```python
import scrapy


class QuotesSpider(scrapy.Spider):
    name = "quotes"
    start_urls = [
        "https://quotes.toscrape.com/page/1/",
        "https://quotes.toscrape.com/page/2/",
    ]

    def parse(self, response):
        for quote in response.css("div.quote"):
            yield {
                "text": quote.css("span.text::text").get(),
                "author": quote.css("small.author::text").get(),
                "tags": quote.css("div.tags a.tag::text").getall(),
            }
```

è¦è¿è¡Œè¿™ä¸ªçˆ¬è™«ï¼Œè¯·è¾“å…¥ä»¥ä¸‹å‘½ä»¤é€€å‡º `scrapy shell`ï¼š<br>

```bash
quit()
```

ç„¶åè¿è¡Œ:<br>

```bash
scrapy crawl quotes
```

ç°åœ¨ï¼Œå®ƒåº”è¯¥è¾“å‡ºæå–çš„æ•°æ®ï¼Œå¹¶é™„æœ‰æ—¥å¿—ï¼š<br>

```txt
2016-09-19 18:57:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/1/>
{'tags': ['life', 'love'], 'author': 'AndrÃ© Gide', 'text': 'â€œIt is better to be hated for what you are than to be loved for what you are not.â€'}
2016-09-19 18:57:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/1/>
{'tags': ['edison', 'failure', 'inspirational', 'paraphrased'], 'author': 'Thomas A. Edison', 'text': "â€œI have not failed. I've just found 10,000 ways that won't work.â€"}
```


### å°†æŠ“å–çš„æ•°æ®å­˜å‚¨èµ·æ¥(Storing the scraped data):

å°†æŠ“å–çš„æ•°æ®å­˜å‚¨æœ€ç®€å•çš„æ–¹æ³•æ˜¯ä½¿ç”¨ [Feed](https://docs.scrapy.org/en/latest/topics/feed-exports.html#topics-feed-exports) å¯¼å‡ºï¼Œé€šè¿‡ä»¥ä¸‹å‘½ä»¤å®ç°ï¼š<br>

```bash
scrapy crawl quotes -O quotes.json
```

è¿™å°†ç”Ÿæˆä¸€ä¸ªåä¸º `quotes.json` çš„æ–‡ä»¶ï¼Œå…¶ä¸­åŒ…å«æ‰€æœ‰æŠ“å–çš„é¡¹ç›®ï¼Œä»¥JSONæ ¼å¼åºåˆ—åŒ–ã€‚<br>

å‘½ä»¤è¡Œå¼€å…³ `-O` **ä¼šè¦†ç›–ä»»ä½•ç°æœ‰æ–‡ä»¶** âš ï¸ï¼›ä½¿ç”¨-oåˆ™å¯ä»¥å°†æ–°å†…å®¹ **è¿½åŠ ** âœ…åˆ°ä»»ä½•ç°æœ‰æ–‡ä»¶ä¸­ã€‚**ç„¶è€Œï¼Œè¿½åŠ åˆ°JSONæ–‡ä»¶ä¼šä½¿æ–‡ä»¶å†…å®¹æˆä¸ºæ— æ•ˆçš„JSONã€‚å½“è¿½åŠ åˆ°æ–‡ä»¶æ—¶ï¼Œè€ƒè™‘ä½¿ç”¨ä¸åŒçš„åºåˆ—åŒ–æ ¼å¼ï¼Œä¾‹å¦‚JSON Lines** ğŸš¨ğŸš¨ğŸš¨ï¼š<br>

```bahs
scrapy crawl quotes -o quotes.jsonl
```

JSON Linesæ ¼å¼çš„æœ‰ç”¨ä¹‹å¤„åœ¨äºå®ƒç±»ä¼¼äºæµ(stream-like)ï¼Œä½ å¯ä»¥è½»æ¾åœ°å‘å…¶ä¸­è¿½åŠ æ–°è®°å½•ã€‚å®ƒä¸åƒJSONé‚£æ ·åœ¨ä½ è¿è¡Œä¸¤æ¬¡æ—¶ä¼šé‡åˆ°åŒæ ·çš„é—®é¢˜ã€‚æ­¤å¤–ï¼Œç”±äºæ¯æ¡è®°å½•éƒ½æ˜¯ç‹¬ç«‹çš„ä¸€è¡Œï¼Œä½ å¯ä»¥å¤„ç†å¤§æ–‡ä»¶è€Œæ— éœ€å°†æ‰€æœ‰å†…å®¹éƒ½æ”¾å…¥å†…å­˜ä¸­ï¼Œåƒ [JQ](https://jqlang.github.io/jq/) è¿™æ ·çš„å·¥å…·å¯ä»¥å¸®åŠ©ä½ åœ¨å‘½ä»¤è¡Œä¸Šå®Œæˆè¿™äº›æ“ä½œã€‚<br>

åœ¨å°å‹é¡¹ç›®ä¸­ï¼ˆå°±åƒæœ¬æ•™ç¨‹ä¸­çš„é‚£æ ·ï¼‰ï¼Œè¿™åº”è¯¥å°±è¶³å¤Ÿäº†ã€‚ç„¶è€Œï¼Œå¦‚æœä½ æƒ³å¯¹æŠ“å–çš„é¡¹ç›®æ‰§è¡Œæ›´å¤æ‚çš„æ“ä½œï¼Œä½ å¯ä»¥ç¼–å†™ä¸€ä¸ª [é¡¹ç›®ç®¡é“](https://docs.scrapy.org/en/latest/topics/item-pipeline.html#topics-item-pipeline) ã€‚å½“é¡¹ç›®åˆ›å»ºæ—¶ï¼Œå·²ç»ä¸ºä½ å‡†å¤‡å¥½äº†ä¸€ä¸ªé¡¹ç›®ç®¡é“çš„å ä½æ–‡ä»¶ï¼Œåœ¨ `tutorial/pipelines.py` ä¸­ã€‚ä¸è¿‡ï¼Œå¦‚æœä½ åªæ˜¯æƒ³å­˜å‚¨æŠ“å–çš„é¡¹ç›®ï¼Œé‚£ä¹ˆä½ ä¸éœ€è¦å®ç°ä»»ä½•é¡¹ç›®ç®¡é“ã€‚<br>


### è·Ÿéšé“¾æ¥(ç¿»é¡µ)(Following links"):

è®©æˆ‘ä»¬å‡è®¾ï¼Œä½ ä¸åªæ˜¯ä» `https://quotes.toscrape.com` ç½‘ç«™çš„å‰ä¸¤é¡µæŠ“å–å†…å®¹ï¼Œè€Œæ˜¯æƒ³è¦ä»è¯¥ç½‘ç«™çš„æ‰€æœ‰é¡µé¢è·å–åè¨€ã€‚<br>

æ—¢ç„¶ä½ å·²ç»çŸ¥é“å¦‚ä½•ä»é¡µé¢æå–æ•°æ®ï¼Œé‚£ä¹ˆæ¥ä¸‹æ¥è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•ä»è¿™äº›é¡µé¢ä¸­è·Ÿè¸ªé“¾æ¥ã€‚<br>

é¦–å…ˆè¦åšçš„äº‹æƒ…æ˜¯æå–æˆ‘ä»¬æƒ³è¦è·Ÿè¸ªçš„é¡µé¢çš„é“¾æ¥ã€‚æ£€æŸ¥æˆ‘ä»¬çš„é¡µé¢ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æœ‰ä¸€ä¸ªåˆ°ä¸‹ä¸€é¡µçš„é“¾æ¥ï¼Œå…¶æ ‡è®°å¦‚ä¸‹ï¼š<br>

```html
<ul class="pager">
    <li class="next">
        <a href="/page/2/">Next <span aria-hidden="true">&rarr;</span></a>
    </li>
</ul>
```

æˆ‘ä»¬å¯ä»¥å°è¯•åœ¨å‘½ä»¤è¡Œä¸­æå–å®ƒï¼š<br>

```bash
>>> response.css('li.next a').get()
'<a href="/page/2/">Next <span aria-hidden="true">â†’</span></a>'
```

è¿™æ®µä»£ç è·å–äº†é”šç‚¹å…ƒç´ ï¼Œä½†æˆ‘ä»¬éœ€è¦çš„æ˜¯ `href` (è¶…æ–‡æœ¬å¼•ç”¨)å±æ€§ã€‚ä¸ºæ­¤ï¼ŒScrapy æ”¯æŒä¸€ä¸ª CSS æ‰©å±•ï¼Œå¯ä»¥è®©ä½ é€‰æ‹©å±æ€§å†…å®¹ï¼Œåƒè¿™æ ·ï¼š<br>

```bash
>>> response.css("li.next a::attr(href)").get()
'/page/2/'
```

è¿˜æœ‰ä¸€ä¸ªå¯ç”¨çš„ `attrib` å±æ€§ï¼ˆæ›´å¤šè¯¦æƒ…è§[é€‰æ‹©å…ƒç´ å±æ€§](https://docs.scrapy.org/en/latest/topics/selectors.html#selecting-attributes)ï¼‰ã€‚<br>

```bash
>>> response.css("li.next a").attrib["href"]
'/page/2/'
```

ç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹æˆ‘ä»¬çš„çˆ¬è™«ç¨‹åºå¦‚ä½•ç»è¿‡ä¿®æ”¹ï¼Œèƒ½å¤Ÿé€’å½’åœ°è·Ÿéšé“¾æ¥åˆ°ä¸‹ä¸€ä¸ªé¡µé¢ï¼Œå¹¶ä»ä¸­æå–æ•°æ®ï¼š<br>

```python
import scrapy


class QuotesSpider(scrapy.Spider):
    name = "quotes"
    start_urls = [
        "https://quotes.toscrape.com/page/1/",
    ]

    def parse(self, response):
        for quote in response.css("div.quote"):
            yield {
                "text": quote.css("span.text::text").get(),
                "author": quote.css("small.author::text").get(),
                "tags": quote.css("div.tags a.tag::text").getall(),
            }

        next_page = response.css("li.next a::attr(href)").get()
        if next_page is not None:
            next_page = response.urljoin(next_page)
            yield scrapy.Request(next_page, callback=self.parse)
```

ç°åœ¨ï¼Œåœ¨æå–æ•°æ®ä¹‹åï¼Œ**`parse()` æ–¹æ³•ä¼šæŸ¥æ‰¾æŒ‡å‘ä¸‹ä¸€é¡µçš„é“¾æ¥**ï¼Œå¹¶ä½¿ç”¨ `urljoin()` æ–¹æ³•**æ„å»ºä¸€ä¸ªå®Œæ•´çš„ç»å¯¹ URL**ğŸ’¦ï¼ˆå› ä¸ºé“¾æ¥å¯èƒ½æ˜¯ç›¸å¯¹çš„ï¼‰ï¼Œç„¶åäº§ç”Ÿä¸€ä¸ªæ–°çš„è¯·æ±‚åˆ°ä¸‹ä¸€é¡µï¼Œå¹¶**å°†è‡ªå·±æ³¨å†Œä¸ºå›è°ƒå‡½æ•°**ï¼Œä»¥å¤„ç†ä¸‹ä¸€é¡µçš„æ•°æ®æå–ï¼Œå¹¶ç»§ç»­é€šè¿‡æ‰€æœ‰é¡µé¢è¿›è¡Œçˆ¬å–ã€‚<br>

ä½ åœ¨è¿™é‡Œçœ‹åˆ°çš„æ˜¯ Scrapy è·Ÿéšé“¾æ¥çš„æœºåˆ¶ï¼šå½“ä½ åœ¨å›è°ƒæ–¹æ³•ä¸­äº§ç”Ÿä¸€ä¸ªè¯·æ±‚æ—¶ï¼ŒScrapy å°†å®‰æ’å‘é€è¯¥è¯·æ±‚ï¼Œå¹¶æ³¨å†Œä¸€ä¸ªå›è°ƒæ–¹æ³•ï¼Œåœ¨è¯¥è¯·æ±‚å®Œæˆæ—¶æ‰§è¡Œã€‚<br>

åˆ©ç”¨è¿™ç§æ–¹å¼ï¼Œä½ å¯ä»¥æ„å»ºå¤æ‚çš„çˆ¬è™«ï¼Œè¿™äº›çˆ¬è™«æ ¹æ®ä½ å®šä¹‰çš„è§„åˆ™è·Ÿéšé“¾æ¥ï¼Œå¹¶æ ¹æ®å®ƒè®¿é—®çš„é¡µé¢æå–ä¸åŒç±»å‹çš„æ•°æ®ã€‚<br>

åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œå®ƒåˆ›å»ºäº†ä¸€ç§å¾ªç¯ï¼Œè·Ÿéšæ‰€æœ‰æŒ‡å‘ä¸‹ä¸€é¡µçš„é“¾æ¥ï¼Œç›´åˆ°æ‰¾ä¸åˆ°ä¸ºæ­¢â€”â€”è¿™å¯¹äºçˆ¬å–åšå®¢ã€è®ºå›å’Œå…¶ä»–å¸¦æœ‰åˆ†é¡µçš„ç½‘ç«™éå¸¸æ–¹ä¾¿ã€‚<br>


### åˆ›å»ºè¯·æ±‚çš„å¿«æ·æ–¹å¼(A shortcut for creating Requests):

ä½ å¯ä»¥ä½¿ç”¨ `response.follow` ä½œä¸º**åˆ›å»º `Request` å¯¹è±¡**çš„å¿«æ·æ–¹å¼:<br>

```python
import scrapy


class QuotesSpider(scrapy.Spider):
    name = "quotes"
    start_urls = [
        "https://quotes.toscrape.com/page/1/",
    ]

    def parse(self, response):
        for quote in response.css("div.quote"):
            yield {
                "text": quote.css("span.text::text").get(),
                "author": quote.css("span small::text").get(),
                "tags": quote.css("div.tags a.tag::text").getall(),
            }

        next_page = response.css("li.next a::attr(href)").get()
        if next_page is not None:
            yield response.follow(next_page, callback=self.parse)
```

ä¸ `scrapy.Request` ä¸åŒï¼Œ`response.follow` **ç›´æ¥æ”¯æŒç›¸å¯¹ URL** - ä¸éœ€è¦è°ƒç”¨ `urljoin`ã€‚è¯·æ³¨æ„ï¼Œ`response.follow` åªæ˜¯è¿”å›ä¸€ä¸ª `Request` å®ä¾‹ï¼›ä½ ä»ç„¶éœ€è¦ `yield` è¿™ä¸ª `Request`ã€‚<br>

ä½ è¿˜å¯ä»¥ä¼ é€’ä¸€ä¸ªé€‰æ‹©å™¨ç»™ `response.follow`ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²ï¼›è¿™ä¸ªé€‰æ‹©å™¨åº”è¯¥æå–å¿…è¦çš„å±æ€§ï¼š<br>

```python
for href in response.css("ul.pager a::attr(href)"):
    yield response.follow(href, callback=self.parse)
```

`response.css("ul.pager a::attr(href)")` è¿”å›çš„å†…å®¹å¦‚ä¸‹:<br>

```bash
>>> response.css("ul.pager a::attr(href)")
[<Selector query="descendant-or-self::ul[@class and contains(concat(' ', normalize-space(@class), ' '), ' pager ')]/descendant-or-self::*/a/@href" data='/page/2/'>]
```

ğŸ¤­ğŸ¤­ğŸ¤­å¯¹äº `<a>` å…ƒç´ ï¼Œæœ‰ä¸€ä¸ªå¿«æ·æ–¹å¼ï¼š`response.follow` ä¼šè‡ªåŠ¨ä½¿ç”¨å®ƒä»¬çš„ `href` å±æ€§ã€‚å› æ­¤ï¼Œä»£ç å¯ä»¥è¿›ä¸€æ­¥ç®€åŒ–ï¼š<br>

```python
for a in response.css("ul.pager a"):
    yield response.follow(a, callback=self.parse)
```

è¦ä»ä¸€ä¸ªå¯è¿­ä»£å¯¹è±¡åˆ›å»ºå¤šä¸ªè¯·æ±‚ï¼Œä½ å¯ä»¥ä½¿ç”¨ `response.follow_all` æ›¿ä»£ï¼š<br>

```python
anchors = response.css("ul.pager a")
yield from response.follow_all(anchors, callback=self.parse)
```

æˆ–è€…ï¼Œè¿›ä¸€æ­¥ç®€åŒ–ä¸ºï¼š<br>

```python
yield from response.follow_all(css="ul.pager a", callback=self.parse)
```


### æ›´å¤šçš„ä¾‹å­å’Œæ¨¡å¼(More examples and patterns):

è¿™æ˜¯å¦ä¸€ä¸ªçˆ¬è™«ç¤ºä¾‹ï¼Œç”¨äºå±•ç¤ºå›è°ƒå’Œè·Ÿè¸ªé“¾æ¥ï¼Œè¿™æ¬¡æ˜¯ç”¨æ¥æŠ“å–ä½œè€…ä¿¡æ¯çš„ï¼š<br>

```python
import scrapy


class AuthorSpider(scrapy.Spider):
    name = "author"

    start_urls = ["https://quotes.toscrape.com/"]

    def parse(self, response):
        author_page_links = response.css(".author + a")
        yield from response.follow_all(author_page_links, self.parse_author)

        pagination_links = response.css("li.next a")
        yield from response.follow_all(pagination_links, self.parse)

    def parse_author(self, response):
        def extract_with_css(query):
            return response.css(query).get(default="").strip()

        yield {
            "name": extract_with_css("h3.author-title::text"),
            "birthdate": extract_with_css(".author-born-date::text"),
            "bio": extract_with_css(".author-description::text"),
        }
```

è¿™ä¸ªçˆ¬è™«å°†ä»ä¸»é¡µå¼€å§‹ï¼Œå®ƒä¼šè·Ÿè¸ªæ‰€æœ‰æŒ‡å‘ä½œè€…(authors)é¡µé¢çš„é“¾æ¥ï¼Œå¹¶å¯¹æ¯ä¸ªé¡µé¢è°ƒç”¨ `parse_author` å›è°ƒå‡½æ•°ï¼ŒåŒæ—¶ä¹Ÿä¼šè·Ÿè¸ªåˆ†é¡µé“¾æ¥ï¼Œå¹¶åƒæˆ‘ä»¬ä¹‹å‰çœ‹åˆ°çš„é‚£æ ·ä½¿ç”¨ `parse` å›è°ƒå‡½æ•°ã€‚<br>

è¿™é‡Œæˆ‘ä»¬å°†å›è°ƒå‡½æ•°ä½œä¸ºä½ç½®å‚æ•°ä¼ é€’ç»™ `response.follow_all`ï¼Œä»¥ç®€åŒ–ä»£ç ï¼›è¿™å¯¹äº `Request` ä¹ŸåŒæ ·é€‚ç”¨ã€‚<br>

`parse_author` å›è°ƒå‡½æ•°å®šä¹‰äº†ä¸€ä¸ªè¾…åŠ©å‡½æ•°ï¼Œ**ç”¨äºæå–å’Œæ¸…ç† CSS æŸ¥è¯¢çš„æ•°æ®ï¼Œå¹¶ç”ŸæˆåŒ…å«ä½œè€…æ•°æ®çš„ Python å­—å…¸ã€‚** <br>

ğŸŒ¿ğŸŒ¿ğŸŒ¿ğŸŒ¿ğŸŒ¿ğŸŒ¿<br>

è¿™ä¸ªçˆ¬è™«å±•ç¤ºçš„å¦ä¸€ä¸ªæœ‰è¶£ä¹‹å¤„åœ¨äºï¼Œå³ä½¿æœ‰è®¸å¤šæ¥è‡ªåŒä¸€ä½œè€…çš„å¼•ç”¨ï¼Œæˆ‘ä»¬ä¹Ÿä¸éœ€è¦æ‹…å¿ƒå¤šæ¬¡è®¿é—®åŒä¸€ä¸ªä½œè€…çš„é¡µé¢ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œ`Scrapy` ä¼šè¿‡æ»¤æ‰å¯¹å·²è®¿é—®è¿‡çš„ `URL` çš„é‡å¤è¯·æ±‚ï¼Œé¿å…äº†å› ç¼–ç¨‹é”™è¯¯è€Œè¿‡åº¦è¯·æ±‚æœåŠ¡å™¨çš„é—®é¢˜ã€‚è¿™å¯ä»¥é€šè¿‡è®¾ç½® `DUPEFILTER_CLASS` æ¥é…ç½®ã€‚<br>

ğŸŒ¿ğŸŒ¿ğŸŒ¿ğŸŒ¿ğŸŒ¿ğŸŒ¿<br>

å¸Œæœ›åˆ°ç›®å‰ä¸ºæ­¢ï¼Œä½ å·²ç»å¯¹å¦‚ä½•ä½¿ç”¨ `Scrapy` çš„è·Ÿè¸ªé“¾æ¥å’Œ**å›è°ƒæœºåˆ¶**æœ‰äº†è‰¯å¥½çš„ç†è§£ã€‚<br>

ä½œä¸ºå¦ä¸€ä¸ªåˆ©ç”¨è·Ÿè¸ªé“¾æ¥æœºåˆ¶çš„ç¤ºä¾‹çˆ¬è™«ï¼Œè¯·æŸ¥çœ‹ `CrawlSpider` ç±»ï¼Œè¿™æ˜¯ä¸€ä¸ªé€šç”¨çš„çˆ¬è™«ï¼Œå®ç°äº†ä¸€ä¸ªå°è§„åˆ™å¼•æ“ï¼Œä½ å¯ä»¥åœ¨å…¶åŸºç¡€ä¸Šç¼–å†™ä½ çš„çˆ¬è™«ã€‚<br>

æ­¤å¤–ï¼Œä¸€ä¸ªå¸¸è§çš„æ¨¡å¼æ˜¯ä»å¤šä¸ªé¡µé¢æ„å»ºä¸€ä¸ªé¡¹ç›®ï¼Œä½¿ç”¨ä¸€ç§æŠ€å·§å°†é¢å¤–çš„æ•°æ®ä¼ é€’ç»™å›è°ƒå‡½æ•°ã€‚<br>


### ä½¿ç”¨spiderå‚æ•°(Using spider arguments):

ä½ å¯ä»¥åœ¨è¿è¡Œçˆ¬è™«æ—¶ä½¿ç”¨ `-a` é€‰é¡¹ä¸ºæ‚¨çš„çˆ¬è™«æä¾›å‘½ä»¤è¡Œå‚æ•°ï¼š<br>

```bash
scrapy crawl quotes -O quotes-humor.json -a tag=humor
```

è¿™äº›å‚æ•°ä¼šä¼ é€’ç»™Spiderçš„ __init__ æ–¹æ³•ï¼Œå¹¶é»˜è®¤æˆä¸ºSpiderçš„å±æ€§ã€‚<br>

åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œä¸º `tag` å‚æ•°æä¾›çš„å€¼å°†é€šè¿‡ `self.tag` å¯ç”¨ã€‚ä½ å¯ä»¥åˆ©ç”¨è¿™ä¸€ç‚¹æ¥è®©ä½ çš„SpideråªæŠ“å–å¸¦æœ‰ç‰¹å®šæ ‡ç­¾çš„å¼•ç”¨ï¼Œæ ¹æ®å‚æ•°æ„å»ºURLï¼š<br>

```python
import scrapy


class QuotesSpider(scrapy.Spider):
    name = "quotes"

    def start_requests(self):
        url = "https://quotes.toscrape.com/"
        tag = getattr(self, "tag", None)
        if tag is not None:
            url = url + "tag/" + tag
        yield scrapy.Request(url, self.parse)

    def parse(self, response):
        for quote in response.css("div.quote"):
            yield {
                "text": quote.css("span.text::text").get(),
                "author": quote.css("small.author::text").get(),
            }

        next_page = response.css("li.next a::attr(href)").get()
        if next_page is not None:
            yield response.follow(next_page, self.parse)
```

å¦‚æœä½ å‘è¿™ä¸ªçˆ¬è™«ä¼ é€’äº† `tag=humor` å‚æ•°ï¼Œä½ ä¼šå‘ç°å®ƒåªä¼šè®¿é—®å¸¦æœ‰ **`humor`** æ ‡ç­¾çš„URLï¼Œæ¯”å¦‚ `https://quotes.toscrape.com/tag/humorã€‚` <br>

[ä½ å¯ä»¥åœ¨è¿™é‡Œäº†è§£æ›´å¤šå…³äºå¤„ç†çˆ¬è™«å‚æ•°çš„ä¿¡æ¯](https://docs.scrapy.org/en/latest/topics/spiders.html#spiderargs)ã€‚<br>

### ä¸‹ä¸€æ­¥(Next steps):

æœ¬æ•™ç¨‹ä»…æ¶µç›–äº† `Scrapy` çš„åŸºç¡€çŸ¥è¯†ï¼Œä½†è¿˜æœ‰è®¸å¤šå…¶ä»–ç‰¹æ€§åœ¨æ­¤æœªæåŠã€‚è¯·æŸ¥çœ‹ [ Scrapy at a glance](https://docs.scrapy.org/en/latest/intro/overview.html#intro-overview) ç« èŠ‚ä¸­çš„ [What else?](https://docs.scrapy.org/en/latest/intro/overview.html#topics-whatelse) éƒ¨åˆ†ï¼Œä»¥å¿«é€Ÿäº†è§£æœ€é‡è¦çš„ç‰¹æ€§ã€‚<br>

ä½ å¯ä»¥ä» [Basic concepts](https://docs.scrapy.org/en/latest/index.html#section-basics) éƒ¨åˆ†ç»§ç»­å­¦ä¹ ï¼Œäº†è§£æ›´å¤šå…³äºå‘½ä»¤è¡Œå·¥å…·ã€çˆ¬è™«ã€é€‰æ‹©å™¨ä»¥åŠæœ¬æ•™ç¨‹æœªæ¶‰åŠçš„å…¶ä»–å†…å®¹ï¼Œæ¯”å¦‚å¯¹æŠ“å–æ•°æ®çš„å»ºæ¨¡ã€‚å¦‚æœæ‚¨æ›´å–œæ¬¢é€šè¿‡å®ä¾‹é¡¹ç›®è¿›è¡Œå­¦ä¹ ï¼Œè¯·æŸ¥çœ‹ [Examples](https://docs.scrapy.org/en/latest/intro/examples.html#intro-examples) éƒ¨åˆ†ã€‚<br>


## ç¤ºä¾‹(Examples):

å­¦ä¹ æœ€å¥½çš„æ–¹å¼æ˜¯é€šè¿‡å®ä¾‹ï¼Œ`Scrapy` ä¹Ÿä¸ä¾‹å¤–ã€‚å› æ­¤ï¼Œæœ‰ä¸€ä¸ªåä¸º [quotesbot](https://github.com/scrapy/quotesbot) çš„ç¤ºä¾‹ Scrapy é¡¹ç›®ï¼Œä½ å¯ä»¥ç”¨å®ƒæ¥è¿›è¡Œå®è·µå¹¶æ›´æ·±å…¥åœ°äº†è§£ Scrapyã€‚è¯¥é¡¹ç›®åŒ…å«äº†é’ˆå¯¹ https://quotes.toscrape.com çš„**ä¸¤ä¸ªçˆ¬è™«ï¼Œä¸€ä¸ªä½¿ç”¨ CSS é€‰æ‹©å™¨ï¼Œå¦ä¸€ä¸ªä½¿ç”¨ XPath è¡¨è¾¾å¼**ã€‚<br>

`quotesbot` é¡¹ç›®å¯åœ¨ä»¥ä¸‹åœ°å€æ‰¾åˆ°ï¼š`https://github.com/scrapy/quotesbot`ã€‚ä½ å¯ä»¥åœ¨é¡¹ç›®çš„ `README` æ–‡ä»¶ä¸­æ‰¾åˆ°æ›´å¤šå…³äºå®ƒçš„ä¿¡æ¯ã€‚<br>

> ç¬”è€…åœ¨å½“å‰é¡¹ç›®ä¸‹ï¼Œä¸‹è½½äº†è¯¥é¡¹ç›®ï¼Œé¡¹ç›®æ–‡ä»¶å¤¹åç§°ä¸º: `quotesbot`ã€‚

å¦‚æœä½ ç†Ÿæ‚‰ gitï¼Œä½ å¯ä»¥æ£€æŸ¥ä»£ç ã€‚å¦åˆ™ï¼Œä½ å¯ä»¥é€šè¿‡ç‚¹å‡» [è¿™é‡Œ](https://github.com/scrapy/quotesbot/archive/master.zip) ä¸‹è½½é¡¹ç›®çš„ zip æ–‡ä»¶ã€‚<br>


## é€‰æ‹©å™¨(Selectors):

åœ¨è¿›è¡Œç½‘é¡µæŠ“å–æ—¶ï¼Œä½ **æœ€å¸¸éœ€è¦æ‰§è¡Œçš„ä»»åŠ¡æ˜¯ä»HTMLæºä»£ç ä¸­æå–æ•°æ®**ã€‚æœ‰å‡ ä¸ªåº“å¯ç”¨äºå®ç°è¿™ä¸€ç›®æ ‡ï¼Œä¾‹å¦‚ï¼š<br>

- `BeautifulSoup` æ˜¯ä¸€ä¸ªåœ¨ Python ç¨‹åºå‘˜ä¸­éå¸¸å—æ¬¢è¿çš„ç½‘é¡µæŠ“å–åº“ï¼Œå®ƒåŸºäº HTML ä»£ç çš„ç»“æ„æ„å»ºä¸€ä¸ª Python å¯¹è±¡ï¼Œå¹¶ä¸”å¯¹äºç³Ÿç³•çš„æ ‡è®°ä¹Ÿå¤„ç†å¾—ç›¸å½“ä¸é”™ï¼Œä½†**å®ƒæœ‰ä¸€ä¸ªç¼ºç‚¹ï¼šé€Ÿåº¦æ…¢ã€‚ğŸš¨**

- `lxml` æ˜¯ä¸€ä¸ªåŸºäº `ElementTree` çš„å…·æœ‰ pythonic API çš„ **XML è§£æåº“ï¼ˆä¹Ÿå¯è§£æ HTMLï¼‰**ã€‚ï¼ˆlxml ä¸æ˜¯ Python æ ‡å‡†åº“çš„ä¸€éƒ¨åˆ†ã€‚ï¼‰

Scrapy è‡ªå¸¦äº†æå–æ•°æ®çš„æœºåˆ¶ã€‚å®ƒä»¬è¢«ç§°ä¸ºé€‰æ‹©å™¨ï¼Œå› ä¸ºå®ƒä»¬é€šè¿‡ `XPath` æˆ– `CSS` è¡¨è¾¾å¼ â€œé€‰æ‹©â€ HTML æ–‡æ¡£ä¸­çš„ç‰¹å®šéƒ¨åˆ†ã€‚<br>

XPath æ˜¯ä¸€ç§ç”¨äºé€‰æ‹© XML æ–‡æ¡£ä¸­èŠ‚ç‚¹çš„è¯­è¨€ï¼Œä¹Ÿå¯ç”¨äº HTMLã€‚CSS æ˜¯ä¸€ç§åº”ç”¨äº HTML æ–‡æ¡£çš„æ ·å¼è¯­è¨€ã€‚å®ƒå®šä¹‰äº†é€‰æ‹©å™¨ï¼Œå°†è¿™äº›æ ·å¼ä¸ç‰¹å®šçš„ HTML å…ƒç´ ç›¸å…³è”ã€‚<br>

âš ï¸æ³¨æ„:<br>

Scrapy é€‰æ‹©å™¨æ˜¯å›´ç»• parsel åº“çš„ä¸€ä¸ªè–„åŒ…è£…å±‚ï¼›è¿™ä¸ªåŒ…è£…å±‚çš„ç›®çš„æ˜¯ä¸ºäº†æ›´å¥½åœ°ä¸ Scrapy å“åº”å¯¹è±¡é›†æˆã€‚<br>

parsel æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„ç½‘é¡µæŠ“å–åº“ï¼Œå¯ä»¥åœ¨ä¸ä½¿ç”¨ Scrapy çš„æƒ…å†µä¸‹ä½¿ç”¨ã€‚å®ƒåœ¨åº•å±‚ä½¿ç”¨ lxml åº“ï¼Œå¹¶åœ¨ lxml API ä¹‹ä¸Šå®ç°äº†ä¸€ä¸ªç®€å•çš„ APIã€‚è¿™æ„å‘³ç€ Scrapy é€‰æ‹©å™¨åœ¨é€Ÿåº¦å’Œè§£æå‡†ç¡®æ€§ä¸Šä¸ lxml éå¸¸ç›¸ä¼¼ã€‚<br>

### ä½¿ç”¨é€‰æ‹©å™¨(Using selectors):

#### æ„é€ é€‰æ‹©å™¨(Constructing selectors):

`Response` å¯¹è±¡åœ¨ `.selector` å±æ€§ä¸Šæš´éœ²äº†ä¸€ä¸ª Selector å®ä¾‹ï¼š<br>

```bash
>>> response.selector.xpath("//span/text()").get()
'good'
```

ä½¿ç”¨ XPath å’Œ CSS æŸ¥è¯¢å“åº”(Querying responses)æ˜¯éå¸¸å¸¸è§çš„ï¼Œå› æ­¤å“åº”(responses)è¿˜åŒ…æ‹¬äº†ä¸¤ä¸ªæ›´ä¾¿æ·çš„å¿«æ·æ–¹å¼ï¼š`response.xpath()` å’Œ `response.css()`ï¼š<br>

```bash
>>> response.xpath("//span/text()").get()
'good'
>>> response.css("span::text").get()
'good'
```

Scrapy é€‰æ‹©å™¨æ˜¯é€šè¿‡ä¼ é€’ `TextResponse` å¯¹è±¡æˆ–æ ‡è®°ï¼ˆä½œä¸ºå­—ç¬¦ä¸²ï¼Œåœ¨ text å‚æ•°ä¸­ï¼‰æ¥æ„å»ºçš„ Selector ç±»çš„å®ä¾‹ã€‚<br>

é€šå¸¸ä¸éœ€è¦æ‰‹åŠ¨æ„å»º Scrapy é€‰æ‹©å™¨ï¼šåœ¨ Spider å›è°ƒä¸­å¯ä»¥ä½¿ç”¨ `response` (å“åº”)å¯¹è±¡ï¼Œå› æ­¤åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œä½¿ç”¨ `response.css()` å’Œ `response.xpath()` è¿™äº›å¿«æ·æ–¹å¼æ›´ä¸ºæ–¹ä¾¿ã€‚é€šè¿‡ä½¿ç”¨ `response.selector` æˆ–è¿™äº›å¿«æ·æ–¹å¼ï¼Œä½ è¿˜å¯ä»¥ç¡®ä¿å“åº”ä½“åªè¢«è§£æä¸€æ¬¡ã€‚<br>

ä½†å¦‚æœéœ€è¦ï¼Œä¹Ÿå¯ä»¥ç›´æ¥ä½¿ç”¨ `Selector`ã€‚ä»æ–‡æœ¬æ„å»ºï¼š<br>

```bash
>>> from scrapy.selector import Selector
>>> body = "<html><body><span>good</span></body></html>"
>>> Selector(text=body).xpath("//span/text()").get()
'good'
```

ä»response(å“åº”)æ„å»º - `HtmlResponse` æ˜¯ `TextResponse` å­ç±»ä¹‹ä¸€ï¼š<br>

```bash
>>> from scrapy.selector import Selector
>>> from scrapy.http import HtmlResponse
>>> response = HtmlResponse(url="http://example.com", body=body, encoding="utf-8")
>>> Selector(response=response).xpath("//span/text()").get()
'good'
```

`Selector` (é€‰æ‹©å™¨)ä¼šæ ¹æ®è¾“å…¥ç±»å‹è‡ªåŠ¨é€‰æ‹©æœ€ä½³çš„è§£æè§„åˆ™ï¼ˆXML å¯¹æ¯” HTMLï¼‰ã€‚<br>

#### ä½¿ç”¨é€‰æ‹©å™¨(Using selectors):

ä¸ºäº†è§£é‡Šå¦‚ä½•ä½¿ç”¨é€‰æ‹©å™¨ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ `Scrapy shell`ï¼ˆå®ƒæä¾›äº†äº¤äº’å¼æµ‹è¯•ï¼‰å’Œä½äº Scrapy æ–‡æ¡£æœåŠ¡å™¨ä¸Šçš„ä¸€ä¸ªç¤ºä¾‹é¡µé¢ï¼š<br>

- https://docs.scrapy.org/en/latest/_static/selectors-sample1.html

ä¸ºäº†å®Œæ•´æ€§ï¼Œè¿™é‡Œæ˜¯å®ƒçš„å®Œæ•´ HTML ä»£ç ï¼š<br>

```html
<!DOCTYPE html>

<html>
  <head>
    <base href='http://example.com/' />
    <title>Example website</title>
  </head>
  <body>
    <div id='images'>
      <a href='image1.html'>Name: My image 1 <br /><img src='image1_thumb.jpg' alt='image1'/></a>
      <a href='image2.html'>Name: My image 2 <br /><img src='image2_thumb.jpg' alt='image2'/></a>
      <a href='image3.html'>Name: My image 3 <br /><img src='image3_thumb.jpg' alt='image3'/></a>
      <a href='image4.html'>Name: My image 4 <br /><img src='image4_thumb.jpg' alt='image4'/></a>
      <a href='image5.html'>Name: My image 5 <br /><img src='image5_thumb.jpg' alt='image5'/></a>
    </div>
  </body>
</html>
```

é¦–å…ˆï¼Œè®©æˆ‘ä»¬æ‰“å¼€ shellï¼š<br>

```bash
scrapy shell https://docs.scrapy.org/en/latest/_static/selectors-sample1.html
```

ç„¶åï¼Œåœ¨ shell åŠ è½½åï¼Œä½ å°†åœ¨ `response shell` å˜é‡ä¸­æ‹¥æœ‰å¯ç”¨çš„ `response` (å“åº”)ï¼Œå¹¶ä¸”å…¶é™„åŠ çš„é€‰æ‹©å™¨ä½äº `response.selector` å±æ€§ä¸­ã€‚<br>

ç”±äºæˆ‘ä»¬å¤„ç†çš„æ˜¯ HTMLï¼Œé€‰æ‹©å™¨å°†è‡ªåŠ¨ä½¿ç”¨ HTML è§£æå™¨ã€‚<br>

å› æ­¤ï¼Œé€šè¿‡æŸ¥çœ‹è¯¥é¡µé¢çš„ HTML ä»£ç ï¼Œè®©æˆ‘ä»¬æ„é€ ä¸€ä¸ª XPathï¼Œç”¨äºé€‰æ‹© title æ ‡ç­¾(tag)å†…çš„æ–‡æœ¬(text)ï¼š<br>

```bash
>>> response.xpath("//title/text()")
[<Selector query='//title/text()' data='Example website'>]
```

è¦çœŸæ­£æå–æ–‡æœ¬æ•°æ®ï¼Œä½ å¿…é¡»è°ƒç”¨é€‰æ‹©å™¨çš„ `.get()` æˆ– `.getall()` æ–¹æ³•ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š<br>

```bash
>>> response.xpath("//title/text()").getall()
['Example website']
>>> response.xpath("//title/text()").get()
'Example website'
```

`.get()` æ€»æ˜¯è¿”å›å•ä¸ªç»“æœï¼›**å¦‚æœæœ‰å¤šä¸ªåŒ¹é…é¡¹ï¼Œå°†è¿”å›ç¬¬ä¸€ä¸ªåŒ¹é…é¡¹çš„å†…å®¹ï¼›** å¦‚æœæ²¡æœ‰åŒ¹é…é¡¹ï¼Œåˆ™è¿”å› Noneã€‚`.getall()` è¿”å›åŒ…å«æ‰€æœ‰ç»“æœçš„åˆ—è¡¨ã€‚<br>

è¯·æ³¨æ„ï¼ŒCSS é€‰æ‹©å™¨å¯ä»¥ä½¿ç”¨ CSS3 ä¼ªå…ƒç´ é€‰æ‹©æ–‡æœ¬æˆ–å±æ€§èŠ‚ç‚¹ï¼š<br>

```bash
>>> response.css("title::text").get()
'Example website'
```

å¦‚ä½ æ‰€è§ï¼Œ`.xpath()` å’Œ `.css()` æ–¹æ³•è¿”å›ä¸€ä¸ª `SelectorList` å®ä¾‹ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é€‰æ‹©å™¨çš„åˆ—è¡¨ã€‚è¿™ä¸ª API å¯ç”¨äºå¿«é€Ÿé€‰æ‹©åµŒå¥—æ•°æ®ï¼š<br>

```bash
>>> response.css("img").xpath("@src").getall()
['image1_thumb.jpg',
'image2_thumb.jpg',
'image3_thumb.jpg',
'image4_thumb.jpg',
'image5_thumb.jpg']
```

å¦‚æœæ‚¨åªæƒ³æå–ç¬¬ä¸€ä¸ªåŒ¹é…çš„å…ƒç´ ï¼Œå¯ä»¥è°ƒç”¨é€‰æ‹©å™¨çš„ `.get()` æ–¹æ³•ï¼ˆæˆ–å…¶åœ¨ä¹‹å‰ Scrapy ç‰ˆæœ¬ä¸­å¸¸ç”¨çš„åˆ«å `.extract_first()`ï¼‰ï¼š<br>

```bash
>>> response.xpath('//div[@id="images"]/a/text()').get()
'Name: My image 1 '
```

å¦‚æœæ²¡æœ‰æ‰¾åˆ°å…ƒç´ ï¼Œå®ƒå°†è¿”å› `None`ï¼š<br>

```bash
>>> response.xpath('//div[@id="not-exists"]/text()').get() is None
True
```

å¯ä»¥æä¾›ä¸€ä¸ªé»˜è®¤çš„è¿”å›å€¼ä½œä¸ºå‚æ•°ï¼Œä»¥ä»£æ›¿ `None` ä½¿ç”¨ï¼š<br>

```bash
>>> response.xpath('//div[@id="not-exists"]/text()').get(default="not-found")
'not-found'
```

å¯ä»¥ä¸ä½¿ç”¨ä¾‹å¦‚ `'@src'` çš„ **XPath**ï¼Œè€Œæ˜¯é€šè¿‡é€‰æ‹©å™¨çš„ `.attrib` å±æ€§æ¥æŸ¥è¯¢å±æ€§ï¼š<br>

```bash
>>> [img.attrib["src"] for img in response.css("img")]
['image1_thumb.jpg',
'image2_thumb.jpg',
'image3_thumb.jpg',
'image4_thumb.jpg',
'image5_thumb.jpg']
```

ä½œä¸ºä¸€ç§å¿«æ·æ–¹å¼ï¼Œ`.attrib` ä¹Ÿå¯ä»¥ç›´æ¥åœ¨ `SelectorList` ä¸Šä½¿ç”¨ï¼›å®ƒè¿”å›ç¬¬ä¸€ä¸ªåŒ¹é…å…ƒç´ çš„å±æ€§ï¼š<br>

```bash
>>> response.css("img").attrib["src"]
'image1_thumb.jpg'
```

è¿™åœ¨åªæœŸæœ›å•ä¸ªç»“æœæ—¶æœ€æœ‰ç”¨ï¼Œä¾‹å¦‚é€šè¿‡ `id` é€‰æ‹©ï¼Œæˆ–åœ¨ç½‘é¡µä¸Šé€‰æ‹©ç‹¬ç‰¹çš„å…ƒç´ æ—¶ï¼š<br>

```bash
>>> response.css("base").attrib["href"]
'http://example.com/'
```

`<base>` æ ‡ç­¾ä½äº`<head>`éƒ¨åˆ†ï¼š<br>

```html
<base href='http://example.com/' />
```

è¿™ä¸ª`<base>`æ ‡ç­¾ä¸ºé¡µé¢ä¸Šçš„æ‰€æœ‰ç›¸å¯¹URLæŒ‡å®šäº†ä¸€ä¸ªåŸºç¡€URLã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼ŒåŸºç¡€URLè¢«è®¾ç½®ä¸º`'http://example.com/'`ã€‚è¿™æ„å‘³ç€é¡µé¢ä¸­æ‰€æœ‰ç›¸å¯¹è·¯å¾„çš„URLï¼ˆå¦‚åœ¨`<a>`æ ‡ç­¾ä¸­çš„`href`å±æ€§ï¼‰éƒ½ä¼šä»¥è¿™ä¸ªURLä½œä¸ºå…¶åŸºç¡€ã€‚<br>

å› æ­¤ï¼Œåœ¨Scrapyä¸­ä½¿ç”¨`response.css("base").attrib["href"]`èƒ½å¤ŸæˆåŠŸåœ°æå–å‡º`<base>`æ ‡ç­¾çš„`href`å±æ€§å€¼ï¼Œå³`'http://example.com/'`ã€‚è¿™åœ¨å¤„ç†å’Œæ„å»ºå®Œæ•´çš„ç»å¯¹URLæ—¶éå¸¸æœ‰ç”¨ï¼Œç‰¹åˆ«æ˜¯å½“é¡µé¢ä¸­åŒ…å«å¤§é‡ç›¸å¯¹è·¯å¾„çš„é“¾æ¥æ—¶ã€‚<br>

> `<base>`æ ‡ç­¾åœ¨HTMLä¸­æ˜¯ä¸€ä¸ªä¸å¸¸ç”¨çš„æ ‡ç­¾ï¼Œå®ƒç”¨äºæŒ‡å®šé¡µé¢å†…æ‰€æœ‰ç›¸å¯¹URLçš„åŸºç¡€URLã€‚å¦‚æœHTMLä¸­æ²¡æœ‰`<base>`æ ‡ç­¾ï¼Œè¿™æ®µä»£ç ä¼šè¿”å›ä¸€ä¸ªé”™è¯¯æˆ–ç©ºå€¼ï¼Œå› ä¸ºå®ƒè¯•å›¾è®¿é—®ä¸å­˜åœ¨çš„å…ƒç´ çš„å±æ€§ã€‚è¿™ç§æƒ…å†µä¸‹ï¼Œä½¿ç”¨è¿™æ®µä»£ç ä¹‹å‰æ£€æŸ¥HTMLæ˜¯å¦åŒ…å«`<base>`æ ‡ç­¾æ˜¯ä¸€ä¸ªå¥½ä¹ æƒ¯ã€‚


ç°åœ¨æˆ‘ä»¬å°†è·å– **åŸºç¡€URL** å’Œä¸€äº› **å›¾ç‰‡é“¾æ¥** ï¼š<br>

```bash
>>> response.xpath("//base/@href").get()
'http://example.com/'
```

ä»£ç è¯¦è§£ï¼š<br>

1. `response`: è¿™æ˜¯Scrapyä¸­çš„ä¸€ä¸ªå“åº”å¯¹è±¡ï¼Œä»£è¡¨äº†çˆ¬è™«ä¸‹è½½çš„é¡µé¢ã€‚

2. `.xpath("//base/@href")`: è¿™æ˜¯ä¸€ä¸ªXPathé€‰æ‹©å™¨ã€‚XPathæ˜¯ä¸€ç§åœ¨XMLå’ŒHTMLæ–‡æ¡£ä¸­æŸ¥æ‰¾ä¿¡æ¯çš„è¯­è¨€ã€‚è¿™é‡Œçš„`//base/@href`æ„å‘³ç€æŸ¥æ‰¾æ‰€æœ‰`<base>`æ ‡ç­¾çš„`href`å±æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œ`//`è¡¨ç¤ºé€‰æ‹©æ–‡æ¡£ä¸­æ‰€æœ‰ç¬¦åˆæ¡ä»¶çš„èŠ‚ç‚¹ï¼Œ`base`æ˜¯HTMLä¸­çš„ä¸€ä¸ªç‰¹å®šæ ‡ç­¾ï¼Œè€Œ`@href`è¡¨ç¤ºé€‰æ‹©è¿™ä¸ªæ ‡ç­¾çš„`href`å±æ€§ã€‚

3. `.get()`: è¿™ä¸ªæ–¹æ³•ç”¨äºè·å–XPathé€‰æ‹©å™¨æ‰¾åˆ°çš„ç¬¬ä¸€ä¸ªåŒ¹é…é¡¹çš„æ•°æ®ã€‚

ç»¼ä¸Šæ‰€è¿°ï¼Œè¿™è¡Œä»£ç çš„ä½œç”¨æ˜¯ä»ä¸‹è½½çš„ç½‘é¡µä¸­æ‰¾å‡ºç¬¬ä¸€ä¸ª`<base>`æ ‡ç­¾çš„`href`å±æ€§å€¼ã€‚åœ¨è¿™ä¸ªç¤ºä¾‹ä¸­ï¼Œè¿”å›çš„ç»“æœæ˜¯`'http://example.com/'`ï¼Œè¿™æ„å‘³ç€åœ¨çˆ¬å–çš„é¡µé¢ä¸­ï¼Œ`<base>`æ ‡ç­¾çš„`href`å±æ€§è¢«è®¾ç½®ä¸º`http://example.com/`ã€‚è¿™é€šå¸¸ç”¨äºè§£æç½‘é¡µçš„åŸºç¡€URLæˆ–æ ¹URLã€‚<br>


```bash
>>> response.css("base::attr(href)").get()
'http://example.com/'
```

`response` : æ˜¯ Scrapy æ¡†æ¶ä¸­çš„ä¸€ä¸ªå“åº”å¯¹è±¡ï¼ŒåŒ…å«äº†ä¸€ä¸ªç½‘é¡µçš„å…¨éƒ¨å†…å®¹ã€‚<br>

`.css("base::attr(href)")`: <br>

- `.css("...")` æ˜¯ Scrapy ç”¨äºé€‰æ‹©ç½‘é¡µä¸Šå…ƒç´ çš„æ–¹æ³•ï¼Œå®ƒä½¿ç”¨ CSS é€‰æ‹©å™¨æ¥å®šä½å…ƒç´ ã€‚

- `"base"` æŒ‡çš„æ˜¯ `<base>` æ ‡ç­¾ï¼Œè¿™æ˜¯ HTML ä¸­ç”¨äºæŒ‡å®šç›¸å¯¹ URL çš„åŸºç¡€è·¯å¾„çš„æ ‡ç­¾ã€‚

- `"::attr(href)"` æ˜¯ä¸€ä¸ªä¼ªç±»é€‰æ‹©å™¨ï¼Œç”¨äºè·å–è¯¥å…ƒç´ çš„ `href` å±æ€§å€¼ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå®ƒæå–çš„æ˜¯ `<base>` æ ‡ç­¾çš„ `href` å±æ€§å€¼ã€‚

`.get()`: è¿™ä¸ªæ–¹æ³•ä»é€‰æ‹©çš„å…ƒç´ ä¸­æå–å‡ºç¬¬ä¸€ä¸ªåŒ¹é…é¡¹çš„æ•°æ®ã€‚å¦‚æœä½ æƒ³è·å–æ‰€æœ‰åŒ¹é…é¡¹ï¼Œå¯ä»¥ä½¿ç”¨ `.getall()` æ–¹æ³•ã€‚<br>

æ€»çš„æ¥è¯´ï¼Œè¿™æ®µä»£ç çš„ç›®çš„æ˜¯æå–ç½‘é¡µä¸­ `<base>` æ ‡ç­¾çš„ `href` å±æ€§å€¼ã€‚åœ¨è¿™ä¸ªç¤ºä¾‹ä¸­ï¼Œå®ƒè¿”å›çš„æ˜¯ `'http://example.com/'`ï¼Œè¿™é€šå¸¸æ˜¯ç”¨æ¥è§£æé¡µé¢ä¸Šçš„ç›¸å¯¹ URL çš„åŸºç¡€ URLã€‚<br>

```bash
>>> response.css("base").attrib["href"]
'http://example.com/'
```

```bash
>>> response.xpath('//a[contains(@href, "image")]/@href').getall()
['image1.html',
'image2.html',
'image3.html',
'image4.html',
'image5.html']
```

1. `response`: è¿™æ˜¯ä¸€ä¸ª Scrapy Response å¯¹è±¡ï¼Œä»£è¡¨äº†ä¸€ä¸ªå·²ç»çˆ¬å–åˆ°çš„ç½‘é¡µã€‚

2. `.xpath('//a[contains(@href, "image")]')`: è¿™æ˜¯ä¸€ä¸ª XPath æŸ¥è¯¢ã€‚å®ƒçš„ä½œç”¨æ˜¯åœ¨æ•´ä¸ªæ–‡æ¡£ä¸­æŸ¥æ‰¾æ‰€æœ‰ `<a>` æ ‡ç­¾ï¼Œå…¶ä¸­çš„ `href` å±æ€§åŒ…å«äº†å­—ç¬¦ä¸²â€œimageâ€ã€‚`//` è¡¨ç¤ºåœ¨æ•´ä¸ªæ–‡æ¡£ä¸­æŸ¥æ‰¾ï¼Œ`a` æ˜¯ HTML ä¸­ç”¨äºåˆ›å»ºé“¾æ¥çš„æ ‡ç­¾ï¼Œ`contains(@href, "image")` æ˜¯ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºæŸ¥æ‰¾ `href` å±æ€§ä¸­åŒ…å«â€œimageâ€å­—æ ·çš„ `<a>` æ ‡ç­¾ã€‚

3. `/@href`: è¿™éƒ¨åˆ†ä»ä¸Šé¢æ‰¾åˆ°çš„ `<a>` æ ‡ç­¾ä¸­æå– `href` å±æ€§çš„å€¼ã€‚

4. `.getall()`: è¿™æ˜¯ Scrapy ä¸­çš„ä¸€ä¸ªæ–¹æ³•ï¼Œç”¨äºä» XPath æŸ¥è¯¢ç»“æœä¸­æå–æ‰€æœ‰åŒ¹é…é¡¹çš„å€¼ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œå®ƒå°†ä¼šæå–æ‰€æœ‰ç¬¦åˆæ¡ä»¶çš„ `href` å±æ€§å€¼ã€‚

æ€»çš„æ¥è¯´ï¼Œè¿™ä¸ªç‰¹å®šçš„ä»£ç ç‰‡æ®µæ˜¯ç”¨æ¥æå–ç½‘é¡µä¸­æ‰€æœ‰åŒ…å«â€œimageâ€å­—æ ·åœ¨å…¶ href å±æ€§ä¸­çš„ `<a>` æ ‡ç­¾çš„ href å€¼ã€‚<br>

```bash
>>> response.css("a[href*=image]::attr(href)").getall()
['image1.html',
'image2.html',
'image3.html',
'image4.html',
'image5.html']
```

`.css("a[href*=image]::attr(href)")` æ˜¯ä¸€ä¸ªCSSé€‰æ‹©å™¨ã€‚è¿™é‡Œçš„æ„æ€æ˜¯ï¼šâ€œé€‰æ‹©æ‰€æœ‰ `href` å±æ€§ä¸­åŒ…å« 'image' å­—ç¬¦ä¸²çš„ `<a>` æ ‡ç­¾ï¼Œå¹¶è·å–è¿™äº›æ ‡ç­¾çš„ `href` å±æ€§å€¼ã€‚â€<br>

- `a` è¡¨ç¤ºé€‰æ‹©æ‰€æœ‰çš„é”šç‚¹ï¼ˆé“¾æ¥ï¼‰æ ‡ç­¾ `<a>`ã€‚

- `[href*=image]` æ˜¯ä¸€ä¸ªå±æ€§é€‰æ‹©å™¨ï¼Œç”¨äºé€‰æ‹©é‚£äº› `href` å±æ€§ä¸­åŒ…å«â€œimageâ€æ–‡æœ¬çš„ `<a>` æ ‡ç­¾ã€‚æ˜Ÿå· `*=` è¡¨ç¤ºå±æ€§å€¼åŒ…å«ç‰¹å®šå­—ç¬¦ä¸²ã€‚

- `::attr(href)` ç”¨äºè·å–æ¯ä¸ªç¬¦åˆæ¡ä»¶çš„ `<a>` æ ‡ç­¾çš„ `href` å±æ€§å€¼ã€‚

`.getall()` æ˜¯ä¸€ä¸ªæ–¹æ³•ï¼Œç”¨äºè·å–æ‰€æœ‰åŒ¹é…çš„å…ƒç´ çš„åˆ—è¡¨ï¼Œè€Œä¸åªæ˜¯ç¬¬ä¸€ä¸ªåŒ¹é…é¡¹ã€‚<br>

æ€»çš„æ¥è¯´ï¼Œè¿™æ®µä»£ç çš„ä½œç”¨æ˜¯ä»HTMLä¸­æå–æ‰€æœ‰ `<a>` æ ‡ç­¾çš„ `href` å±æ€§å€¼ï¼Œè¿™äº›æ ‡ç­¾çš„ `href` å±æ€§åŒ…å«å­—ç¬¦ä¸²â€œimageâ€ã€‚<br>

æ ¹æ®ä¸Šé¢æä¾›çš„HTMLç¤ºä¾‹ï¼Œå®ƒä¼šè¿”å›ä»¥ä¸‹åˆ—è¡¨ï¼š<br>

```python
['image1.html', 'image2.html', 'image3.html', 'image4.html', 'image5.html']
```

è¿™äº›æ˜¯åŒ…å«åœ¨`<div id='images'>`å†…éƒ¨çš„äº”ä¸ªå›¾ç‰‡é“¾æ¥çš„åœ°å€ã€‚<br>


```bash
response.xpath('//a[contains(@href, "image")]/img/@src').getall()
['image1_thumb.jpg',
'image2_thumb.jpg',
'image3_thumb.jpg',
'image4_thumb.jpg',
'image5_thumb.jpg']
```

#### æ‹“å±•-XPathè¯­æ³•è§£é‡Š:

```bash
response.xpath('//a[contains(@href, "image")]/img/@src').getall()
['image1_thumb.jpg',
'image2_thumb.jpg',
'image3_thumb.jpg',
'image4_thumb.jpg',
'image5_thumb.jpg']
```

è¿™é‡Œæ ¹æ®ä¸Šè¿°è¯­æ³•è¯¦ç»†è§£é‡Šè¿™ä¸ªXPathæŸ¥è¯¢çš„å„ä¸ªç»„æˆéƒ¨åˆ†ã€‚XPathæ˜¯ä¸€ç§åœ¨XMLå’ŒHTMLæ–‡æ¡£ä¸­æŸ¥æ‰¾ä¿¡æ¯çš„è¯­è¨€ã€‚åœ¨ä¸Šè¿°ä¾‹å­ä¸­ï¼ŒXPathç”¨äºå®šä½å’Œæå–ç‰¹å®šçš„HTMLå…ƒç´ ã€‚ä¸‹é¢æ˜¯å„ä¸ªéƒ¨åˆ†çš„è¯¦ç»†è§£é‡Šï¼š<br>

1. `//`

- `//` æ˜¯XPathçš„è¯­æ³•ï¼Œç”¨äºé€‰æ‹©æ–‡æ¡£ä¸­çš„èŠ‚ç‚¹ï¼Œè€Œä¸è€ƒè™‘å®ƒä»¬åœ¨æ–‡æ¡£ä¸­çš„ä½ç½®ã€‚ä¾‹å¦‚ï¼Œ`//div` ä¼šé€‰æ‹©æ–‡æ¡£ä¸­çš„æ‰€æœ‰`<div>`å…ƒç´ ã€‚

2. `[]`

- `[]` ç”¨äº**åœ¨XPathä¸­æ·»åŠ æ¡ä»¶**ã€‚å®ƒå¯ä»¥ç”¨æ¥è¿›ä¸€æ­¥ç»†åŒ–ä½ æƒ³è¦é€‰æ‹©çš„èŠ‚ç‚¹ã€‚ä¾‹å¦‚ï¼Œ`//a[condition]` ä¼šé€‰æ‹©æ»¡è¶³`condition`æ¡ä»¶çš„æ‰€æœ‰`<a>`å…ƒç´ ã€‚

3. `contains`

- `contains()` æ˜¯ä¸€ä¸ªXPathå‡½æ•°ï¼Œç”¨äºæ£€æŸ¥ä¸€ä¸ªå­—ç¬¦ä¸²æ˜¯å¦åŒ…å«å¦ä¸€ä¸ªå­—ç¬¦ä¸²ã€‚åœ¨ä½ çš„ä¾‹å­ä¸­ï¼Œ`contains(@href, "image")` ç”¨äºæ£€æŸ¥`href`å±æ€§çš„å€¼æ˜¯å¦åŒ…å«å­—ç¬¦ä¸²â€œimageâ€ã€‚

4. `@`

- `@` åœ¨XPathä¸­ç”¨äºæŒ‡ä»£ä¸€ä¸ªå±æ€§ã€‚ä¾‹å¦‚ï¼Œ`@href` è¡¨ç¤ºé€‰æ‹©`href`å±æ€§ã€‚

5. `/`

- `/` åœ¨XPathä¸­ç”¨ä½œè·¯å¾„åˆ†éš”ç¬¦ã€‚å®ƒ**ç”¨äºé€‰æ‹©ç›´æ¥çš„å­èŠ‚ç‚¹**ã€‚ä¾‹å¦‚ï¼Œ`div/a` ä¼šé€‰æ‹©æ‰€æœ‰`<div>`å…ƒç´ çš„ç›´æ¥å­`<a>`å…ƒç´ ã€‚

å°†è¿™äº›ç»„ä»¶æ”¾åœ¨ä¸€èµ·ç†è§£ï¼š<br>

- `//a[contains(@href, "image")]`ï¼šè¿™ä¸ªè¡¨è¾¾å¼é€‰æ‹©æ–‡æ¡£ä¸­æ‰€æœ‰`<a>`å…ƒç´ ï¼Œè¿™äº›`<a>`å…ƒç´ çš„`href`å±æ€§ä¸­åŒ…å«å­—ç¬¦ä¸²â€œimageâ€ã€‚`//` è¡¨ç¤ºåœ¨æ•´ä¸ªæ–‡æ¡£ä¸­æœç´¢ï¼Œ`a` æŒ‡å®šè¦æœç´¢çš„å…ƒç´ ç±»å‹ï¼Œ`contains(@href, "image")` æ˜¯ä¸€ä¸ªæ¡ä»¶ï¼Œè¡¨ç¤ºä»…é€‰æ‹©é‚£äº›`href`å±æ€§ä¸­åŒ…å«â€œimageâ€çš„`<a>`å…ƒç´ ã€‚

- `/img/@src`ï¼šè¿™ä¸ªè¡¨è¾¾å¼æ˜¯åœ¨ä¸Šé¢æ‰¾åˆ°çš„æ¯ä¸ª`<a>`å…ƒç´ çš„åŸºç¡€ä¸Šè¿›ä¸€æ­¥å®šä½ã€‚å®ƒé€‰æ‹©è¿™äº›`<a>`å…ƒç´ çš„å­å…ƒç´ `<img>`ï¼Œå¹¶æå–è¿™äº›`<img>`å…ƒç´ çš„`src`å±æ€§ã€‚`/img` æŒ‡å®šé€‰æ‹©`<img>`å­å…ƒç´ ï¼Œ`@src` è¡¨ç¤ºæå–è¿™äº›`<img>`å…ƒç´ çš„`src`å±æ€§å€¼ã€‚

æ‰€ä»¥ï¼Œæ•´ä¸ªè¡¨è¾¾å¼`//a[contains(@href, "image")]/img/@src` ä¼šæ‰¾åˆ°æ–‡æ¡£ä¸­æ‰€æœ‰å…¶`href`å±æ€§åŒ…å«â€œimageâ€çš„`<a>`å…ƒç´ ï¼Œç„¶åä»è¿™äº›`<a>`å…ƒç´ çš„å­å…ƒç´ `<img>`ä¸­æå–`src`å±æ€§å€¼ã€‚è¿™é€šå¸¸æ˜¯å›¾ç‰‡çš„URLã€‚<br>


```bash
response.css("a[href*=image] img::attr(src)").getall()
['image1_thumb.jpg',
'image2_thumb.jpg',
'image3_thumb.jpg',
'image4_thumb.jpg',
'image5_thumb.jpg']
```

å‘½ä»¤è§£é‡Š:<br>

1. `[]`ï¼ˆæ–¹æ‹¬å·ï¼‰ï¼šç”¨äºå±æ€§é€‰æ‹©å™¨ã€‚å½“ä½ éœ€è¦é€‰æ‹©å…·æœ‰ç‰¹å®šå±æ€§çš„HTMLå…ƒç´ æ—¶ï¼Œä½ å¯ä»¥åœ¨å…ƒç´ ååä½¿ç”¨æ–¹æ‹¬å·æ¥æŒ‡å®šå±æ€§ã€‚ä¾‹å¦‚ï¼Œåœ¨`a[href]`ä¸­ï¼Œå®ƒé€‰æ‹©æ‰€æœ‰å…·æœ‰`href`å±æ€§çš„`<a>`å…ƒç´ ã€‚

2. `*=`ï¼šè¿™æ˜¯ä¸€ç§å±æ€§é€‰æ‹©å™¨çš„æ“ä½œç¬¦ï¼Œç”¨äºé€‰æ‹©å±æ€§å€¼ä¸­åŒ…å«ç‰¹å®šæ–‡æœ¬çš„å…ƒç´ ã€‚ä¾‹å¦‚ï¼Œåœ¨`[href*=image]`ä¸­ï¼Œ`*=`æ“ä½œç¬¦ç”¨äºé€‰æ‹©é‚£äº›`href`å±æ€§ä¸­åŒ…å«"image"æ–‡æœ¬çš„å…ƒç´ ã€‚è¿™æ„å‘³ç€å®ƒä¼šåŒ¹é…ä»»ä½•`href`å€¼ä¸­å«æœ‰"image"è¿™ä¸ªå­ä¸²çš„å…ƒç´ ã€‚

3. `::`ï¼šç”¨äºä¼ªå…ƒç´ é€‰æ‹©å™¨ã€‚åœ¨CSSä¸­ï¼Œä¼ªå…ƒç´ ç”¨äºæ·»åŠ ä¸€äº›ç‰¹æ®Šçš„æ•ˆæœæˆ–è€…é€‰æ‹©æŸäº›éƒ¨åˆ†çš„å…ƒç´ ï¼Œè€Œä¸æ˜¯æ•´ä¸ªå…ƒç´ ã€‚ä¾‹å¦‚ï¼Œ`::attr(src)`æ˜¯ä¸€ä¸ªä¼ªå…ƒç´ é€‰æ‹©å™¨ï¼Œç”¨äºè·å–å…ƒç´ çš„`src`å±æ€§ã€‚åœ¨ä½ çš„ä»£ç ç¤ºä¾‹ä¸­ï¼Œ`img::attr(src)`é€‰æ‹©äº†æ‰€æœ‰`<img>`å…ƒç´ ï¼Œå¹¶è·å–äº†å®ƒä»¬çš„`src`å±æ€§å€¼ã€‚

æ‰€ä»¥ï¼Œåœ¨ä½ çš„Scrapyä»£ç ç¤ºä¾‹ `response.css("a[href*=image] img::attr(src)").getall()` ä¸­ï¼Œå®ƒé¦–å…ˆé€‰æ‹©æ‰€æœ‰`href`å±æ€§ä¸­åŒ…å«"image"çš„`<a>`å…ƒç´ ï¼Œç„¶ååœ¨è¿™äº›`<a>`å…ƒç´ ä¸­é€‰æ‹©`<img>`å…ƒç´ ï¼Œå¹¶æå–è¿™äº›`<img>`å…ƒç´ çš„`src`å±æ€§å€¼ã€‚


#### æ‹“å±•--"::" é€‰æ‹© "ä¼ªå…ƒç´ " :

ç†è§£ä¼ªå…ƒç´ çš„æ¦‚å¿µå¯¹äºæŒæ¡CSSé€‰æ‹©å™¨éå¸¸é‡è¦ã€‚ä¼ªå…ƒç´ æ˜¯CSSçš„ä¸€ä¸ªç‰¹æ€§ï¼Œå®ƒç”¨äº**æŒ‡å®šé¡µé¢ä¸ŠæŸä¸ªå…ƒç´ çš„æŸä¸ªéƒ¨åˆ†æˆ–è€…æ·»åŠ ç‰¹å®šçš„æ•ˆæœ**ã€‚ä¼ªå…ƒç´ ä¸æ˜¯HTMLæ–‡æ¡£ä¸­å®é™…å®šä¹‰çš„å…ƒç´ ï¼Œè€Œæ˜¯CSSåˆ›å»ºçš„è™šæ‹Ÿå…ƒç´ ï¼Œç”¨äºåº”ç”¨æ ·å¼æˆ–é€‰æ‹©æ–‡æ¡£ä¸­çš„ç‰¹å®šå†…å®¹ã€‚<br>

åœ¨Scrapyä¸­ï¼Œä½¿ç”¨CSSé€‰æ‹©å™¨æ—¶ï¼Œä¼ªå…ƒç´ `::`çš„ä½¿ç”¨ç•¥æœ‰ä¸åŒã€‚åœ¨Scrapyä¸­ï¼Œä¼ªå…ƒç´ ä¸»è¦ç”¨äºæå–å…ƒç´ çš„ç‰¹å®šå±æ€§ã€‚ä¾‹å¦‚ï¼š<br>

- `::text`ï¼šè¿™ä¸ªä¼ªå…ƒç´ ç”¨äºé€‰æ‹©å…ƒç´ çš„æ–‡æœ¬å†…å®¹ã€‚ä¾‹å¦‚ï¼Œ`p::text`ä¼šé€‰æ‹©æ‰€æœ‰`<p>`æ ‡ç­¾ä¸­çš„æ–‡æœ¬(text)ã€‚

- `::attr(attribute-name)`ï¼šè¿™ä¸ªä¼ªå…ƒç´ ç”¨äºæå–å…ƒç´ çš„ç‰¹å®šå±æ€§å€¼ã€‚ä¾‹å¦‚ï¼Œ`img::attr(src)`ä¼šé€‰æ‹©æ‰€æœ‰`<img>`æ ‡ç­¾çš„`src`å±æ€§å€¼ã€‚

åœ¨ä½ çš„Scrapyä»£ç ç¤ºä¾‹ä¸­ï¼Œ`img::attr(src)`è¡¨ç¤ºé€‰æ‹©æ¯ä¸ª`<img>`å…ƒç´ çš„`src`å±æ€§ã€‚è¿™æ˜¯ä¸€ä¸ªéå¸¸æœ‰ç”¨çš„ç‰¹æ€§ï¼Œå› ä¸ºå®ƒå…è®¸ä½ ç›´æ¥æå–HTMLå…ƒç´ çš„å±æ€§ï¼Œè€Œä¸ä»…ä»…æ˜¯å…ƒç´ æœ¬èº«ã€‚è¿™åœ¨ç½‘ç»œçˆ¬è™«å’Œæ•°æ®æå–ä¸­å°¤å…¶æœ‰ç”¨ï¼Œå› ä¸ºå®ƒæä¾›äº†ä¸€ç§ç®€æ´çš„æ–¹æ³•æ¥è·å–è¯¸å¦‚å›¾åƒé“¾æ¥ã€é¡µé¢é“¾æ¥ç­‰ä¿¡æ¯ã€‚<br>


### CSS é€‰æ‹©å™¨çš„æ‰©å±•(Extensions to CSS Selectors):

æ ¹æ® W3C æ ‡å‡†ï¼ŒCSS é€‰æ‹©å™¨ä¸æ”¯æŒé€‰æ‹©æ–‡æœ¬èŠ‚ç‚¹(text nodes)æˆ–å±æ€§å€¼(attribute values)ã€‚ä½†åœ¨ç½‘é¡µæŠ“å–ç¯å¢ƒä¸­ï¼Œé€‰æ‹©è¿™äº›å†…å®¹æ˜¯éå¸¸å¿…è¦çš„ï¼Œå› æ­¤**Scrapy(parsel)** å®ç°äº†ä¸€äº›éæ ‡å‡†çš„ä¼ªå…ƒç´ ï¼š<br>

- è¦é€‰æ‹©æ–‡æœ¬èŠ‚ç‚¹(text nodes)ï¼Œè¯·ä½¿ç”¨ `::text`

- è¦é€‰æ‹©å±æ€§å€¼(attribute values)ï¼Œè¯·ä½¿ç”¨ `::attr(name)`ï¼Œå…¶ä¸­ `name` æ˜¯ä½ æƒ³è¦è·å–å…¶å€¼çš„å±æ€§çš„åç§°

ğŸš¨ğŸš¨ğŸš¨è­¦å‘Šï¼š<br>

è¿™äº›ä¼ªå…ƒç´ æ˜¯ä¸“é—¨ä¸º `Scrapy(parsel)` è®¾è®¡çš„ã€‚å®ƒä»¬å¾ˆå¯èƒ½ä¸é€‚ç”¨äºå…¶ä»–åº“ï¼Œå¦‚ `lxml` æˆ– `PyQuery`ã€‚<br>

ç¤ºä¾‹ï¼š<br>

- `title::text` é€‰æ‹© <title> å…ƒç´ çš„åä»£ä¸­çš„å­æ–‡æœ¬èŠ‚ç‚¹(text nodes)ï¼š

```bash
>>> response.css("title::text").get()
'Example website'
```

`*::text`é€‰æ‹©å½“å‰é€‰æ‹©å™¨ä¸Šä¸‹æ–‡çš„æ‰€æœ‰åä»£æ–‡æœ¬èŠ‚ç‚¹ã€‚<br>

```bash
>>> response.css("#images *::text").getall()
['\n   ',
'Name: My image 1 ',
'\n   ',
'Name: My image 2 ',
'\n   ',
'Name: My image 3 ',
'\n   ',
'Name: My image 4 ',
'\n   ',
'Name: My image 5 ',
'\n  ']
```

å‘½ä»¤è§£é‡Š:<br>

1. **`response`**ï¼šè¿™æ˜¯Scrapyä¸­çš„ä¸€ä¸ªå¯¹è±¡ï¼Œä»£è¡¨äº†çˆ¬è™«è·å–çš„ç½‘é¡µçš„å“åº”ã€‚å®ƒåŒ…å«äº†ä½ è¯·æ±‚çš„ç½‘é¡µçš„å…¨éƒ¨å†…å®¹ã€‚

2. **`css` æ–¹æ³•**ï¼šè¿™æ˜¯Scrapy Responseå¯¹è±¡çš„ä¸€ä¸ªæ–¹æ³•ï¼Œç”¨äºå¯¹ç½‘é¡µå†…å®¹åº”ç”¨CSSé€‰æ‹©å™¨ã€‚CSSé€‰æ‹©å™¨æ˜¯ä¸€ç§è¡¨è¾¾å¼ï¼Œç”¨äºä»HTMLæ–‡æ¡£ä¸­é€‰æ‹©å…ƒç´ ã€‚åœ¨è¿™ä¸ªä¸Šä¸‹æ–‡ä¸­ï¼Œå®ƒè¢«ç”¨æ¥æ‰¾åˆ°ç‰¹å®šçš„HTMLå…ƒç´ ã€‚

3. **`"#images *"`**ï¼šè¿™æ˜¯ä¸€ä¸ªCSSé€‰æ‹©å™¨ã€‚`#images` é€‰æ‹©äº†æ‰€æœ‰idä¸º`images`çš„å…ƒç´ ï¼ˆé€šå¸¸æ˜¯ä¸€ä¸ªdivæˆ–å…¶ä»–å®¹å™¨å…ƒç´ ï¼‰ã€‚æ˜Ÿå·ï¼ˆ`*`ï¼‰æ˜¯ä¸€ä¸ªé€šé…ç¬¦ï¼Œè¡¨ç¤ºé€‰æ‹©`#images`å…ƒç´ å†…çš„æ‰€æœ‰å­å…ƒç´ ã€‚æ‰€ä»¥ï¼Œè¿™ä¸ªé€‰æ‹©å™¨çš„ç›®çš„æ˜¯é€‰å–idä¸º`images`çš„å…ƒç´ ä¸‹çš„æ‰€æœ‰å†…å®¹ã€‚

```html
<div id='images'>
    <a href='image1.html'>Name: My image 1 <br /><img src='image1_thumb.jpg' alt='image1'/></a>
    ... ...
</div>
```

4. **`::text`**ï¼šè¿™æ˜¯ä¸€ä¸ªCSSä¼ªå…ƒç´ é€‰æ‹©å™¨ï¼Œç”¨äºé€‰æ‹©å…ƒç´ çš„æ–‡æœ¬å†…å®¹ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œå®ƒä¼šé€‰æ‹©æ‰€æœ‰ç”±`#images *`é€‰æ‹©å™¨æ‰¾åˆ°çš„å…ƒç´ çš„æ–‡æœ¬å†…å®¹ã€‚

5. **`getall()` æ–¹æ³•**ï¼šè¿™ä¸ªæ–¹æ³•æ˜¯ç”¨æ¥æå–æ‰€æœ‰åŒ¹é…çš„é€‰æ‹©å™¨çš„æ•°æ®ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œå®ƒä¼šè¿”å›æ‰€æœ‰ç”±`#images *::text`é€‰æ‹©å™¨æ‰¾åˆ°çš„æ–‡æœ¬å†…å®¹çš„åˆ—è¡¨ã€‚

æ€»ç»“æ¥è¯´ï¼Œè¿™ä¸ªå‘½ä»¤åœ¨Scrapyä¸­çš„ä½œç”¨æ˜¯é€‰å–idä¸º`images`çš„å…ƒç´ å†…çš„æ‰€æœ‰å­å…ƒç´ çš„æ–‡æœ¬å†…å®¹ï¼Œå¹¶å°†è¿™äº›æ–‡æœ¬å†…å®¹ä½œä¸ºä¸€ä¸ªåˆ—è¡¨è¿”å›ã€‚è¿™åœ¨çˆ¬è™«ä¸­å¸¸ç”¨äºæå–é¡µé¢ç‰¹å®šéƒ¨åˆ†çš„æ‰€æœ‰æ–‡æœ¬æ•°æ®ã€‚<br>


#### æ‹“å±•-CSSä¸­ `#` ç”¨æ³•è§£æ:

`#` ç¬¦å·åœ¨CSSé€‰æ‹©å™¨ä¸­ç”¨æ¥æŒ‡å®šä¸€ä¸ªç‰¹å®šçš„idã€‚åœ¨HTMLä¸­ï¼Œidå±æ€§æ˜¯ç”¨æ¥å”¯ä¸€æ ‡è¯†é¡µé¢ä¸Šçš„ä¸€ä¸ªå…ƒç´ ã€‚æ¯ä¸ªidåœ¨ä¸€ä¸ªHTMLæ–‡æ¡£ä¸­åº”è¯¥æ˜¯ç‹¬ä¸€æ— äºŒçš„ï¼Œè¿™æ„å‘³ç€æ²¡æœ‰ä¸¤ä¸ªå…ƒç´ åº”è¯¥æœ‰ç›¸åŒçš„idå€¼ã€‚<br>

å½“ä½ åœ¨CSSé€‰æ‹©å™¨ä¸­ä½¿ç”¨ `#` ç¬¦å·ï¼Œåé¢ç´§è·Ÿä¸€ä¸ªidçš„å€¼ï¼Œé€‰æ‹©å™¨å°±ä¼šåŒ¹é…æ‰€æœ‰å…·æœ‰é‚£ä¸ªç‰¹å®šidçš„å…ƒç´ ã€‚ä¾‹å¦‚ï¼š<br>

- `#header` ä¼šé€‰æ‹©IDä¸º `header` çš„å…ƒç´ ã€‚
- `#main-content` ä¼šé€‰æ‹©IDä¸º `main-content` çš„å…ƒç´ ã€‚

è¿™ä½¿å¾—idé€‰æ‹©å™¨éå¸¸æœ‰ç”¨äºå®šä½é¡µé¢ä¸Šçš„ç‰¹å®šå…ƒç´ ï¼Œå°¤å…¶æ˜¯å½“ä½ åªå¯¹æ–‡æ¡£ä¸­çš„ä¸€ä¸ªç‰¹å®šéƒ¨åˆ†æ„Ÿå…´è¶£æ—¶ã€‚åœ¨Webå¼€å‘å’Œç½‘é¡µæ•°æ®æŠ“å–ä¸­ï¼Œåˆ©ç”¨IDé€‰æ‹©å™¨å¯ä»¥ç²¾ç¡®åœ°é€‰æ‹©éœ€è¦æ“ä½œæˆ–æå–æ•°æ®çš„éƒ¨åˆ†ã€‚<br>

äº†è§£HTMLçš„åŸºç¡€å¯¹ç†è§£idçš„å«ä¹‰éå¸¸é‡è¦ã€‚HTMLï¼ˆè¶…æ–‡æœ¬æ ‡è®°è¯­è¨€ï¼‰æ˜¯ç”¨äºåˆ›å»ºç½‘é¡µçš„æ ‡å‡†æ ‡è®°è¯­è¨€ã€‚åœ¨HTMLä¸­ï¼Œä¸€ä¸ªç½‘é¡µç”±è®¸å¤šä¸åŒçš„å…ƒç´ æ„æˆï¼Œæ¯”å¦‚æ®µè½ã€æ ‡é¢˜ã€å›¾ç‰‡ç­‰ã€‚è¿™äº›å…ƒç´ é€šè¿‡æ ‡ç­¾ï¼ˆå¦‚ `<p>`ã€`<h1>`ã€`<img>` ç­‰ï¼‰æ¥å®šä¹‰ã€‚<br>

åœ¨è¿™äº›HTMLå…ƒç´ ä¸­ï¼Œ`id` å±æ€§æ˜¯ä¸€ä¸ªç‰¹åˆ«çš„å±æ€§ï¼Œç”¨äºç»™å…ƒç´ æŒ‡å®šä¸€ä¸ªå”¯ä¸€çš„æ ‡è¯†ç¬¦ï¼ˆIDï¼‰ã€‚è¿™ä¸ªIDåœ¨æ•´ä¸ªHTMLæ–‡æ¡£ä¸­å¿…é¡»æ˜¯å”¯ä¸€çš„ã€‚é€šè¿‡ä½¿ç”¨è¿™ä¸ªIDï¼Œä½ å¯ä»¥å¯¹ç‰¹å®šçš„HTMLå…ƒç´ è¿›è¡Œå®šä½å’Œæ“ä½œã€‚è¿™åœ¨ç½‘é¡µè®¾è®¡å’Œå¼€å‘ä¸­éå¸¸æœ‰ç”¨ï¼Œå°¤å…¶æ˜¯åœ¨ä½¿ç”¨CSSï¼ˆç”¨äºè®¾ç½®ç½‘é¡µæ ·å¼çš„è¯­è¨€ï¼‰å’ŒJavaScriptï¼ˆç½‘é¡µä¸Šç”¨äºæ·»åŠ äº¤äº’æ€§çš„ç¼–ç¨‹è¯­è¨€ï¼‰æ—¶ã€‚<br>

ä¾‹å¦‚ï¼Œä¸€ä¸ªç®€å•çš„HTMLå…ƒç´ å¸¦æœ‰IDå¯èƒ½æ˜¯è¿™æ ·çš„ï¼š<br>

```html
<p id="first-paragraph">è¿™æ˜¯ä¸€ä¸ªæ®µè½ã€‚</p>
```

åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œ`<p>` æ˜¯ä¸€ä¸ªæ®µè½æ ‡ç­¾ï¼Œ`id="first-paragraph"` ç»™è¿™ä¸ªæ®µè½æŒ‡å®šäº†ä¸€ä¸ªå”¯ä¸€çš„IDï¼Œå³ `first-paragraph`ã€‚ä½ å¯ä»¥åœ¨CSSä¸­ä½¿ç”¨è¿™ä¸ªIDæ¥ç‰¹å®šåœ°é€‰æ‹©å’Œæ ·å¼åŒ–è¿™ä¸ªæ®µè½ï¼Œæˆ–è€…åœ¨JavaScriptä¸­ç”¨å®ƒæ¥è·å–æˆ–æ“ä½œè¿™ä¸ªæ®µè½çš„å†…å®¹ã€‚<br>

å½“ä½ åœ¨CSSé€‰æ‹©å™¨ä¸­ä½¿ç”¨ `#` ç¬¦å·ï¼Œæ¯”å¦‚ `#first-paragraph`ï¼Œè¿™ä¼šé€‰æ‹©æ‰€æœ‰IDä¸º `first-paragraph` çš„å…ƒç´ ã€‚è¿™å°±æ˜¯IDåœ¨HTMLä¸­çš„åŸºæœ¬ç”¨é€”å’Œå«ä¹‰ã€‚<br>

å¦‚æœ `foo` å…ƒç´ å­˜åœ¨ä½†ä¸åŒ…å«ä»»ä½•æ–‡æœ¬ï¼ˆå³æ–‡æœ¬ä¸ºç©ºï¼‰æ—¶ï¼Œ`foo::text` å°†ä¸è¿”å›ä»»ä½•ç»“æœ:<br>

```bash
>>> response.css("img::text").getall()
[]

This means ``.css('foo::text').get()`` could return None even if an element
exists. Use ``default=''`` if you always want a string:
```

```bash
>>> response.css("img::text").get()
>>> response.css("img::text").get(default="")
''
```

`a::attr(href)` é€‰æ‹©å…¶åé“¾æ¥çš„ `href` å±æ€§å€¼:<br>

```bash
>>> response.css("a::attr(href)").getall()
['image1.html',
'image2.html',
'image3.html',
'image4.html',
'image5.html']
```

### åµŒå¥—é€‰æ‹©å™¨(Nesting selectors):


## Item Pipeline(é¡¹ç›®ç®¡é“):

After an item has been scraped by a spider, it is sent to the Item Pipeline which processes it through several components(ç»„ä»¶) that are executed sequentially(æŒ‰é¡ºåº).<br>

åœ¨ä¸€ä¸ªé¡¹ç›®è¢«çˆ¬è™«æŠ“å–ä¹‹åï¼Œå®ƒä¼šè¢«å‘é€åˆ°é¡¹ç›®ç®¡é“ï¼Œåœ¨é‚£é‡Œå®ƒä¼šé€šè¿‡å‡ ä¸ªä¾æ¬¡æ‰§è¡Œçš„ç»„ä»¶è¿›è¡Œå¤„ç†ã€‚ğŸš€ğŸš€ğŸš€<br>

Each item pipeline component (sometimes referred as just â€œItem Pipelineâ€) is a Python class that implements(å®ç°ï¼›å®æ–½ï¼›æ‰§è¡Œ) a simple method. They receive(æ¥æ”¶) an item and perform an action(æ“ä½œ) over it, also deciding if the item should continue through the pipeline or be dropped and no longer processed.<br>

æ¯ä¸ªé¡¹ç›®ç®¡é“ç»„ä»¶ï¼ˆæœ‰æ—¶ä»…ç§°ä¸ºâ€œé¡¹ç›®ç®¡é“â€ï¼‰æ˜¯ä¸€ä¸ªå®ç°äº†ç®€å•æ–¹æ³•çš„Pythonç±»ã€‚å®ƒä»¬æ¥æ”¶ä¸€ä¸ªé¡¹ç›®å¹¶å¯¹å…¶æ‰§è¡Œæ“ä½œï¼ŒåŒæ—¶å†³å®šè¯¥é¡¹ç›®æ˜¯å¦åº”è¯¥ç»§ç»­é€šè¿‡ç®¡é“æˆ–è¢«ä¸¢å¼ƒä¸å†å¤„ç†ã€‚<br>

Typical uses of item pipelines are:<br>

é¡¹ç›®ç®¡é“çš„å…¸å‹ç”¨é€”åŒ…æ‹¬ï¼š<br>

- cleansing HTML data(æ¸…æ´—HTMLæ•°æ®)

- validating(ç¡®è®¤ï¼›è¯å®ï¼›éªŒè¯) scraped data (checking that the items contain certain fields)(éªŒè¯æŠ“å–çš„æ•°æ®ï¼ˆæ£€æŸ¥é¡¹ç›®æ˜¯å¦åŒ…å«ç‰¹å®šå­—æ®µï¼‰)

- checking for duplicates (and dropping them)(æ£€æŸ¥é‡å¤é¡¹ï¼ˆå¹¶ä¸¢å¼ƒå®ƒä»¬ï¼‰)

- storing(å­˜å‚¨) the scraped item in a database(å°†æŠ“å–çš„é¡¹ç›®å­˜å‚¨åœ¨æ•°æ®åº“ä¸­)

### Writing your own item pipeline(ç¼–å†™ä½ è‡ªå·±çš„é¡¹ç›®ç®¡é“):

Each item pipeline component is a Python class that must implement the following method:<br>

æ¯ä¸ªé¡¹ç›®ç®¡é“ç»„ä»¶éƒ½æ˜¯ä¸€ä¸ªPythonç±»ï¼Œå¿…é¡»å®ç°ä»¥ä¸‹æ–¹æ³•ï¼š<br>

`process_item(self, item, spider)`<br>

This method is called for every item pipeline component.<br>

è¿™ä¸ªæ–¹æ³•ä¼šè¢«æ¯ä¸ªé¡¹ç›®ç®¡é“ç»„ä»¶è°ƒç”¨ã€‚<br>

item is an [item object](https://docs.scrapy.org/en/latest/topics/items.html#item-types), see [Supporting All Item Types](https://docs.scrapy.org/en/latest/topics/items.html#supporting-item-types).<br>

itemæ˜¯ä¸€ä¸ªé¡¹ç›®å¯¹è±¡ï¼Œè¯·å‚é˜…æ”¯æŒæ‰€æœ‰é¡¹ç›®ç±»å‹ã€‚<br>

`process_item()` must either: return an [item object](https://docs.scrapy.org/en/latest/topics/items.html#item-types), return a `Deferred` or raise a `DropItem` exception.<br>

`process_item()` å¿…é¡»ï¼šè¿”å›ä¸€ä¸ªé¡¹ç›®å¯¹è±¡ï¼Œè¿”å›ä¸€ä¸ª `Deferred`ï¼Œæˆ–æŠ›å‡ºä¸€ä¸ª `DropItem` å¼‚å¸¸ã€‚<br>

Dropped items are no longer processed by further(åç»­çš„) pipeline components.<br>

è¢«ä¸¢å¼ƒçš„é¡¹ç›®å°†ä¸å†ç”±åç»­çš„ç®¡é“ç»„ä»¶å¤„ç†ã€‚<br>

Parameters(å‚æ•°):<br>

- item (item object) â€“ the scraped item
- itemï¼ˆé¡¹ç›®å¯¹è±¡ï¼‰- è¢«æŠ“å–çš„é¡¹ç›®

- spider (Spider object) â€“ the spider which scraped the item
- spiderï¼ˆSpiderå¯¹è±¡ï¼‰- æŠ“å–è¯¥é¡¹ç›®çš„çˆ¬è™«

ç¬”è€…çœ‹åˆ°çš„ `pipelines.py` ä¸­çš„ä»£ç å¦‚ä¸‹:<br>

```python
class QuotesbotPipeline(object):
    def process_item(self, item, spider):
        return item
```



## htmlä¸­çš„ `href` æ˜¯ä»€ä¹ˆï¼Ÿ

åœ¨HTMLï¼ˆè¶…æ–‡æœ¬æ ‡è®°è¯­è¨€ï¼‰ä¸­ï¼Œ`href` æ˜¯ä¸€ä¸ªå±æ€§ï¼Œé€šå¸¸ç”¨åœ¨ `<a>` ï¼ˆé”šç‚¹ï¼‰æ ‡ç­¾ä¸­ã€‚`href` ä»£è¡¨ "Hypertext Reference"ï¼Œå³ "è¶…æ–‡æœ¬å¼•ç”¨"ã€‚è¿™ä¸ªå±æ€§æŒ‡å®šäº†ä¸€ä¸ªé“¾æ¥çš„ç›®çš„åœ°åœ°å€ï¼Œå½“ç”¨æˆ·ç‚¹å‡»è¿™ä¸ªé“¾æ¥æ—¶ï¼Œæµè§ˆå™¨ä¼šè·³è½¬åˆ°è¿™ä¸ªåœ°å€ã€‚<br>

ä¾‹å¦‚ï¼š<br>

```html
<a href="https://www.example.com">è®¿é—®ä¾‹å­ç½‘ç«™</a>
```

åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œ`<a>` æ ‡ç­¾åˆ›å»ºäº†ä¸€ä¸ªåˆ° "https://www.example.com" çš„é“¾æ¥ã€‚å½“ç”¨æˆ·ç‚¹å‡»è¿™ä¸ªé“¾æ¥æ—¶ï¼Œä»–ä»¬çš„æµè§ˆå™¨ä¼šè·³è½¬åˆ° "Example.com" ç½‘ç«™ã€‚<br>

`href` å±æ€§ä¸ä»…é™äºç½‘é¡µé“¾æ¥ï¼Œå®ƒä¹Ÿå¯ä»¥ç”¨äºæŒ‡å‘ç”µå­é‚®ä»¶åœ°å€ã€æ–‡ä»¶ã€ç”µè¯å·ç ç­‰ã€‚ä¾‹å¦‚ï¼Œåˆ›å»ºä¸€ä¸ªæŒ‡å‘ç”µå­é‚®ä»¶çš„é“¾æ¥ï¼š<br>

```html
<a href="mailto:someone@example.com">å‘é€ç”µå­é‚®ä»¶</a>
```

åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œç‚¹å‡»é“¾æ¥å°†ä¼šæ‰“å¼€ç”¨æˆ·çš„é»˜è®¤é‚®ä»¶å®¢æˆ·ç«¯ï¼Œå¹¶åˆ›å»ºä¸€ä¸ªå‘é€åˆ° "someone@example.com" çš„æ–°é‚®ä»¶ã€‚<br>


## scrapyè¿›è¡Œçˆ¬è™«æ—¶ï¼Œä¸ºä»€ä¹ˆä½¿ç”¨yieldå…³é”®å­—ï¼Ÿ

åœ¨ä½¿ç”¨Scrapyè¿›è¡Œçˆ¬è™«æ—¶ï¼Œ`yield` å…³é”®å­—çš„ä½¿ç”¨æ˜¯å‡ºäºä»¥ä¸‹å‡ ä¸ªåŸå› ï¼š<br>

1. **å¼‚æ­¥å¤„ç†ï¼š** Scrapy æ˜¯åŸºäºTwistedï¼Œä¸€ä¸ªå¼‚æ­¥ç½‘ç»œæ¡†æ¶ã€‚`yield` å…³é”®å­—ç”¨äºç”Ÿæˆä¸€ä¸ªç”Ÿæˆå™¨ï¼ˆgeneratorï¼‰ï¼Œå…è®¸Scrapyåœ¨çˆ¬è™«è¿‡ç¨‹ä¸­æš‚åœå’Œæ¢å¤è¯·æ±‚ï¼Œè€Œä¸ä¼šé˜»å¡æ•´ä¸ªçˆ¬è™«çš„æ‰§è¡Œã€‚è¿™æ˜¯å¼‚æ­¥ç¼–ç¨‹çš„ä¸€éƒ¨åˆ†ï¼Œæœ‰åŠ©äºæé«˜çˆ¬è™«çš„æ•ˆç‡å’Œé€Ÿåº¦ã€‚

2. **å†…å­˜æ•ˆç‡ï¼š** ä½¿ç”¨ `yield`ï¼Œçˆ¬è™«åœ¨å¤„ç†å•ä¸ªé¡µé¢æˆ–è¯·æ±‚æ—¶ä¸éœ€è¦å°†æ‰€æœ‰æ•°æ®ä¸€æ¬¡æ€§åŠ è½½åˆ°å†…å­˜ä¸­ã€‚è¿™æ„å‘³ç€å³ä½¿åœ¨å¤„ç†å¤§é‡æ•°æ®æ—¶ï¼Œçˆ¬è™«ä¹Ÿèƒ½ä¿æŒè¾ƒä½çš„å†…å­˜å ç”¨ã€‚

3. **æµå¼å¤„ç†æ•°æ®ï¼š** `yield` å…è®¸Scrapyä¸€è¾¹ä¸‹è½½ç½‘é¡µï¼Œä¸€è¾¹å¤„ç†é¡µé¢å†…å®¹ã€‚è¿™ç§æµå¼å¤„ç†æ–¹å¼å¯ä»¥æ›´å¿«åœ°å¤„ç†æ•°æ®ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†åŒ…å«å¤§é‡é“¾æ¥çš„å¤§å‹ç½‘ç«™æ—¶ã€‚

4. **æ–¹ä¾¿çš„æ•°æ®å¤„ç†å’Œç®¡é“é›†æˆï¼š** é€šè¿‡ `yield` è¿”å›çš„é¡¹ç›®æˆ–è¯·æ±‚å¯ä»¥è¢«Scrapyçš„å…¶ä»–ç»„ä»¶ï¼ˆå¦‚Item Pipelineï¼‰è½»æ¾æ¥æ”¶å’Œå¤„ç†ã€‚è¿™æ ·å¯ä»¥çµæ´»åœ°å°†çˆ¬å–çš„æ•°æ®ä¼ é€’ç»™ä¸åŒçš„åå¤„ç†ç»„ä»¶ã€‚

æ€»ä¹‹ï¼Œä½¿ç”¨ `yield` å…³é”®å­—å¯ä»¥è®©Scrapyçˆ¬è™«æ›´åŠ é«˜æ•ˆã€çµæ´»ï¼Œå¹¶ä¸”èƒ½å¤Ÿæ›´å¥½åœ°ç®¡ç†èµ„æºï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤§è§„æ¨¡æ•°æ®æŠ“å–æ—¶ã€‚<br>


## åœ¨çˆ¬è™«ä¸­ï¼ŒCSSå’ŒXPathæ˜¯ä»€ä¹ˆå…³ç³»ï¼Ÿ

åœ¨çˆ¬è™«é¢†åŸŸï¼ŒCSSé€‰æ‹©å™¨å’ŒXPathæ˜¯ä¸¤ç§å¸¸è§çš„æ–¹æ³•ï¼Œç”¨äºä»HTMLæ–‡æ¡£ä¸­æå–ä¿¡æ¯ã€‚è™½ç„¶å®ƒä»¬éƒ½ç”¨äºç›¸åŒçš„ç›®çš„ï¼Œä½†å®ƒä»¬åœ¨è¯­æ³•å’Œä½¿ç”¨æ–¹å¼ä¸Šæœ‰æ‰€ä¸åŒã€‚<br>

1. **CSSé€‰æ‹©å™¨**ï¼š

- CSSé€‰æ‹©å™¨æ˜¯ç”¨äºé€‰æ‹©HTMLæ–‡æ¡£ä¸­å…ƒç´ çš„æ¨¡å¼ï¼Œä»¥ä¾¿å¯¹å…ƒç´ åº”ç”¨æ ·å¼ã€‚åœ¨çˆ¬è™«ä¸­ï¼ŒCSSé€‰æ‹©å™¨è¢«ç”¨æ¥å®šä½å’Œæå–HTMLä¸­çš„ç‰¹å®šæ•°æ®ã€‚

- CSSé€‰æ‹©å™¨ç®€æ´æ˜äº†ï¼Œæ˜“äºå­¦ä¹ ï¼Œç‰¹åˆ«æ˜¯å¦‚æœä½ å·²ç»ç†Ÿæ‚‰å‰ç«¯å¼€å‘ã€‚

- ä¾‹å¦‚ï¼Œè¦é€‰æ‹©æ‰€æœ‰çš„`<h1>`æ ‡ç­¾ï¼Œä½ ä¼šä½¿ç”¨`h1`ä½œä¸ºé€‰æ‹©å™¨ã€‚

2. **XPath**ï¼š

- XPathï¼ˆXMLè·¯å¾„è¯­è¨€ï¼‰æ˜¯ä¸€ç§åœ¨XMLæ–‡æ¡£ä¸­æŸ¥æ‰¾ä¿¡æ¯çš„è¯­è¨€ï¼Œå®ƒä¹Ÿå¯ä»¥ç”¨äºHTMLæ–‡æ¡£ï¼Œå› ä¸ºHTMLæ˜¯XMLçš„ä¸€ä¸ªå­é›†ã€‚

- XPathéå¸¸å¼ºå¤§å’Œçµæ´»ï¼Œå…è®¸ä½ è¿›è¡Œéå¸¸å…·ä½“ä¸”å¤æ‚çš„æŸ¥è¯¢ã€‚

- ä¾‹å¦‚ï¼Œè¦é€‰æ‹©æ‰€æœ‰çš„`<h1>`æ ‡ç­¾ï¼Œä½ ä¼šä½¿ç”¨`//h1`ã€‚

**åœ¨Scrapyä¸­ï¼Œä½ å¯ä»¥é€‰æ‹©ä½¿ç”¨CSSé€‰æ‹©å™¨æˆ–XPathæ¥æå–æ•°æ®ã€‚Scrapyä¸ºä¸¤è€…æä¾›äº†è‰¯å¥½çš„æ”¯æŒï¼Œä½ å¯ä»¥æ ¹æ®ä¸ªäººåå¥½å’Œç‰¹å®šä»»åŠ¡çš„éœ€æ±‚æ¥é€‰æ‹©ä½¿ç”¨å“ªä¸€ç§ã€‚** â¤ï¸â¤ï¸â¤ï¸<br>

ä¸€äº›å¼€å‘è€…æ›´å–œæ¬¢CSSé€‰æ‹©å™¨å› ä¸ºå®ƒä»¬æ›´ç®€æ´ï¼Œè€Œå¦ä¸€äº›å¯èƒ½ä¼šé€‰æ‹©XPathå› ä¸ºå®ƒçš„åŠŸèƒ½æ›´å¼ºå¤§ã€‚åœ¨å®é™…ä½¿ç”¨ä¸­ï¼Œä½ ç”šè‡³å¯ä»¥åœ¨åŒä¸€ä¸ªé¡¹ç›®ä¸­åŒæ—¶ä½¿ç”¨CSSé€‰æ‹©å™¨å’ŒXPathã€‚<br>


## åŠ¨æ€ç½‘é¡µçˆ¬å–:

### é—®é¢˜æè¿°:

æˆ‘æƒ³è¦çˆ¬å–çš„ç•Œé¢éšç€ä¸‹æ‹‰è¿›åº¦æ¡æ‰ä¼šåŠ è½½å‰©ä¸‹çš„éƒ¨åˆ†ï¼Œè¿™ç§æƒ…å†µä¸‹å¯ä»¥ä½¿ç”¨scrapyè¿›è¡Œçˆ¬å–å—ï¼Ÿ<br>

### è§£å†³æ–¹æ¡ˆ:

åœ¨å¤„ç†éœ€è¦æ»šåŠ¨é¡µé¢æ‰èƒ½åŠ è½½å†…å®¹çš„ç½‘ç«™æ—¶ï¼ŒScrapy æœ¬èº«å¹¶ä¸æ”¯æŒ JavaScript æ¸²æŸ“æˆ–ä¸é¡µé¢äº¤äº’ï¼Œå¦‚æ»šåŠ¨æ¡æ“ä½œã€‚è¿™æ˜¯å› ä¸º Scrapy æ˜¯ä¸€ä¸ªå¿«é€Ÿé«˜çº§çš„çˆ¬è™«æ¡†æ¶ï¼Œç”¨äºæŠ“å–ç½‘ç«™å¹¶ä»é¡µé¢ä¸­æå–æ•°æ®ï¼Œä½†å®ƒä¸æ”¯æŒ JavaScriptã€‚<br>

ä½†æ˜¯ï¼Œä½ å¯ä»¥ä½¿ç”¨å¦‚ä¸‹æ–¹æ³•æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼š<br>

1. **Selenium**: ä½¿ç”¨ Seleniumï¼Œä¸€ä¸ªç”¨äºè‡ªåŠ¨åŒ–ç½‘é¡µæµè§ˆå™¨æ“ä½œçš„å·¥å…·ï¼Œä½ å¯ä»¥æ¨¡æ‹ŸçœŸå®ç”¨æˆ·çš„è¡Œä¸ºï¼ŒåŒ…æ‹¬æ»šåŠ¨é¡µé¢ã€‚ç„¶åï¼Œä½ å¯ä»¥å°†é¡µé¢çš„æºä»£ç ä¼ é€’ç»™ Scrapy ä»¥æå–æ‰€éœ€æ•°æ®ã€‚

2. **Splash**: Splash æ˜¯ä¸€ä¸ªä¸ Scrapy é…åˆè‰¯å¥½çš„ JavaScript æ¸²æŸ“æœåŠ¡ï¼Œå®ƒå…è®¸æ‰§è¡Œ JavaScript è„šæœ¬ï¼Œä»è€Œå¯ä»¥æ¨¡æ‹Ÿé¡µé¢æ»šåŠ¨ç­‰æ“ä½œã€‚

3. **åˆ†æAJAXè¯·æ±‚**: æœ‰äº›ç½‘ç«™åœ¨æ»šåŠ¨åˆ°é¡µé¢åº•éƒ¨æ—¶ä¼šå‘é€ä¸€ä¸ªAJAXè¯·æ±‚æ¥åŠ è½½æ›´å¤šæ•°æ®ã€‚ä½ å¯ä»¥é€šè¿‡æ£€æŸ¥ç½‘ç»œè¯·æ±‚æ¥æ‰¾å‡ºè¿™ä¸ªè¯·æ±‚ï¼Œå¹¶ç›´æ¥ç”¨ Scrapy å‘é€è¿™äº›è¯·æ±‚æ¥è·å–æ•°æ®ã€‚

æ¯ç§æ–¹æ³•éƒ½æœ‰å…¶ä¼˜ç¼ºç‚¹ï¼Œé€‰æ‹©å“ªä¸€ç§å–å†³äºå…·ä½“çš„ç½‘ç«™ç»“æ„å’Œä½ çš„å…·ä½“éœ€æ±‚ã€‚é€šå¸¸ï¼Œåˆ†æ AJAX è¯·æ±‚æ˜¯æœ€å¿«ä¸”æ•ˆç‡æœ€é«˜çš„æ–¹æ³•ï¼Œä½†å®ƒéœ€è¦ä½ èƒ½å¤Ÿç†è§£å’Œæ¨¡æ‹Ÿæµè§ˆå™¨å‘å‡ºçš„ç½‘ç»œè¯·æ±‚ã€‚Selenium å’Œ Splash åˆ™æä¾›äº†ä¸€ç§æ›´ç›´è§‚ã€æ›´çµæ´»çš„æ–¹å¼æ¥å¤„ç† JavaScriptï¼Œä½†å¯èƒ½ä¼šæ…¢ä¸€äº›ï¼Œå¹¶ä¸”éœ€è¦æ›´å¤šçš„èµ„æºã€‚<br>

## çˆ¬å–é¼ æ ‡æ‚¬åœæ‰èƒ½æ˜¾ç¤ºçš„å†…å®¹:

### é—®é¢˜æè¿°:

æˆ‘è¦çˆ¬å–çš„ç½‘é¡µå†…å®¹æ˜¯é¼ æ ‡æ‚¬åœåœ¨é‚£ä¸ªä½ç½®æ‰ä¼šæ˜¾ç¤ºå…¶ä¸­çš„å†…å®¹ï¼Œä¸æ˜¯ç‚¹å‡»ï¼Œç‚¹å‡»å°±è·³è½¬ç•Œé¢äº†ï¼Œè¿™ç§æƒ…å†µä¸‹ä¹Ÿèƒ½ä½¿ç”¨scrapyè¿›è¡Œçˆ¬å–å—ï¼Ÿ<br>

### è§£å†³æ–¹æ¡ˆ:

å½“å†…å®¹åªåœ¨é¼ æ ‡æ‚¬åœæ—¶æ˜¾ç¤ºï¼Œå¹¶ä¸”ç‚¹å‡»ä¼šå¯¼è‡´é¡µé¢è·³è½¬ï¼Œä½¿ç”¨Scrapyå•ç‹¬è¿›è¡Œçˆ¬å–å¯èƒ½ä¼šæœ‰ä¸€äº›é™åˆ¶ã€‚Scrapyæ˜¯ä¸€ä¸ªéå¸¸å¼ºå¤§çš„çˆ¬è™«æ¡†æ¶ï¼Œä¸»è¦ç”¨äºæå–HTMLå’ŒXMLä¸­çš„æ•°æ®ã€‚ç„¶è€Œï¼Œå®ƒæœ¬èº«ä¸å¤„ç†JavaScriptç”Ÿæˆçš„å†…å®¹ï¼Œè¿™æ„å‘³ç€å¦‚æœç½‘é¡µå†…å®¹æ˜¯åœ¨é¼ æ ‡æ‚¬åœäº‹ä»¶ä¹‹åç”±JavaScriptåŠ¨æ€ç”Ÿæˆçš„ï¼ŒScrapyå¯èƒ½æ— æ³•ç›´æ¥æ•è·è¿™äº›å†…å®¹ã€‚<br>

ä½†æ˜¯ï¼Œæœ‰å‡ ç§æ–¹æ³•å¯ä»¥å¤„ç†è¿™ç§æƒ…å†µï¼š<br>

1. **åˆ†æAJAXè¯·æ±‚**ï¼šæœ‰æ—¶ï¼Œæ‚¬åœäº‹ä»¶ä¼šè§¦å‘ä¸€ä¸ªAJAXè¯·æ±‚ï¼Œè¯¥è¯·æ±‚ä»æœåŠ¡å™¨è·å–è¦æ˜¾ç¤ºçš„æ•°æ®ã€‚ä½ å¯ä»¥ä½¿ç”¨æµè§ˆå™¨çš„å¼€å‘è€…å·¥å…·æ¥ç›‘è§†ç½‘ç»œè¯·æ±‚ï¼Œå¹¶æ‰¾å‡ºå¯¹åº”çš„AJAXè¯·æ±‚ã€‚ä¸€æ—¦æ‰¾åˆ°ï¼Œä½ å¯ä»¥ç›´æ¥ç”¨Scrapyæ¨¡æ‹Ÿè¿™äº›è¯·æ±‚æ¥è·å–æ•°æ®ã€‚

2. **é›†æˆSelenium**ï¼šSeleniumæ˜¯ä¸€ä¸ªè‡ªåŠ¨åŒ–å·¥å…·ï¼Œå¯ä»¥æ¨¡æ‹ŸçœŸå®ç”¨æˆ·çš„è¡Œä¸ºï¼ŒåŒ…æ‹¬é¼ æ ‡æ‚¬åœã€‚ä½ å¯ä»¥å°†Seleniumä¸Scrapyç»“åˆä½¿ç”¨ï¼ŒSeleniumç”¨æ¥å¤„ç†ç”¨æˆ·äº¤äº’å’ŒJavaScriptæ¸²æŸ“ï¼Œè€ŒScrapyç”¨æ¥å¤„ç†æ•°æ®æå–ã€‚

3. **ä½¿ç”¨Splash**ï¼šSplashæ˜¯ä¸€ä¸ªä¸Scrapyå…¼å®¹çš„JavaScriptæ¸²æŸ“æœåŠ¡ã€‚å®ƒå¯ä»¥å¤„ç†JavaScriptï¼Œå¹¶æä¾›äº†ä¸€ä¸ªHTTP APIæ¥æ§åˆ¶æµè§ˆå™¨çš„è¡Œä¸ºï¼Œæ¯”å¦‚æ¨¡æ‹Ÿé¼ æ ‡æ‚¬åœã€‚ä½ å¯ä»¥å°†Splashä½œä¸ºä¸€ä¸ªä¸­é—´ä»¶é›†æˆåˆ°ä½ çš„Scrapyé¡¹ç›®ä¸­ã€‚

æ¯ç§æ–¹æ³•éƒ½æœ‰å…¶ä¼˜ç¼ºç‚¹ï¼Œä½ å¯èƒ½éœ€è¦æ ¹æ®å…·ä½“çš„ç½‘ç«™ç»“æ„å’Œéœ€æ±‚æ¥é€‰æ‹©æœ€åˆé€‚çš„æ–¹æ³•ã€‚åœ¨å¤„ç†å¤æ‚çš„JavaScriptäº¤äº’æ—¶ï¼Œé€šå¸¸éœ€è¦ä¸€å®šçš„è¯•é”™è¿‡ç¨‹ã€‚<br>

## scrapyå®æ“:

> ç¬”è€…ä½¿ç”¨çš„æ˜¯macï¼Œå¿«æ·é”®æä¾›çš„æ˜¯macå¯æ‰§è¡Œæ–¹å¼ã€‚

åœ¨ä½¿ç”¨æµè§ˆå™¨çš„å¼€å‘è€…å·¥å…·æŸ¥çœ‹ç½‘é¡µå…ƒç´ æ—¶ï¼Œç¡®å®šé¼ æ ‡æ‚¬åœä½ç½®å¯¹åº”çš„å…ƒç´ å¯ä»¥é€šè¿‡ä»¥ä¸‹æ­¥éª¤å®Œæˆï¼š

1. **æ‰“å¼€å¼€å‘è€…å·¥å…·**ï¼š
   - åœ¨ Mac ä¸Šï¼Œå¯¹äºå¤§å¤šæ•°æµè§ˆå™¨ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨å¿«æ·é”® `Command + Option + I` æ‰“å¼€å¼€å‘è€…å·¥å…·ã€‚
   - æˆ–è€…ï¼Œæ‚¨ä¹Ÿå¯ä»¥å³é”®ç‚¹å‡»ç½‘é¡µä¸Šçš„ä»»ä½•å…ƒç´ ï¼Œç„¶åé€‰æ‹©â€œæ£€æŸ¥â€æˆ–â€œå®¡æŸ¥å…ƒç´ â€æ¥æ‰“å¼€å¼€å‘è€…å·¥å…·ã€‚

2. **ä½¿ç”¨å…ƒç´ æ£€æŸ¥å™¨**ï¼š
   - åœ¨å¼€å‘è€…å·¥å…·ä¸­ï¼Œé€šå¸¸ä¼šæœ‰ä¸€ä¸ªç±»ä¼¼æ”¾å¤§é•œçš„å›¾æ ‡ï¼Œè¿™æ˜¯â€œå…ƒç´ æ£€æŸ¥å™¨â€å·¥å…·ã€‚ç‚¹å‡»è¿™ä¸ªå·¥å…·ï¼Œç„¶åå°†é¼ æ ‡ç§»åŠ¨åˆ°ç½‘é¡µä¸Šã€‚
   - å½“æ‚¨å°†é¼ æ ‡æ‚¬åœåœ¨ç½‘é¡µçš„ä¸åŒéƒ¨åˆ†æ—¶ï¼Œç›¸åº”çš„ HTML å…ƒç´ å°†åœ¨å¼€å‘è€…å·¥å…·çš„ Elementsï¼ˆå…ƒç´ ï¼‰é¢æ¿ä¸­é«˜äº®æ˜¾ç¤ºã€‚

3. **æŸ¥çœ‹å’Œé€‰æ‹©å…ƒç´ **ï¼š
   - å½“æ‚¨æ‚¬åœåœ¨ç½‘é¡µä¸Šçš„æŸä¸ªéƒ¨åˆ†æ—¶ï¼Œè¯¥éƒ¨åˆ†å¯¹åº”çš„ HTML ä»£ç ä¼šåœ¨ Elements é¢æ¿ä¸­é«˜äº®æ˜¾ç¤ºã€‚
   - è¿™æ ·ï¼Œæ‚¨å°±å¯ä»¥çœ‹åˆ°å½“å‰é¼ æ ‡æ‚¬åœä½ç½®å¯¹åº”çš„ HTML æ ‡ç­¾ã€ç±»åã€ID æˆ–å…¶ä»–å±æ€§ã€‚

4. **åˆ†æå…ƒç´ ç»“æ„**ï¼š
   - åˆ†æé«˜äº®æ˜¾ç¤ºçš„ HTML ä»£ç ï¼Œç¡®å®šæ˜¯å“ªä¸ªæ ‡ç­¾æˆ–å±æ€§æœ€é€‚åˆæ‚¨çš„çˆ¬è™«ä»»åŠ¡ã€‚æ¯”å¦‚ï¼Œç±»ç›®å¯èƒ½åŒ…å«åœ¨ä¸€ä¸ªåˆ—è¡¨ `<ul>` é‡Œï¼Œæ¯ä¸ªç±»ç›®æ˜¯ä¸€ä¸ªåˆ—è¡¨é¡¹ `<li>`ã€‚

5. **ç¼–å†™é€‰æ‹©å™¨**ï¼š
   - ä¸€æ—¦ç¡®å®šäº†æ‚¨è¦æŠ“å–çš„ HTML å…ƒç´ ï¼Œå°±å¯ä»¥åœ¨æ‚¨çš„ Scrapy çˆ¬è™«ä»£ç ä¸­ä½¿ç”¨ç›¸åº”çš„é€‰æ‹©å™¨ï¼ˆXPath æˆ– CSSï¼‰æ¥å®šä½è¿™äº›å…ƒç´ ã€‚

è¯·è®°ä½ï¼Œç½‘ç«™çš„ç»“æ„å¯èƒ½å› è®¾è®¡è€Œå¼‚ï¼Œå› æ­¤æ²¡æœ‰ä¸€ç§ç»Ÿä¸€çš„æ–¹æ³•æ¥é€‰æ‹©å…ƒç´ ã€‚é€šå¸¸ï¼Œæ‚¨éœ€è¦æ ¹æ®å…·ä½“çš„ç½‘é¡µç»“æ„æ¥ç¡®å®šæœ€ä½³çš„é€‰æ‹©å™¨ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™å¯èƒ½éœ€è¦ä¸€äº›è¯•é”™å’Œè°ƒæ•´ã€‚

![å®šä½html](./scrapy_picture/å®šä½.jpg)


## HTMLç®€ä»‹:

ç†è§£HTMLçš„åŸºæœ¬æ„æˆï¼šå…ƒç´ ã€å±æ€§å’Œæ ‡ç­¾ï¼Œæ˜¯å­¦ä¹ ç½‘é¡µå¼€å‘çš„å…³é”®ã€‚è¿™ä¸‰ä¸ªæ¦‚å¿µæ„æˆäº†HTMLæ–‡æ¡£çš„åŸºç¡€ï¼Œä¸‹é¢æˆ‘ç¬”è€…é€ä¸€è§£é‡Šå®ƒä»¬ã€‚<br>

### HTMLæ ‡ç­¾

HTMLæ ‡ç­¾æ˜¯æ„æˆHTMLé¡µé¢çš„åŸºæœ¬å•ä½ã€‚å®ƒä»¬ç”±å°–æ‹¬å·`<>`åŒ…è£¹çš„å…³é”®è¯ç»„æˆï¼Œç”¨äºæ ‡è®°æ•°æ®ï¼Œå‘Šè¯‰æµè§ˆå™¨ä»¥ä½•ç§æ–¹å¼æ˜¾ç¤ºå†…å®¹æˆ–è€…å¦‚ä½•è¡¨ç°ç½‘é¡µçš„ç»“æ„ã€‚HTMLæ ‡ç­¾å¤§å¤šæ•°æƒ…å†µä¸‹æ˜¯æˆå¯¹å‡ºç°çš„ï¼ŒåŒ…æ‹¬ä¸€ä¸ªå¼€å§‹æ ‡ç­¾å’Œä¸€ä¸ªç»“æŸæ ‡ç­¾ï¼Œç»“æŸæ ‡ç­¾çš„åç§°å‰ä¼šæœ‰ä¸€ä¸ªæ–œæ `/`ã€‚<br>

ä¾‹å¦‚ï¼Œä¸€ä¸ªæ®µè½æ˜¯è¿™æ ·æ ‡è®°çš„ï¼š<br>

```html
<p>è¿™æ˜¯ä¸€ä¸ªæ®µè½ã€‚</p>
```

åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œ`<p>`æ˜¯å¼€å§‹æ ‡ç­¾ï¼Œ`</p>`æ˜¯ç»“æŸæ ‡ç­¾ï¼Œå®ƒä»¬ä¸€èµ·å®šä¹‰äº†ä¸€ä¸ªæ®µè½å…ƒç´ ğŸ³ã€‚<br>

### HTMLå…ƒç´ 

HTMLå…ƒç´ ä»å¼€å§‹æ ‡ç­¾èµ·ï¼Œåˆ°ç»“æŸæ ‡ç­¾æ­¢ï¼Œæ‰€æœ‰çš„å†…å®¹éƒ½æ˜¯å…ƒç´ çš„ä¸€éƒ¨åˆ†ã€‚å…ƒç´ å¯ä»¥åŒ…å«å…¶ä»–æ ‡ç­¾å’Œæ–‡æœ¬ï¼Œä¹Ÿå¯ä»¥åªåŒ…å«æ–‡æœ¬ã€‚å…ƒç´ å®šä¹‰äº†ç½‘é¡µçš„ç»“æ„å’Œå†…å®¹ã€‚<br>

åœ¨å‰é¢çš„ä¾‹å­ä¸­ï¼Œ`<p>è¿™æ˜¯ä¸€ä¸ªæ®µè½ã€‚</p>`æ•´ä½“æ„æˆäº†ä¸€ä¸ªæ®µè½å…ƒç´ ï¼ŒåŒ…æ‹¬äº†å¼€å§‹æ ‡ç­¾ã€ç»“æŸæ ‡ç­¾å’Œæ ‡ç­¾ä¹‹é—´çš„æ–‡æœ¬å†…å®¹ã€‚<br>

### HTMLå±æ€§

å±æ€§æä¾›äº†å…³äºHTMLå…ƒç´ çš„é¢å¤–ä¿¡æ¯ï¼Œå®ƒä»¬æ€»æ˜¯å†™åœ¨å¼€å§‹æ ‡ç­¾ä¸­ï¼Œé€šå¸¸ä»¥åç§°/å€¼å¯¹çš„å½¢å¼å‡ºç°ã€‚<br>

ä¾‹å¦‚ï¼Œè¦ä¸ºä¸€ä¸ªå›¾åƒå®šä¹‰æºæ–‡ä»¶å’Œå°ºå¯¸ï¼Œå¯ä»¥è¿™æ ·å†™ï¼š<br>

```html
<img src="image.jpg" width="500" height="600">
```

åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œ`<img>`æ ‡ç­¾å®šä¹‰äº†ä¸€ä¸ªå›¾åƒå…ƒç´ ï¼Œ`src`ã€`width`å’Œ`height`éƒ½æ˜¯è¿™ä¸ªå…ƒç´ çš„å±æ€§ï¼š<br>
- `src`å±æ€§æŒ‡å®šäº†å›¾åƒæ–‡ä»¶çš„è·¯å¾„ï¼›
- `width`å’Œ`height`å±æ€§åˆ†åˆ«å®šä¹‰äº†å›¾åƒçš„å®½åº¦å’Œé«˜åº¦ã€‚

### å…³ç³»å’ŒåŒºåˆ«

- **å…ƒç´ **æ˜¯ä»å¼€å§‹æ ‡ç­¾åˆ°ç»“æŸæ ‡ç­¾çš„å…¨éƒ¨å†…å®¹ï¼ŒåŒ…æ‹¬æ ‡ç­¾ã€å±æ€§å’ŒåŒ…å«çš„æ–‡æœ¬æˆ–å…¶ä»–å…ƒç´ ã€‚
- **æ ‡ç­¾**æ˜¯æ„å»ºHTMLå…ƒç´ çš„ä»£ç æ ‡è®°ï¼Œé€šå¸¸æˆå¯¹å‡ºç°ï¼Œå®šä¹‰äº†å…ƒç´ çš„å¼€å§‹å’Œç»“æŸã€‚
- **å±æ€§**æä¾›äº†å…³äºå…ƒç´ çš„é™„åŠ ä¿¡æ¯ï¼Œå®ƒä»¬å®šä¹‰åœ¨å¼€å§‹æ ‡ç­¾ä¸­ï¼Œä»¥åç§°/å€¼å¯¹çš„å½¢å¼å­˜åœ¨ã€‚

ç†è§£äº†è¿™ä¸‰ä¸ªæ¦‚å¿µä¹‹é—´çš„å…³ç³»å’ŒåŒºåˆ«ï¼Œä½ å°±èƒ½æ›´å¥½åœ°ç†è§£HTMLæ–‡æ¡£çš„ç»“æ„å’Œå·¥ä½œæ–¹å¼äº†ã€‚<br>


## CSSå’ŒXPathè¯­æ³•:

å½“ä½ æƒ³ä½¿ç”¨Scrapyæ¡†æ¶æ¥çˆ¬å–ç½‘é¡µæ•°æ®æ—¶ï¼Œç†è§£å¦‚ä½•ç¼–å†™CSSé€‰æ‹©å™¨å’ŒXPathé€‰æ‹©å™¨ä»£ç æ˜¯éå¸¸å…³é”®çš„ã€‚è¿™ä¸¤ç§é€‰æ‹©å™¨éƒ½ç”¨äºå®šä½å’Œé€‰æ‹©HTMLæ–‡æ¡£ä¸­çš„å…ƒç´ ï¼Œä»¥ä¾¿æå–ä½ éœ€è¦çš„æ•°æ®ã€‚ç¬”è€…å°†ä¼šåˆ†åˆ«è§£é‡Šè¿™ä¸¤ç§é€‰æ‹©å™¨çš„åŸºç¡€çŸ¥è¯†å’Œä½¿ç”¨æ–¹æ³•ã€‚<br>

### CSSé€‰æ‹©å™¨:

CSSé€‰æ‹©å™¨ä¸»è¦ç”¨äºHTMLæ–‡æ¡£çš„æ ·å¼è®¾ç½®ï¼Œä½†åœ¨Scrapyä¸­ï¼Œå®ƒä»¬ä¹Ÿè¢«ç”¨æ¥é€‰æ‹©å’Œæå–æ•°æ®ã€‚åŸºæœ¬è¯­æ³•éå¸¸ç›´æ¥ï¼š<br>

- **é€‰æ‹©å…ƒç´ **ï¼šç›´æ¥ä½¿ç”¨å…ƒç´ æ ‡ç­¾ï¼Œå¦‚`p`é€‰æ‹©æ‰€æœ‰`<p>`å…ƒç´ ã€‚
- **ç±»é€‰æ‹©å™¨**ï¼šä½¿ç”¨`.`åŠ ç±»åï¼Œå¦‚`.my-class`é€‰æ‹©æ‰€æœ‰`class="my-class"`çš„å…ƒç´ ã€‚
- **IDé€‰æ‹©å™¨**ï¼šä½¿ç”¨`#`åŠ IDåï¼Œå¦‚`#my-id`é€‰æ‹©æ‰€æœ‰`id="my-id"`çš„å…ƒç´ ã€‚
- **å±æ€§é€‰æ‹©å™¨**ï¼šä½¿ç”¨`[å±æ€§å="å€¼"]`ï¼Œå¦‚`[href="https://example.com"]`é€‰æ‹©æ‰€æœ‰`href`å±æ€§ä¸º`https://example.com`çš„å…ƒç´ ã€‚

ä¾‹å¦‚ï¼Œè¦é€‰æ‹©æ‰€æœ‰ä½äº`<div>`æ ‡ç­¾å†…çš„`<a>`æ ‡ç­¾ï¼Œä½ å¯ä»¥ä½¿ç”¨CSSé€‰æ‹©å™¨`div a`ã€‚<br>

ç½‘é¡µçš„å¼€å‘è€…å·¥å…·ä¹Ÿå¯ä»¥è¿™æ ·ä½¿ç”¨ï¼ŒCtrl+f åè¾“å…¥ä½ çš„CSSè¯­å¥å³å¯ï¼Œæ•ˆæœå¦‚ä¸‹:<br>

[](./scrapy_picture/css_program.jpg)


### XPathé€‰æ‹©å™¨:

XPathæ˜¯ä¸€ç§åœ¨XMLæ–‡æ¡£ä¸­æŸ¥æ‰¾ä¿¡æ¯çš„è¯­è¨€ï¼Œä¹Ÿå¯ç”¨äºHTMLã€‚å®ƒæä¾›äº†éå¸¸å¼ºå¤§çš„é€‰æ‹©åŠŸèƒ½ï¼ŒåŒ…æ‹¬é€‰æ‹©åµŒå¥—ã€å±æ€§ã€å¤„ç†ä¸åŒçš„è½´ç­‰ã€‚<br>

- **é€‰æ‹©å…ƒç´ **ï¼šä½¿ç”¨`//æ ‡ç­¾å`é€‰æ‹©æ‰€æœ‰åç§°åŒ¹é…çš„å…ƒç´ ï¼Œå¦‚`//div`é€‰æ‹©æ‰€æœ‰`<div>`å…ƒç´ ã€‚
- **å±æ€§é€‰æ‹©**ï¼šä½¿ç”¨`[@å±æ€§å="å€¼"]`ï¼Œå¦‚`//a[@href="https://example.com"]`ã€‚
- **é€‰æ‹©ç‰¹å®šå­å…ƒç´ **ï¼šä½¿ç”¨`/`é€‰æ‹©ç›´æ¥å­å…ƒç´ ï¼Œå¦‚`//div/a`é€‰æ‹©æ‰€æœ‰`<div>`ç›´æ¥å­å…ƒç´ çš„`<a>`æ ‡ç­¾ã€‚
- **é€‰æ‹©æ‰€æœ‰å­å­™å…ƒç´ **ï¼šä½¿ç”¨`//`ï¼Œå¦‚`//div//a`é€‰æ‹©æ‰€æœ‰ä½äº`<div>`å†…éƒ¨çš„`<a>`æ ‡ç­¾ï¼Œä¸è®ºæ·±åº¦ã€‚
- **ä½¿ç”¨ç´¢å¼•é€‰æ‹©**ï¼šXPathç´¢å¼•ä»1å¼€å§‹ï¼Œå¦‚`//div[1]`é€‰æ‹©ç¬¬ä¸€ä¸ª`<div>`å…ƒç´ ã€‚

### Scrapyä¸­çš„ä½¿ç”¨:

åœ¨Scrapyé¡¹ç›®ä¸­ï¼Œä½ å¯ä»¥åœ¨spideræ–‡ä»¶ä¸­ä½¿ç”¨è¿™äº›é€‰æ‹©å™¨æ¥æå–æ•°æ®ã€‚<br>

- **ä½¿ç”¨CSSé€‰æ‹©å™¨**ï¼š`response.css('selector')`ã€‚
- **ä½¿ç”¨XPathé€‰æ‹©å™¨**ï¼š`response.xpath('selector')`ã€‚

ä¾‹å¦‚ï¼Œä½ æƒ³æå–æŸä¸ªé¡µé¢ä¸Šæ‰€æœ‰çš„å¤´æ¡æ–°é—»é“¾æ¥(`xx.html`)ï¼Œå…¶HTMLç»“æ„å¯èƒ½å¦‚ä¸‹ï¼š<br>

```html
<div class="news">
  <a href="news1.html">æ–°é—»1</a>
  <a href="news2.html">æ–°é—»2</a>
</div>
```

ä½¿ç”¨CSSé€‰æ‹©å™¨ï¼š<br>

```python
for news in response.css('div.news a'):
    link = news.css('::attr(href)').get()
    print(link)
```

`::attr(href)` æ˜¯ä¸€ç§åœ¨CSSé€‰æ‹©å™¨ä¸­ç”¨äºæå–å…ƒç´ å±æ€§å€¼çš„è¯­æ³•ï¼Œå®ƒå¹¶ä¸ç”¨äºé€‰æ‹©å…ƒç´ ï¼Œè€Œæ˜¯ç”¨äºè·å–å·²é€‰æ‹©å…ƒç´ çš„æŸä¸ªå±æ€§çš„å€¼ã€‚<br>

è¿™ç§è¯­æ³•æ˜¯Scrapyæ¡†æ¶ç‰¹æœ‰çš„ï¼Œä¸æ˜¯æ ‡å‡†çš„CSSé€‰æ‹©å™¨è¯­æ³•ã€‚â€¼ï¸â€¼ï¸â€¼ï¸<br>

åœ¨æ ‡å‡†çš„CSSä¸­ï¼Œç¡®å®æ²¡æœ‰è¿™æ ·çš„å†™æ³•æ¥ç›´æ¥è·å–å±æ€§å€¼ã€‚`[å±æ€§å="å€¼"]` è¿™æ ·çš„å†™æ³•æ˜¯ç”¨äºé€‰æ‹©å…·æœ‰ç‰¹å®šå±æ€§å€¼çš„å…ƒç´ ï¼Œè€Œ `::attr(å±æ€§å)` æ˜¯åœ¨Scrapyç­‰ç‰¹å®šç¯å¢ƒä¸­ï¼Œç”¨äºä»å·²é€‰ä¸­çš„å…ƒç´ ä¸­æå–å‡ºè¯¥å±æ€§çš„å€¼ã€‚<br>


ä½¿ç”¨XPathé€‰æ‹©å™¨ï¼š<br>

```python
for news in response.xpath('//div[@class="news"]/a'):
    link = news.xpath('./@href').get()
    print(link)
```

`//div[@class="news"]/a` è¡¨è¾¾å¼è¯¦è§£ï¼š<br>
- `//`ï¼šåœ¨æ–‡æ¡£çš„ä»»ä½•ä½ç½®å¼€å§‹æŸ¥æ‰¾ã€‚
- `div[@class="news"]`ï¼šé€‰æ‹©æ‰€æœ‰classå±æ€§ä¸ºnewsçš„divå…ƒç´ ã€‚
- `/a`ï¼šç„¶åï¼Œä»æ‰¾åˆ°çš„divå…ƒç´ ä¸­ï¼Œé€‰æ‹©æ‰€æœ‰ç›´æ¥å­å…ƒç´ aã€‚

ä»¥ä¸Šå°±æ˜¯ä½¿ç”¨CSSé€‰æ‹©å™¨å’ŒXPathé€‰æ‹©å™¨çš„åŸºæœ¬æ–¹æ³•ã€‚å®è·µä¸­ï¼Œä½ å¯èƒ½éœ€è¦æ ¹æ®å…·ä½“çš„HTMLç»“æ„å’Œéœ€æ±‚ï¼Œç»„åˆä½¿ç”¨ä¸åŒçš„é€‰æ‹©å™¨æ¥ç²¾ç¡®åœ°å®šä½å’Œæå–æ•°æ®ã€‚é€šè¿‡å¤šå°è¯•å’Œå®è·µï¼Œä½ å°†èƒ½å¤Ÿæ›´ç†Ÿç»ƒåœ°è¿ç”¨è¿™äº›é€‰æ‹©å™¨ã€‚<br>

## CSS å’Œ XPath é€‰æ‹©å™¨çš„è·¯å¾„é—®é¢˜:

### é—®é¢˜æè¿°:

æœ‰äººé—®ï¼ŒCSS å’Œ XPath é€‰æ‹©å™¨æ£€ç´¢çš„æ—¶å€™ï¼Œä¸éœ€è¦å®Œæ•´çš„è·¯å¾„å—ï¼Ÿç›´æ¥ `'div.news a'` æˆ– `'//div[@class="news"]/a'` å°±èƒ½æ‰¾åˆ°è‡ªå·±éœ€è¦çš„å†…å®¹å—ï¼Ÿ<br>

### é—®é¢˜è§£ç­”:

åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œä½ ä¸éœ€è¦æä¾›å®Œæ•´çš„è·¯å¾„æ¥ä½¿ç”¨CSSé€‰æ‹©å™¨æˆ–XPathé€‰æ‹©å™¨å®šä½å…ƒç´ ã€‚<br>

ç›´æ¥ä½¿ç”¨å¦‚ `'div.news a'` æˆ– `'//div[@class="news"]/a'` è¿™æ ·çš„ç®€åŒ–è·¯å¾„å·²ç»è¶³å¤Ÿå¼ºå¤§ï¼Œèƒ½å¤Ÿå®šä½åˆ°ä½ éœ€è¦çš„å†…å®¹ã€‚è¿™ç§æ–¹æ³•çš„ä¼˜ç‚¹æ˜¯ç®€æ´ä¸”æ˜“äºç†è§£ï¼Œç‰¹åˆ«æ˜¯å½“ä½ éœ€è¦ä»å…·æœ‰ç‰¹å®šç±»åæˆ–å±æ€§çš„å…ƒç´ ä¸­æå–ä¿¡æ¯æ—¶ã€‚<br>

### ä¸ºä»€ä¹ˆä¸éœ€è¦å®Œæ•´è·¯å¾„ï¼Ÿ

1. **çµæ´»æ€§**ï¼šé¿å…äº†å› é¡µé¢ç»“æ„è½»å¾®å˜åŠ¨ï¼ˆå¦‚å¢åŠ ä¸€ä¸ªåŒ…è£¹å…ƒç´ ï¼‰è€Œå¯¼è‡´çš„é€‰æ‹©å™¨å¤±æ•ˆé—®é¢˜ã€‚
2. **ç®€æ´æ€§**ï¼šå‡å°‘äº†éœ€è¦ä¹¦å†™å’Œç»´æŠ¤çš„ä»£ç é‡ï¼Œä½¿å¾—é€‰æ‹©å™¨è¡¨è¾¾å¼æ›´åŠ æ¸…æ™°ã€‚
3. **æ€§èƒ½**ï¼šåœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œ**ç®€åŒ–çš„è·¯å¾„é€‰æ‹©è¡¨è¾¾å¼èƒ½å¤Ÿé«˜æ•ˆåœ°è¢«æµè§ˆå™¨æˆ–è§£æå™¨å¤„ç†ã€‚ğŸ˜„ğŸ˜„ğŸ˜„**

### ä»€ä¹ˆæ—¶å€™è€ƒè™‘ä½¿ç”¨æ›´å…·ä½“çš„è·¯å¾„ï¼Ÿ

è™½ç„¶ç®€åŒ–çš„è·¯å¾„é€‰æ‹©è¡¨è¾¾å¼åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹éƒ½æ˜¯æœ‰æ•ˆçš„ï¼Œä½†åœ¨ä¸€äº›ç‰¹å®šæƒ…å†µä¸‹ï¼Œä½¿ç”¨æ›´å…·ä½“çš„è·¯å¾„å¯èƒ½ä¼šæ›´å¥½ï¼š<br>

- **é¡µé¢ç»“æ„å¤æ‚**ï¼šå½“é¡µé¢ç»“æ„éå¸¸å¤æ‚ï¼Œä¸”å­˜åœ¨å¤šä¸ªç›¸ä¼¼çš„å…ƒç´ æ—¶ï¼Œä½¿ç”¨æ›´å…·ä½“çš„è·¯å¾„å¯ä»¥å¸®åŠ©ç²¾ç¡®å®šä½ã€‚
- **é¿å…æ­§ä¹‰**ï¼šå¦‚æœé¡µé¢ä¸­æœ‰å¤šä¸ªç›¸åŒçš„ç±»æˆ–å±æ€§å€¼ï¼Œä½†ä½ åªæƒ³é€‰æ‹©å…¶ä¸­ä¸€éƒ¨åˆ†å…ƒç´ ï¼Œè¿™æ—¶å€™å…·ä½“è·¯å¾„å°±æ˜¾å¾—å¾ˆæœ‰ç”¨ã€‚
- **æ€§èƒ½è€ƒè™‘**ï¼šåœ¨æŸäº›æç«¯æƒ…å†µä¸‹ï¼Œå¦‚æœé¡µé¢éå¸¸å¤§ï¼Œä½¿ç”¨è¿‡äºå®½æ³›çš„é€‰æ‹©å™¨å¯èƒ½ä¼šå½±å“æ€§èƒ½ã€‚æ›´ç²¾ç¡®çš„è·¯å¾„å¯ä»¥å‡å°‘éœ€è¦éå†çš„èŠ‚ç‚¹æ•°é‡ã€‚

### ç¤ºä¾‹

å‡è®¾ä½ è¦ä»ä¸€ä¸ªåŒ…å«å¤šä¸ªç›¸åŒç±»å`news`çš„åŒºåŸŸä¸­æå–æ•°æ®ï¼Œä½†åªå¯¹ç¬¬ä¸€ä¸ª`div`ä¸­çš„é“¾æ¥æ„Ÿå…´è¶£ï¼š<br>

- **ä½¿ç”¨æ›´å…·ä½“çš„CSSé€‰æ‹©å™¨**ï¼š`'div.news:first-child a'`ï¼ˆé€‰æ‹©ç¬¬ä¸€ä¸ªå…·æœ‰`news`ç±»çš„`div`ä¸­çš„æ‰€æœ‰`a`æ ‡ç­¾ï¼‰
- **ä½¿ç”¨æ›´å…·ä½“çš„XPathé€‰æ‹©å™¨**ï¼š`'(//div[@class="news"])[1]/a'`ï¼ˆé€‰æ‹©ç¬¬ä¸€ä¸ªå…·æœ‰`news`ç±»çš„`div`ä¸­çš„æ‰€æœ‰`a`æ ‡ç­¾ï¼‰

### ç»“è®º

ç›´æ¥ä½¿ç”¨å¦‚`'div.news a'`æˆ–`'//div[@class="news"]/a'`è¿™æ ·çš„ç®€åŒ–è·¯å¾„æ˜¯å¸¸è§ä¸”å®ç”¨çš„åšæ³•ï¼Œå°¤å…¶æ˜¯åœ¨ç›®æ ‡æ•°æ®ç›¸å¯¹å®¹æ˜“å®šä½çš„æƒ…å†µä¸‹ã€‚<br>

ä½†æ˜¯ï¼Œåœ¨é¢å¯¹å¤æ‚æˆ–ç‰¹å®šéœ€æ±‚çš„é¡µé¢æ—¶ï¼Œé‡‡ç”¨æ›´å…·ä½“çš„è·¯å¾„å¯èƒ½ä¼šæ›´åŠ æœ‰æ•ˆã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œæœ€ä½³åšæ³•æ˜¯æ ¹æ®é¡µé¢çš„å…·ä½“æƒ…å†µå’Œéœ€æ±‚çµæ´»é€‰æ‹©ä½¿ç”¨ç®€åŒ–è·¯å¾„æˆ–å…·ä½“è·¯å¾„ã€‚<br>


## `allowed_domains` å’Œ `response.follow` ç”¨æ³•è§£æ:

Scrapyæ˜¯ä¸€ä¸ªå¿«é€Ÿçš„é«˜çº§webçˆ¬è™«æ¡†æ¶ï¼Œç”¨äºçˆ¬å–ç½‘ç«™å¹¶ä»é¡µé¢ä¸­æå–ç»“æ„æ€§æ•°æ®ã€‚`allowed_domains` å’Œ `response.follow` æ˜¯Scrapyçˆ¬è™«ä¸­å¸¸ç”¨çš„ä¸¤ä¸ªåŠŸèƒ½ï¼Œä¸‹é¢æˆ‘å°†åˆ†åˆ«è§£é‡Šå®ƒä»¬çš„ç”¨é€”ï¼Œå¹¶ç»™å‡ºç¤ºä¾‹ä»£ç ã€‚<br>

### `allowed_domains`

`allowed_domains`æ˜¯ä¸€ä¸ªçˆ¬è™«å±æ€§ï¼Œå®ƒæ˜¯ä¸€ä¸ªåŸŸååˆ—è¡¨ã€‚å½“Scrapyå°è¯•è·Ÿéšåˆ°ä¸€ä¸ªURLæ—¶ï¼Œå¦‚æœURLçš„åŸŸåä¸åœ¨è¿™ä¸ªåˆ—è¡¨ä¸­ï¼Œåˆ™è¯¥URLä¸ä¼šè¢«çˆ¬å–ã€‚è¿™æ˜¯ä¸€ä¸ªå¯é€‰å±æ€§ï¼Œä¸»è¦ç”¨äºé™åˆ¶çˆ¬è™«çˆ¬å–çš„èŒƒå›´ï¼Œé˜²æ­¢çˆ¬è™«è·‘ååˆ°å…¶ä»–ç½‘ç«™ä¸Šå»ã€‚<br>

#### ç¤ºä¾‹ä»£ç 

```python
import scrapy

class MySpider(scrapy.Spider):
    name = "my_spider"
    allowed_domains = ["example.com"]
    start_urls = [
        "http://www.example.com/"
    ]

    def parse(self, response):
        # æå–æ•°æ®çš„ä»£ç 
        pass
```

åœ¨è¿™ä¸ªç¤ºä¾‹ä¸­ï¼Œ`allowed_domains`è¢«è®¾ç½®ä¸º`["example.com"]`ï¼Œè¿™æ„å‘³ç€Scrapyçˆ¬è™«åªä¼šçˆ¬å–`example.com`åŸŸåä¸‹çš„é¡µé¢ã€‚<br>

#### ç–‘æƒ‘ç‚¹è§£ç­”:

å¯èƒ½æœ‰äººé—®ï¼Œ`allowed_domains = ["example.com"]` æ˜¯å¦åªéœ€è¦å†™åˆ°çˆ¬è™«çš„ç±»å±æ€§ä¸­ï¼Ÿçˆ¬è™«ä¸­çš„å‡½æ•°ä¸­ä¸éœ€è¦å‡ºç°ï¼Ÿä¼šè‡ªåŠ¨æ£€æµ‹ï¼Ÿ<br>

æ˜¯çš„ï¼Œ`allowed_domains`åªéœ€è¦åœ¨çˆ¬è™«ç±»çš„å±æ€§ä¸­å®šä¹‰ä¸€æ¬¡ï¼ŒScrapyæ¡†æ¶ä¼šè‡ªåŠ¨å¤„ç†è¿™ä¸ªå±æ€§ï¼Œå¹¶æ ¹æ®å®ƒæ¥è¿‡æ»¤è¯·æ±‚ã€‚ğŸš€ğŸš€ğŸš€<br>

å½“ä½ åœ¨çˆ¬è™«ä¸­ç”Ÿæˆæ–°çš„è¯·æ±‚æ—¶ï¼Œæ— è®ºæ˜¯é€šè¿‡ `start_urls` å±æ€§è‡ªåŠ¨å‘èµ·çš„è¯·æ±‚ï¼Œè¿˜æ˜¯åœ¨çˆ¬è™«çš„å›è°ƒå‡½æ•°ä¸­ä½¿ç”¨ `response.follow` æˆ– `scrapy.Request` æ‰‹åŠ¨å‘èµ·çš„è¯·æ±‚ï¼ŒScrapyéƒ½ä¼šæ£€æŸ¥è¿™ä¸ªè¯·æ±‚çš„URLæ˜¯å¦å±äº`allowed_domains`åˆ—è¡¨ä¸­çš„åŸŸåã€‚å¦‚æœä¸å±äºï¼Œè¿™ä¸ªè¯·æ±‚å°†ä¸ä¼šè¢«æ‰§è¡Œã€‚â€¼ï¸â€¼ï¸â€¼ï¸<br>

è¿™æ„å‘³ç€åœ¨çˆ¬è™«çš„ä»»ä½•å‡½æ•°ä¸­ï¼Œå½“ä½ åˆ›å»ºæ–°çš„è¯·æ±‚æ—¶ï¼Œä½ ä¸éœ€è¦æ‰‹åŠ¨æ£€æŸ¥URLæ˜¯å¦å±äº`allowed_domains`ä¸­çš„åŸŸåï¼ŒScrapyä¼šè‡ªåŠ¨ä¸ºä½ åšè¿™ä¸ªå·¥ä½œã€‚è¿™æ ·å¯ä»¥è®©ä½ æ›´ä¸“æ³¨äºé¡µé¢çš„è§£æå’Œæ•°æ®çš„æå–ï¼Œè€Œä¸ç”¨æ‹…å¿ƒè¯·æ±‚ä¼šè·‘ååˆ°å…¶ä»–ä¸ç›¸å…³çš„ç½‘ç«™ã€‚<br>

ä¾‹å¦‚ï¼š<br>

```python
class MySpider(scrapy.Spider):
    name = "example_spider"
    allowed_domains = ["example.com"]  # åªéœ€è¦åœ¨è¿™é‡Œå®šä¹‰
    start_urls = ["http://www.example.com/"]

    def parse(self, response):
        # çˆ¬è™«é€»è¾‘å’Œé¡µé¢è§£æ
        yield response.follow("/some/path", self.some_callback)  # ä¸éœ€è¦é¢å¤–æ£€æŸ¥URLçš„åŸŸå

    def some_callback(self, response):
        # è¿›ä¸€æ­¥çš„å¤„ç†é€»è¾‘
        pass
```

åœ¨ä¸Šé¢çš„ä»£ç ä¸­ï¼Œ`allowed_domains` è¢«å®šä¹‰ä¸º `["example.com"]`ï¼Œè¿™æ ·Scrapyå°±ä¼š**è‡ªåŠ¨è¿‡æ»¤**æ‰ä¸å±äº `example.com` åŸŸçš„è¯·æ±‚ã€‚è¿™åŒ…æ‹¬ä» `start_urls` è‡ªåŠ¨å‘èµ·çš„è¯·æ±‚å’Œåœ¨ `parse` æ–¹æ³•åŠå…¶ä»–å›è°ƒæ–¹æ³•ä¸­æ‰‹åŠ¨å‘èµ·çš„è¯·æ±‚ã€‚<br>

### `response.follow`

`response.follow` æ˜¯Scrapyçš„ `Response` å¯¹è±¡çš„ä¸€ä¸ªæ–¹æ³•ï¼Œç”¨äºæ ¹æ®é“¾æ¥ï¼ˆURLæˆ–é“¾æ¥æ–‡æœ¬ï¼‰ç”Ÿæˆä¸€ä¸ªè·Ÿè¿›çš„è¯·æ±‚ï¼ˆ`Request`å¯¹è±¡ï¼‰ã€‚<br>

`response.follow`è‡ªåŠ¨å¤„ç†é“¾æ¥çš„URLç¼–ç å’ŒåŸŸåè¿‡æ»¤ï¼Œæ˜¯ç”Ÿæˆçˆ¬å–ä¸‹ä¸€ä¸ªé¡µé¢è¯·æ±‚çš„ä¾¿æ·æ–¹å¼ã€‚<br>

#### ç¤ºä¾‹ä»£ç 

```python
def parse(self, response):
    for href in response.css('a::attr(href)'):
        yield response.follow(href, self.parse_page)

def parse_page(self, response):
    # æå–å•ä¸ªé¡µé¢çš„æ•°æ®çš„ä»£ç 
    pass
```

åœ¨è¿™ä¸ªç¤ºä¾‹ä¸­ï¼Œ`parse` æ–¹æ³•é¦–å…ˆä»å½“å‰é¡µé¢æå–æ‰€æœ‰çš„é“¾æ¥ï¼Œç„¶åä½¿ç”¨ `response.follow` æ¥ç”Ÿæˆå¯¹è¿™äº›é“¾æ¥çš„è·Ÿè¿›è¯·æ±‚ï¼Œå¹¶æŒ‡å®š `parse_page` ä½œä¸ºå›è°ƒå‡½æ•°æ¥å¤„ç†è¿™äº›è¯·æ±‚å¾—åˆ°çš„å“åº”ã€‚è¿™æ ·ï¼ŒScrapyå°±å¯ä»¥é€’å½’åœ°çˆ¬å–ç½‘ç«™ä¸Šçš„æ‰€æœ‰é“¾æ¥äº†ã€‚<br>

`response.follow` **å¯ä»¥æ¥å—ç»å¯¹æˆ–ç›¸å¯¹URL**ï¼Œå¹¶ä¸”å¦‚æœä½ è®¾ç½®äº† `allowed_domains` ï¼ŒScrapyè¿˜ä¼šè‡ªåŠ¨è¿‡æ»¤æ‰ä¸åœ¨è¿™ä¸ªåŸŸååˆ—è¡¨ä¸­çš„é“¾æ¥ï¼Œä»è€Œç¡®ä¿çˆ¬è™«çš„çˆ¬å–è¡Œä¸ºç¬¦åˆé¢„æœŸã€‚<br>
