# Scrapy & JavaScript integration through Splash
- [Scrapy \& JavaScript integration through Splash](#scrapy--javascript-integration-through-splash)
  - [Installation:](#installation)
  - [Configuration:](#configuration)
  - [Usage:](#usage)
    - [Requests:](#requests)
      - [`meta['splash']['args']` contains arguments sent to Splash. scrapy-splash adds some default keys/values to args:](#metasplashargs-contains-arguments-sent-to-splash-scrapy-splash-adds-some-default-keysvalues-to-args)

é€šè¿‡Splashå®ç°Scrapyä¸JavaScriptçš„é›†æˆã€‚<br>

This library provides [Scrapy](https://github.com/scrapy/scrapy) and JavaScript integration using [Splash](https://github.com/scrapinghub/splash). The license is BSD 3-clause.<br>

è¿™ä¸ªåº“ä½¿ç”¨Splashå®ç°äº†Scrapyå’ŒJavaScriptçš„é›†æˆã€‚è®¸å¯è¯æ˜¯BSD 3-clauseã€‚<br>


## Installation:

Install scrapy-splash using pip:<br>

```bash
pip install scrapy-splash
```

Scrapy-Splash uses Splash HTTP API, so you also need a Splash instance. Usually to install & run Splash, something like this is enough:<br>

Scrapy-Splashä½¿ç”¨Splash HTTP APIï¼Œå› æ­¤æ‚¨è¿˜éœ€è¦ä¸€ä¸ªSplashå®ä¾‹ã€‚é€šå¸¸å®‰è£…å¹¶è¿è¡ŒSplashï¼Œåƒè¿™æ ·å°±è¶³å¤Ÿäº†:<br>

```bash
docker run -p 8050:8050 scrapinghub/splash
```

ç»ˆç«¯å°†æ˜¾ç¤ºå¦‚ä¸‹ä¿¡æ¯:<br>

> ä¸‹è½½å†…å®¹ä¼šè‡ªåŠ¨ Pull ...

```log
(langchain) root@iZ2zea5v77oawjy2qz7c20Z:/data/chat_system# docker run -p 8050:8050 scrapinghub/splash
Unable to find image 'scrapinghub/splash:latest' locally
latest: Pulling from scrapinghub/splash
7595c8c21622: Pulling fs layer 
d13af8ca898f: Pulling fs layer 
70799171ddba: Pulling fs layer 
b6c12202c5ef: Pulling fs layer 
60a588d4f36e: Pulling fs layer 
74efcc44bb0a: Pull complete 
630a03095961: Pull complete 
6ece3b427ef5: Pull complete 
a1fce8353093: Pull complete 
3933ca5f4768: Pull complete 
b0097a197686: Pull complete 
eb38ff0231d8: Pull complete 
3d065f0b97ea: Pull complete 
bce087e2552e: Pull complete 
86c96f1d4ecd: Pull complete 
3c421ffe4e4a: Pull complete 
3c667b2546ea: Pull complete 
765d9dad919d: Pull complete 
f20aa104f0ea: Pull complete 
bc2ee8c11554: Pull complete 
185117d6d955: Pull complete 
a40af7862b2f: Pull complete 
38a6d08ed185: Pull complete 
a36db9d4a71b: Pull complete 
adaa646ebfe9: Pull complete 
6ae21b55ecfd: Pull complete 
8ef8d76a1942: Pull complete 
Digest: sha256:b4173a88a9d11c424a4df4c8a41ce67ff6a6a3205bd093808966c12e0b06dacf
Status: Downloaded newer image for scrapinghub/splash:latest
2024-03-22 02:52:34+0000 [-] Log opened.
2024-03-22 02:52:34.943605 [-] Xvfb is started: ['Xvfb', ':1500143512', '-screen', '0', '1024x768x24', '-nolisten', 'tcp']
QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-splash'
2024-03-22 02:52:35.020444 [-] Splash version: 3.5
2024-03-22 02:52:35.058897 [-] Qt 5.14.1, PyQt 5.14.2, WebKit 602.1, Chromium 77.0.3865.129, sip 4.19.22, Twisted 19.7.0, Lua 5.2
2024-03-22 02:52:35.059113 [-] Python 3.6.9 (default, Jul 17 2020, 12:50:27) [GCC 8.4.0]
2024-03-22 02:52:35.059233 [-] Open files limit: 1048576
2024-03-22 02:52:35.059296 [-] Can't bump open files limit
2024-03-22 02:52:35.075799 [-] proxy profiles support is enabled, proxy profiles path: /etc/splash/proxy-profiles
2024-03-22 02:52:35.075955 [-] memory cache: enabled, private mode: enabled, js cross-domain access: disabled
2024-03-22 02:52:35.202764 [-] verbosity=1, slots=20, argument_cache_max_entries=500, max-timeout=90.0
2024-03-22 02:52:35.203049 [-] Web UI: enabled, Lua: enabled (sandbox: enabled), Webkit: enabled, Chromium: enabled
2024-03-22 02:52:35.203470 [-] Site starting on 8050
2024-03-22 02:52:35.203554 [-] Starting factory <twisted.web.server.Site object at 0x7fbe640f45c0>
2024-03-22 02:52:35.203825 [-] Server listening on http://0.0.0.0:8050
```


Check [Splash](https://splash.readthedocs.io/en/latest/install.html) install docs for more info.<br>

æŸ¥çœ‹Splashå®‰è£…æ–‡æ¡£äº†è§£æ›´å¤šä¿¡æ¯ã€‚<br>


## Configuration:

1. Add the Splash server address to `settings.py` of your Scrapy project like this:

åƒè¿™æ ·åœ¨æ‚¨çš„Scrapyé¡¹ç›®çš„`settings.py`ä¸­æ·»åŠ SplashæœåŠ¡å™¨åœ°å€:<br>

```conf
SPLASH_URL = 'http://192.168.59.103:8050'
```

ä½ çš„å†™æ³•ä¹Ÿå¯ä»¥æ˜¯:<br>

```conf
SPLASH_URL = 'http://localhost:8050'
```

2. Enable the Splash middleware by adding it to `DOWNLOADER_MIDDLEWARES` in your `settings.py` file and changing **HttpCompressionMiddleware** priority:

é€šè¿‡åœ¨æ‚¨çš„ `settings.py` æ–‡ä»¶ä¸­å°†å…¶æ·»åŠ åˆ° `DOWNLOADER_MIDDLEWARES` å¹¶æ›´æ”¹ **HttpCompressionMiddleware** ä¼˜å…ˆçº§æ¥å¯ç”¨Splashä¸­é—´ä»¶:<br>

```conf
DOWNLOADER_MIDDLEWARES = {
    'scrapy_splash.SplashCookiesMiddleware': 723,
    'scrapy_splash.SplashMiddleware': 725,
    'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware': 810,
}
```

Order 723 is just before HttpProxyMiddleware (750) in default scrapy settings.<br>

åœ¨é»˜è®¤çš„scrapyè®¾ç½®ä¸­ï¼Œ723çš„é¡ºåºå°±åœ¨HttpProxyMiddlewareï¼ˆ750ï¼‰ä¹‹å‰ã€‚<br>

HttpCompressionMiddleware priority should be changed in order to allow advanced response processing; see [scrapy/scrapy#1895](https://github.com/scrapy/scrapy/issues/1895) for details.<br>

éœ€è¦æ›´æ”¹HttpCompressionMiddlewareçš„ä¼˜å…ˆçº§ï¼Œä»¥å…è®¸é«˜çº§å“åº”å¤„ç†ï¼›è¯¦è§scrapy/scrapy#1895ã€‚<br>

3. Enable `SplashDeduplicateArgsMiddleware` by adding it to `SPIDER_MIDDLEWARES` in your `settings.py`:

é€šè¿‡åœ¨ `settings.py` ä¸­å°†å…¶æ·»åŠ åˆ° `SPIDER_MIDDLEWARES` æ¥å¯ç”¨ `SplashDeduplicateArgsMiddleware`:<br>

```conf
SPIDER_MIDDLEWARES = {
    'scrapy_splash.SplashDeduplicateArgsMiddleware': 100,
}
```

This middleware is needed to support `cache_args` feature; it allows to save disk space by not storing duplicate Splash arguments multiple times in a disk request queue. <br>

è¿™ä¸ªä¸­é—´ä»¶æ˜¯æ”¯æŒ `cache_args` ç‰¹æ€§æ‰€å¿…éœ€çš„ï¼›å®ƒå…è®¸é€šè¿‡ä¸åœ¨ç£ç›˜è¯·æ±‚é˜Ÿåˆ—ä¸­å¤šæ¬¡å­˜å‚¨é‡å¤çš„Splashå‚æ•°æ¥èŠ‚çœç£ç›˜ç©ºé—´ã€‚<br>

If Splash 2.1+ is used the middleware also allows to save network traffic by not sending these duplicate arguments to Splash server multiple times.<br>

å¦‚æœä½¿ç”¨Splash 2.1+ï¼Œä¸­é—´ä»¶è¿˜å…è®¸é€šè¿‡ä¸å¤šæ¬¡å°†è¿™äº›é‡å¤å‚æ•°å‘é€åˆ°SplashæœåŠ¡å™¨æ¥èŠ‚çœç½‘ç»œæµé‡ã€‚<br>

4. Set a custom `DUPEFILTER_CLASS`:

è®¾ç½®ä¸€ä¸ªè‡ªå®šä¹‰çš„ `DUPEFILTER_CLASS` :<br>

```conf
DUPEFILTER_CLASS = 'scrapy_splash.SplashAwareDupeFilter'
```

5. If you use Scrapy HTTP cache then a custom cache storage backend is required. scrapy-splash provides a subclass of `scrapy.contrib.httpcache.FilesystemCacheStorage`:

å¦‚æœæ‚¨ä½¿ç”¨Scrapy HTTPç¼“å­˜ï¼Œåˆ™éœ€è¦ä¸€ä¸ªè‡ªå®šä¹‰çš„ç¼“å­˜å­˜å‚¨åç«¯ã€‚scrapy-splashæä¾›äº†scrapy.contrib.httpcache.FilesystemCacheStorageçš„ä¸€ä¸ªå­ç±»:<br>

```conf
HTTPCACHE_STORAGE = 'scrapy_splash.SplashAwareFSCacheStorage'
```

If you use other cache storage then it is necesary to subclass it and replace all `scrapy.util.request.request_fingerprint` calls with `scrapy_splash.splash_request_fingerprint`.<br>

å¦‚æœæ‚¨ä½¿ç”¨å…¶ä»–ç¼“å­˜å­˜å‚¨ï¼Œåˆ™éœ€è¦å¯¹å…¶è¿›è¡Œå­ç±»åŒ–ï¼Œå¹¶ç”¨ `scrapy_splash.splash_request_fingerprint` æ›¿æ¢æ‰€æœ‰çš„ `scrapy.util.request.request_fingerprint` è°ƒç”¨ã€‚<br>

â€¼ï¸ **Note** :<br>

Steps (4) and (5) are necessary because Scrapy doesn't provide a way to override request fingerprints calculation algorithm globally; this could change in future.<br>

æ­¥éª¤ï¼ˆ4ï¼‰å’Œï¼ˆ5ï¼‰æ˜¯å¿…éœ€çš„ï¼Œå› ä¸ºScrapyæ²¡æœ‰æä¾›ä¸€ç§æ–¹æ³•æ¥å…¨å±€è¦†ç›–è¯·æ±‚æŒ‡çº¹è®¡ç®—ç®—æ³•ï¼›è¿™åœ¨å°†æ¥å¯èƒ½ä¼šæ”¹å˜ã€‚<br>

There are also some additional options available. Put them into your `settings.py` if you want to change the defaults:<br>

è¿˜æœ‰ä¸€äº›é¢å¤–çš„é€‰é¡¹å¯ç”¨ã€‚å¦‚æœæ‚¨æƒ³æ”¹å˜é»˜è®¤è®¾ç½®ï¼Œè¯·å°†å®ƒä»¬æ”¾å…¥æ‚¨çš„ `settings.py` ä¸­ï¼š<br>

```conf
SPLASH_COOKIES_DEBUG = False  # é»˜è®¤ä¸ºFalseã€‚è®¾ä¸ºTrueå¯ç”¨SplashCookiesMiddlewareä¸­çš„è°ƒè¯•cookiesã€‚è¿™ä¸ªé€‰é¡¹ç±»ä¼¼äºå†…ç½®çš„scarpy cookiesä¸­é—´ä»¶çš„COOKIES_DEBUGï¼šå®ƒè®°å½•æ‰€æœ‰è¯·æ±‚çš„å‘é€å’Œæ¥æ”¶çš„cookiesã€‚
SPLASH_LOG_400 = True  # é»˜è®¤ä¸ºTrue - å®ƒæŒ‡ç¤ºè®°å½•æ‰€æœ‰æ¥è‡ªSplashçš„400é”™è¯¯ã€‚å®ƒä»¬å¾ˆé‡è¦ï¼Œå› ä¸ºå®ƒä»¬æ˜¾ç¤ºäº†æ‰§è¡ŒSplashè„šæœ¬æ—¶å‘ç”Ÿçš„é”™è¯¯ã€‚è®¾ç½®ä¸ºFalseä»¥ç¦ç”¨æ­¤æ—¥å¿—è®°å½•ã€‚
SPLASH_SLOT_POLICY = scrapy_splash.SlotPolicy.PER_DOMAIN  # é»˜è®¤ä¸ºscrapy_splash.SlotPolicy.PER_DOMAINï¼ˆä½œä¸ºå¯¹è±¡ï¼Œè€Œä¸ä»…ä»…æ˜¯å­—ç¬¦ä¸²ï¼‰ã€‚å®ƒæŒ‡å®šäº†å¦‚ä½•ä¸ºSplashè¯·æ±‚ç»´æŠ¤å¹¶å‘å’Œç¤¼è²Œæ€§ï¼Œä»¥åŠä¸ºSplashRequestçš„slot_policyå‚æ•°æŒ‡å®šé»˜è®¤å€¼ï¼Œä¸‹é¢å°†è¿›è¡Œæè¿°ã€‚
```


## Usage:

### Requests:

The easiest way to render requests with Splash is to use `scrapy_splash.SplashRequest`:<br>

ä½¿ç”¨Splashæ¸²æŸ“è¯·æ±‚çš„æœ€ç®€å•æ–¹æ³•æ˜¯ä½¿ç”¨ `scrapy_splash.SplashRequest` ï¼š<br>

```python
yield SplashRequest(url, self.parse_result,
    args={
        # optional; parameters passed to Splash HTTP API
        'wait': 0.5,

        # 'url' is prefilled from request url
        # 'http_method' is set to 'POST' for POST requests
        # 'body' is set to request body for POST requests
    },
    endpoint='render.json', # optional; default is render.html
    splash_url='<url>',     # optional; overrides SPLASH_URL
    slot_policy=scrapy_splash.SlotPolicy.PER_DOMAIN,  # optional
)
```

Alternatively, you can use regular scrapy.Request and `'splash'` Request meta key:<br>

æˆ–è€…ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨å¸¸è§„çš„scrapy.Requestå’Œ `'splash'` è¯·æ±‚å…ƒé”®ï¼š<br>

```python
yield scrapy.Request(url, self.parse_result, meta={
    'splash': {
        'args': {
            # set rendering arguments here(åœ¨è¿™é‡Œè®¾ç½®æ¸²æŸ“å‚æ•°)
            'html': 1,
            'png': 1,

            # 'url' is prefilled from request url('url' ä¼šä»è¯·æ±‚çš„urlä¸­é¢„å¡«)
            # 'http_method' is set to 'POST' for POST requests(å¯¹äºPOSTè¯·æ±‚ï¼Œ'http_method' è®¾ç½®ä¸º 'POST')
            # 'body' is set to request.body for POST requests(å¯¹äºPOSTè¯·æ±‚ï¼Œ'body'è®¾ç½®ä¸ºrequest.body)
        },

        # optional parameters(å¯é€‰å‚æ•°)
        'endpoint': 'render.json',  # optional; default is render.json(å¯é€‰ï¼›é»˜è®¤ä¸º render.json)
        'splash_url': '<url>',      # optional; overrides SPLASH_URL(å¯é€‰ï¼›è¦†ç›–SPLASH_URL)
        'slot_policy': scrapy_splash.SlotPolicy.PER_DOMAIN,
        'splash_headers': {},       # optional; a dict with headers sent to Splash(å¯é€‰ï¼›å‘é€ç»™Splashçš„å¤´éƒ¨å­—å…¸)
        'dont_process_response': True, # optional, default is False(å¯é€‰ï¼Œ é»˜è®¤ä¸º False)
        'dont_send_headers': True,  # optional, default is False(å¯é€‰ï¼Œ é»˜è®¤ä¸º False)
        'magic_response': False,    # optional, default is True(å¯é€‰ï¼Œ é»˜è®¤ä¸º True)
    }
})
```

Use `request.meta['splash']` API in middlewares or when scrapy.Request subclasses are used (there is also `SplashFormRequest` described below). <br>

åœ¨ä¸­é—´ä»¶ä¸­æˆ–ä½¿ç”¨scrapy.Requestå­ç±»æ—¶ä½¿ç”¨ `request.meta['splash']` APIï¼ˆä¸‹æ–‡è¿˜å°†æè¿°SplashFormRequestï¼‰ã€‚<br>

For example, `meta['splash']` allows to create a middleware which enables Splash for all outgoing requests by default.<br>

ä¾‹å¦‚ï¼Œ`meta['splash']` å…è®¸åˆ›å»ºä¸€ä¸ªä¸­é—´ä»¶ï¼Œ**é»˜è®¤æƒ…å†µä¸‹ä¸ºæ‰€æœ‰å¤–å‘è¯·æ±‚å¯ç”¨Splashã€‚ğŸš€ğŸš€ğŸš€**<br>

`SplashRequest` is a convenient utility to fill `request.meta['splash']` ;<br>

`SplashRequest`æ˜¯ä¸€ä¸ªæ–¹ä¾¿çš„å·¥å…·ï¼Œç”¨äºå¡«å…… `request.meta['splash']` ï¼›<br>

it should be easier to use in most cases. For each `request.meta['splash']` key there is a corresponding `SplashRequest` keyword argument:<br>

åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œå®ƒåº”è¯¥æ›´å®¹æ˜“ä½¿ç”¨ã€‚å¯¹äºæ¯ä¸ª `request.meta['splash']` é”®ï¼Œéƒ½æœ‰ä¸€ä¸ªå¯¹åº”çš„ `SplashRequest` å…³é”®å­—å‚æ•°ï¼š<br>

for example, to set `meta['splash']['args']` use `SplashRequest(..., args=myargs)` .<br>

ä¾‹å¦‚ï¼Œè®¾ç½® `meta['splash']['args']ä½¿ç”¨SplashRequest(..., args=myargs)` ã€‚<br>

#### `meta['splash']['args']` contains arguments sent to Splash. scrapy-splash adds some default keys/values to args:

meta['splash']['args']åŒ…å«å‘é€åˆ°Splashçš„å‚æ•°ã€‚scrapy-splashå‘argsæ·»åŠ äº†ä¸€äº›é»˜è®¤çš„é”®/å€¼ï¼š<br>

- 'url' is set to request.url('url'è®¾ç½®ä¸ºrequest.url);

- 'http_method' is set to 'POST' for POST requests(å¯¹äºPOSTè¯·æ±‚ï¼Œ'http_method'è®¾ç½®ä¸º'POST');

- 'body' is set to request.body for POST requests(å¯¹äºPOSTè¯·æ±‚ï¼Œ'body'è®¾ç½®ä¸ºrequest.body).

You can override default values by setting them explicitly.<br>

æ‚¨å¯ä»¥é€šè¿‡æ˜ç¡®è®¾ç½®æ¥è¦†ç›–é»˜è®¤å€¼ã€‚<br>

Note that by default Scrapy escapes URL fragments using AJAX escaping scheme. If you want to pass a URL with a fragment to Splash then set `url` in `args` dict manually. <br>

è¯·æ³¨æ„ï¼Œé»˜è®¤æƒ…å†µä¸‹Scrapyä½¿ç”¨AJAXè½¬ä¹‰æ–¹æ¡ˆè½¬ä¹‰URLç‰‡æ®µã€‚å¦‚æœæ‚¨æƒ³å°†å¸¦æœ‰ç‰‡æ®µçš„URLä¼ é€’ç»™Splashï¼Œåˆ™éœ€è¦æ‰‹åŠ¨åœ¨argså­—å…¸ä¸­è®¾ç½®urlã€‚<br>

ğŸš¨ğŸš¨ğŸš¨This is handled automatically if you use `SplashRequest`, but you need to keep that in mind if you use raw `meta['splash']` API.<br>

ğŸš¨ğŸš¨ğŸš¨å¦‚æœæ‚¨ä½¿ç”¨ `SplashRequest` ï¼Œåˆ™ä¼šè‡ªåŠ¨å¤„ç†æ­¤æ“ä½œï¼Œä½†å¦‚æœæ‚¨ä½¿ç”¨åŸå§‹ `meta['splash']` APIï¼Œåˆ™éœ€è¦è®°ä½è¿™ä¸€ç‚¹ã€‚<br>




